{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Fine-tuning BERT Models for Classification</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"datasets>=2.18.0,<3\" transformers>=4.38.2 sentence-transformers>=2.5.1 setfit>=1.0.3 accelerate>=0.27.2 seqeval>=1.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBeVnXxQWy7-"
   },
   "source": [
    "## **Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "e3c168a65bd34a64b2c4d13129f9d750",
      "5ad968464c934a15b19e3924663e5ebf",
      "31756f672e60432da8aa12b033a2ea20",
      "63adb81b029744fb82e31b3f1190bb9b",
      "77511fda45a940d1a6664438b07712b2",
      "677310bbba174f3a828d7f625a483f92",
      "bbb27e92d9e848e9ada7ba0c9866f87a",
      "d6299b347ade47bba93df3eff231be6f",
      "49541355ed224e748754ed4bd89792d8",
      "7cc8919e7f9e4db48a876619edff0952",
      "fdb9b32fdccf4156b537df8e000ec7d2",
      "0cd7314417ab48f99adf352f0cab863f",
      "bbb68b7249674351abfa8661b4645e2e",
      "285d6e493dcb462598190862b186b17e",
      "29419f20fad94db88b6da3b1ecd04725",
      "3d58f1665e0949d1baa1feaffd52aa29",
      "82c561fe82084baaa9eedfb78ec476da",
      "10165d819bea43f1ad531ee0a314d002",
      "ed2ca51dff8847648010e7b28f03d41e",
      "9f216034e74a4f0a9794a812addc82c5",
      "d3748aec876a4ddbb377f16a43b3c23e",
      "081b5fcad38b4627b1d553244a1bed0c",
      "3a4ca5b37d6f426a9d294ade901f6984",
      "673f06ed6138437680a01409cbce3a13",
      "625db697c4084a4fa7f78905c5c830b2",
      "a6c837a7a4fb45bcb035537cf41796e3",
      "0f53e4afed4a42a5a63d8a64f847c27e",
      "127978e95632464fb3b7396de66ff240",
      "a7fa1b4de3b6423783861a6d8213f430",
      "c185b6d9a8514c71b36918027a424204",
      "9fc58f20135e48b897cb6a2b8a67863d",
      "24b94b5fc6fd4aa7ae113d9eeb3a4d6f",
      "7c386e049403493997772cf4aca4a234",
      "09a021dba22548f3bd101e237c5ce51d",
      "36fdfe575de440458c4fe7cd572d2a22",
      "7ef0c97a8b774e57a041a210c3e23ce6",
      "d9379ba58d8e4de9b4ba3cc12be87b4c",
      "ee301aff657e405c8cdfd2e0f5d7d19e",
      "87d2c368a3d44c1994c8fe31fea59212",
      "0db4bd167da44ee7b9fc044d9bb5834d",
      "ebc2ddfef4cd4129997138c615cd15a1",
      "1b0fa0592da147fcab20f0c17f487542",
      "0a5befced7554f0a92523b5dca953ed4",
      "ec515b2a14ff4414baddca0cc672ef14",
      "2eaded0fbbd74c67b802fbb92973fa3a",
      "d1e9c32d66004c70b0e3fa96ccb1e4e3",
      "d27a323324cd45789808c22917fa5d36",
      "7d827c0ee22b47c4b46fd4f59ac3ec23",
      "aba270b810d6461887633f62cf35195c",
      "f43737300b254f2bb3aef90fabf652e2",
      "ff95c667621d46b89412c0638ce91673",
      "5f810926256a4a3aa5271c504cdad278",
      "117e9997a5db42e59e1f73eddd345b25",
      "c807a7269c2a4f8daf8022a177175213",
      "c123b5dae69741d6a10b0b783f5e54ac",
      "5a5d72b8d2484c54977e5ec692a00004",
      "fd5ff63e2c204836b1c6924633880077",
      "1c96b599ae6d466e94062f21e7d3a54b",
      "690d9fa381b74085af8025af1dcffdcf",
      "95ab40e005a9419aa7827dbcbd8a2415",
      "aeffa7363e144c0a9b830c7b532167cb",
      "59cd08daef304e78be3574b4345084b2",
      "1700973179db4c91a9c00fe5e84de1a9",
      "5a22333fd16d46429e16f8b1ed9276a1",
      "34e7d842cf364b9f8b377b17befa20b2",
      "84454ba3262b48d489215b874bd78842",
      "91552574464449e48e54a6939f0a5d18",
      "0f4c3ac40bf84e6293419447a7fbdd15",
      "c82ed37a975149d8895adc331aae34eb",
      "96252dca0e9049fea03228e7ceba066b",
      "25ed22abc1934a3393593d5b10331988",
      "b883ce44ce9e47d6b5ac70e36e049559",
      "17afecdfb20b4e49a79aea97d20a10dd",
      "c6b1edc0dece4354b7a43314c419cb71",
      "19b3c0979b3f45499faa57f341d6d53a",
      "ef4d27b4292240928439104f38c610b9",
      "ce07f53ddea3469282b20581a4f4e4da"
     ]
    },
    "executionInfo": {
     "elapsed": 21399,
     "status": "ok",
     "timestamp": 1719386835780,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "5phRS_z2U_3T",
    "outputId": "439a9900-6d9a-4a53-ea30-4774d3bf4de7"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Prepare data and splits\n",
    "tomatoes = load_dataset(\"rotten_tomatoes\")\n",
    "train_data, test_data = tomatoes[\"train\"], tomatoes[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xya5dfmVoR1R"
   },
   "source": [
    "## **Supervised Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODEmWQe8W_le"
   },
   "source": [
    "### HuggingFace Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232,
     "referenced_widgets": [
      "1d0ff62806e04095a64ea4c9c299a276",
      "17bfe3f4e34a4467ac82642d905877ee",
      "2dcbebcd7c1a4d9db8783e85609e6419",
      "61ae7efc64864dacb03354a9189d0034",
      "f6ceedd97cf84a959d2b631bc677bf0c",
      "e266572c32be4532ae59b0af652ee0f9",
      "b388229af88446358932d199e0fbf70f",
      "ee169c0ee96442b0b0acbbd4ef2feedc",
      "58e0fb0d5c2b401ab49ce3d6a1a46ba9",
      "514f1594aa114a5e98aaa2f3197ff106",
      "726177412b3d4c8f9ca9f481fafcb41a",
      "4bf958fcb13740d29ad463bd4f9b7e5c",
      "7ed87ac491764e4db2d23635a81c4246",
      "67da8e11fe7f49589c923326c7e6e19c",
      "f26b357136fd48a1acaddc2969147337",
      "0c75544ff7784990998c8a398b470f78",
      "efae0be4bc7f41a8a23ca5c23f174f9e",
      "d401147edaa0404b9f00dc881f895509",
      "e1edb2b21532480eb6df74eafeb876a0",
      "a493195172fd48239c720a874acd9d83",
      "d993c04399af4bea87e15db60b7bcb4c",
      "b3de189dfd9948aa958854707b5bc76f",
      "3b9c918bb3f04232a9dac11f4e46baf8",
      "bae0792a9ba44c628906dca306d656df",
      "f82d95c50ea946a79a1587b472367f99",
      "2285d28a20e142169d848e3a3641eec0",
      "a7a9040968ac41f69df95da42e853471",
      "b61d12afe30d4d4c8bb3808225d6ce3f",
      "7e5b26db97454d389794ab03f784f975",
      "cb751178c50f43fb8a06c63218ea8029",
      "6fa29e1fc9c143269d68a72a3349c363",
      "c91875964a24495eb1a0395375a1cc07",
      "5b7fdd9526fb4f96a3f178d021fc51ff",
      "5c43d75618834ca49ff92c52c725dfdc",
      "6d1656f89dbf4af3914ad5b5f7cb8570",
      "ad52002217cf4b85b148f1dd870871f3",
      "328ab43065d647e58d7bc53913107b9a",
      "c2aa35abf89442079f907b9928dda445",
      "281bed63cd5447559eb8305d9eda0b06",
      "7d5a3b0eba09486f88cd3e857790fa5d",
      "3ef14566be734f23a7ccbcc446f94073",
      "513dc889d5eb413e872b807cc98f64bb",
      "d0344e9d2d914fb29f5ce4f7cceca39c",
      "0d45042f22674eb7abcf033a7c34ce3c",
      "35498c89148e4e548e110ff979b33dd8",
      "fcb89932078f484da12e2916ec62ecc1",
      "917ed901f0e74c9eaaa8cedf6b24b1f7",
      "8fb9a0880b4740b4b78e125b31959f4b",
      "9b5ed41f3ede47bf9917830804dd4958",
      "9f0bcc78563a4e7895c8fa9f304b17a9",
      "668812cc3d6d45d393437bb299816015",
      "a1c09a06df704b1388759228eac0aa69",
      "cf599267ff80457ca1080bc7d5636b90",
      "224eb4bbc40240b98ca9128066d301c3",
      "11dc85e0f99d48a68cec364dcdb7d57d"
     ]
    },
    "executionInfo": {
     "elapsed": 9171,
     "status": "ok",
     "timestamp": 1719386844950,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "b-1UGAg7WAHk",
    "outputId": "e1fd7d6f-5b19-4fd7-8be0-690afe164820"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "'''AutoTokenizer: Automatically loads the correct tokenizer class (e.g., BertTokenizer, RobertaTokenizer) based on the model name.\n",
    "AutoModelForSequenceClassification: Automatically loads a model class designed for sequence classification (e.g., sentiment analysis, entailment, etc.).'''\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "'''Specifies the model checkpoint from Hugging Face’s Model Hub.bert-base-cased:BERT base model (12 layers, 110M parameters)\n",
    "Uses cased tokens (keeps capitalization, unlike uncased models)'''\n",
    "model_id = \"bert-base-cased\"\n",
    "\n",
    "'''Loads a pretrained BERT model for classification tasks. num_labels=2: Sets the model's classification head to output 2 logits, typically used for binary classification \n",
    "(e.g., \"entailment\" vs. \"not entailment\").\n",
    "from_pretrained(...): Downloads the model weights from Hugging Face. Loads BERT’s encoder and attaches a new classification head on top'''\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "\n",
    "'''Loads the tokenizer that matches the bert-base-cased model. This tokenizer: Splits text into tokens (word pieces). Converts tokens to input IDs\n",
    "Pads, truncates, and creates attention masks as needed'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r48vDo8fa33D"
   },
   "source": [
    "Tokenize our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "a3ee82becf164af19d6ad09d7897a306",
      "bd64c7b50ad345a9b9f1c253fff11b9a",
      "e4038ea26e7248b08a9bf77390ff7cf5",
      "4ff9a26f1e274b47883cedbf09f40c20",
      "76dcdbcd894640fe92b894bb9497ca78",
      "c94e820825704d9fb42b6ab5b56c8a58",
      "ca8c5e06e4404c39a944f68ecdd64eb2",
      "e9087132832c4b64b4226d9348c3e3fe",
      "49a0d9996c5a4dacb7be81b0c79447a7",
      "7b30340529994e2bb6c1584009eaa165",
      "52dbbd4610d54640a1ce46c767cb79cd",
      "a5637ae2cbd44f21aee434d399e2be37",
      "9bdb983954064b50ab80865c26a74edd",
      "2b272649b2844a98b4d66c8e4dc32798",
      "71ab8ce53934408d94cf07d60c845380",
      "a46b81d1b5384fd8b11a9404864b56c8",
      "928473597d704fc9bc95c9b00c5185ec",
      "86fcfb7fa0b742b3b1afad960d084ab3",
      "96249184adf84fcbbfebf5872aab29a2",
      "399cd589edfd4db2ac8ecc29283c114f",
      "482bcf30a8204361b610a7e04627e57e",
      "65dc3e08e34b463385d782600a7096b3"
     ]
    },
    "executionInfo": {
     "elapsed": 5163,
     "status": "ok",
     "timestamp": 1719386850111,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "5ySNm-a3WCFI",
    "outputId": "892e96a5-7161-485c-b514-9e24ec117394"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba0b81643f64e649d8a88c7833373c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37ba63899dce476abe7e9c6f71c375a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''A data collator dynamically pads inputs in a batch to the longest sequence length in that batch — not globally.\n",
    "This saves memory and speeds up training compared to padding all inputs to a fixed max length.'''\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Pad to the longest sequence in the batch\n",
    "'''Uses the tokenizer's padding rules (e.g., pad token ID). Later, you pass this data_collator into a Trainer or DataLoader for smart batching.'''\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "'''Tokenizes each input string in the dataset. examples[\"text\"] assumes your dataset has a column named \"text\" (can be \"sentence\", \"input\", etc. depending on your dataset).\n",
    "truncation=True: Truncates long inputs to the model's max length (usually 512 for BERT).'''\n",
    "def preprocess_function(examples):\n",
    "   \"\"\"Tokenize input data\"\"\"\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Tokenize train/test data\n",
    "'''map(...): Applies your preprocess_function to every item in the dataset. batched=True: Processes multiple samples at once (faster). Adds tokenized fields like:\n",
    "input_ids, attention_mask, (optionally) token_type_ids'''\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJ1NM1gjbMD7"
   },
   "source": [
    "Define metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Y724gUYyWIvq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "'''evaluate is Hugging Face’s unified library for evaluation metrics (like f1, accuracy, bleu, etc.).'''\n",
    "import evaluate\n",
    "\n",
    "'''eval_pred is a tuple: (logits, true_labels). logits: raw model outputs before softmax. labels: ground-truth labels.'''\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate F1 score\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    '''Loads the F1 metric using the evaluate library. Computes the F1 score using the predicted and true labels.'''\n",
    "    load_f1 = evaluate.load(\"f1\")\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAm-sAl9bOPC"
   },
   "source": [
    "Train model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Dho6VcG9WK5u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:11: SyntaxWarning: invalid escape sequence '\\&'\n",
      "<>:11: SyntaxWarning: invalid escape sequence '\\&'\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3114871731.py:11: SyntaxWarning: invalid escape sequence '\\&'\n",
      "  | `report_to=\"none\"`               | Disables logging to tools like W\\&B, TensorBoard, etc. |\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3114871731.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "'''| Parameter                        | Meaning                                                |\n",
    "| -------------------------------- | ------------------------------------------------------ |\n",
    "| `\"model\"`                        | Output directory where model checkpoints will be saved |\n",
    "| `learning_rate=2e-5`             | Common learning rate for fine-tuning BERT-like models  |\n",
    "| `per_device_train_batch_size=16` | Batch size for training (per GPU)                      |\n",
    "| `per_device_eval_batch_size=16`  | Batch size for evaluation                              |\n",
    "| `num_train_epochs=1`             | Train for 1 epoch (can increase later)                 |\n",
    "| `weight_decay=0.01`              | Adds L2 regularization to prevent overfitting          |\n",
    "| `save_strategy=\"epoch\"`          | Saves model at the end of every epoch                  |\n",
    "| `report_to=\"none\"`               | Disables logging to tools like W\\&B, TensorBoard, etc. |\n",
    "'''\n",
    "\n",
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=1,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "'''| Argument          | Purpose                                                                   |\n",
    "| ----------------- | ------------------------------------------------------------------------- |\n",
    "| `model`           | The model you loaded earlier (`bert-base-cased` with classification head) |\n",
    "| `args`            | Your training configuration                                               |\n",
    "| `train_dataset`   | Tokenized training dataset                                                |\n",
    "| `eval_dataset`    | Tokenized evaluation dataset                                              |\n",
    "| `tokenizer`       | Used for saving and padding data correctly                                |\n",
    "| `data_collator`   | Pads batches dynamically for efficient training                           |\n",
    "| `compute_metrics` | Custom evaluation (e.g., F1 score you defined earlier)                    |\n",
    " '''\n",
    "\n",
    "# Trainer which executes the training process\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 62039,
     "status": "ok",
     "timestamp": 1719386995069,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "vOzl0WnSbVnY",
    "outputId": "0058b56f-6a7c-40d8-f08f-2bd035d7ab79"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 25:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.406100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.402509808986821, metrics={'train_runtime': 1516.7754, 'train_samples_per_second': 5.624, 'train_steps_per_second': 0.352, 'total_flos': 227605451772240.0, 'train_loss': 0.402509808986821, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkBUVlUYbUnn"
   },
   "source": [
    "Evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "d656514d509441548af1dd6c7326d18a",
      "05ce4f6b47354c628c0a41544b9419f5",
      "d55ba50d84d7412daeb6ccc73be98694",
      "d5d54eddd21a4999aec6ea6a7782e223",
      "e8da2d90a93b4ce9a93c2ee93f75d59b",
      "b94e1b170ccc4ff7abbb9ed87ad2aedd",
      "3c38333cafb446bab3f3f75824075927",
      "960641d3e97f4a538a135861ab058bca",
      "fc4d3609cada4615924d5e92ee1ffdf4",
      "f914214414b14945a72ac57ac93dcb5e",
      "8cb0531c591c43f098564ce043eb036f"
     ]
    },
    "executionInfo": {
     "elapsed": 3959,
     "status": "ok",
     "timestamp": 1719386999026,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "wCI9uYDObWU8",
    "outputId": "5c2d54d2-0c81-4a10-d46d-9d7c4c9405d3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.36064156889915466,\n",
       " 'eval_f1': 0.8533585619678334,\n",
       " 'eval_runtime': 47.6687,\n",
       " 'eval_samples_per_second': 22.363,\n",
       " 'eval_steps_per_second': 1.406,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o5gefwxOBllA"
   },
   "source": [
    "### Freeze Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1719386999026,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "t0ZOuoe7Dj3c",
    "outputId": "00b9fb9c-f819-457d-91ad-a57881778d1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Model and Tokenizer\n",
    "'''Loads a pretrained model (e.g., BERT, RoBERTa) with a classification head on top. model_id: could be 'bert-base-cased', 'roberta-base', or any other Hugging Face model name.\n",
    "num_labels=2: sets the output head for binary classification (2 classes). You can change this to match your task.\n",
    "What the model outputs: A tensor of shape [batch_size, num_labels], representing logits for each class.'''\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "\n",
    "'''Loads the corresponding tokenizer for the specified model. It handles: Text cleaning, Tokenization (splitting words into tokens or subwords), Conversion to token IDs\n",
    "Adding special tokens like [CLS], [SEP], Attention masks and padding'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1719386999026,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "iI8vf_mnBniu",
    "outputId": "d5d31516-977b-462b-f878-5a71c084db7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# Print layer names\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "FnpGOry_Bm36"
   },
   "outputs": [],
   "source": [
    "'''This is a form of transfer learning, where you: Use the pretrained model as a frozen feature extractor.\n",
    "Only fine-tune the final classification head (typically a small number of parameters). You can selectively unfreeze the last few layers (for more adaptability) like this:\n",
    "    for name, param in model.named_parameters():\n",
    "    if name.startswith(\"classifier\") or \"encoder.layer.11\" in name or \"encoder.layer.10\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    " '''\n",
    "#This code is freezing most of your model’s layers, except for the classification head, which you keep trainable.\n",
    "for name, param in model.named_parameters():\n",
    "\n",
    "     # Trainable classification head\n",
    "     if name.startswith(\"classifier\"):\n",
    "        param.requires_grad = True\n",
    "\n",
    "      # Freeze everything else\n",
    "     else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1719386999026,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "vf_wYzpMB4uX",
    "outputId": "8a63f6d8-4516-4d84-d0d5-39f1823b4d03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: bert.embeddings.word_embeddings.weight ----- False\n",
      "Parameter: bert.embeddings.position_embeddings.weight ----- False\n",
      "Parameter: bert.embeddings.token_type_embeddings.weight ----- False\n",
      "Parameter: bert.embeddings.LayerNorm.weight ----- False\n",
      "Parameter: bert.embeddings.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.pooler.dense.weight ----- False\n",
      "Parameter: bert.pooler.dense.bias ----- False\n",
      "Parameter: classifier.weight ----- True\n",
      "Parameter: classifier.bias ----- True\n"
     ]
    }
   ],
   "source": [
    "# We can check whether the model was correctly updated\n",
    "for name, param in model.named_parameters():\n",
    "     print(f\"Parameter: {name} ----- {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 15480,
     "status": "ok",
     "timestamp": 1719387014504,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "rVi36FJSG4ue",
    "outputId": "c2e92d2d-247e-4065-8d06-688e24f472f7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\386210857.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 10:02, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.704600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.7044561864731463, metrics={'train_runtime': 603.8468, 'train_samples_per_second': 14.126, 'train_steps_per_second': 0.884, 'total_flos': 227605451772240.0, 'train_loss': 0.7044561864731463, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this code is launching the fine-tuning process for your classification model using Hugging Face's Trainer\n",
    "'''| Parameter         | Description                                                                                                                      |\n",
    "| ----------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `model`           | Your pretrained BERT-like model with a classification head, where only the `classifier` layer is trainable (you froze the rest). |\n",
    "| `args`            | Training settings like batch size, learning rate, number of epochs, save strategy, etc. (from `TrainingArguments`).              |\n",
    "| `train_dataset`   | Your tokenized training dataset.                                                                                                 |\n",
    "| `eval_dataset`    | Your tokenized validation set (used for evaluation after each epoch).                                                            |\n",
    "| `tokenizer`       | Used to process text during training (e.g., saving the model properly).                                                          |\n",
    "| `data_collator`   | Automatically pads inputs in a batch to the longest input in that batch (efficient and GPU-friendly).                            |\n",
    "| `compute_metrics` | Your custom function to calculate F1 score (can add accuracy, precision, etc. too).                                              |\n",
    " '''\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Trainer which executes the training process\n",
    "'''Behind the Scenes When You Call trainer.train() Dataset is loaded in batches, with dynamic padding via data_collator.\n",
    "Model is trained for the number of epochs you set (num_train_epochs=1 in your case).\n",
    "After each epoch: Model is evaluated on the validation dataset. F1 score is computed and printed. If save_strategy=\"epoch\" was set (as in your case), \n",
    "it saves the model checkpoint after each epoch. Progress bar + training logs are shown unless disabled.'''\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 2623,
     "status": "ok",
     "timestamp": 1719387017125,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "eCPpixB1HCsI",
    "outputId": "3d77ed38-0565-492b-b488-09eb525fc316"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6915770769119263,\n",
       " 'eval_f1': 0.6515704894083273,\n",
       " 'eval_runtime': 55.8284,\n",
       " 'eval_samples_per_second': 19.094,\n",
       " 'eval_steps_per_second': 1.2,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uw729mLhIQL6"
   },
   "source": [
    "### Freeze blocks 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 640,
     "status": "ok",
     "timestamp": 1719387017762,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "RbsLR561Kje-",
    "outputId": "ae1b3e9b-443d-4a06-94bb-6792d1751e27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: 0bert.embeddings.word_embeddings.weight ----- False\n",
      "Parameter: 1bert.embeddings.position_embeddings.weight ----- False\n",
      "Parameter: 2bert.embeddings.token_type_embeddings.weight ----- False\n",
      "Parameter: 3bert.embeddings.LayerNorm.weight ----- False\n",
      "Parameter: 4bert.embeddings.LayerNorm.bias ----- False\n",
      "Parameter: 5bert.encoder.layer.0.attention.self.query.weight ----- False\n",
      "Parameter: 6bert.encoder.layer.0.attention.self.query.bias ----- False\n",
      "Parameter: 7bert.encoder.layer.0.attention.self.key.weight ----- False\n",
      "Parameter: 8bert.encoder.layer.0.attention.self.key.bias ----- False\n",
      "Parameter: 9bert.encoder.layer.0.attention.self.value.weight ----- False\n",
      "Parameter: 10bert.encoder.layer.0.attention.self.value.bias ----- False\n",
      "Parameter: 11bert.encoder.layer.0.attention.output.dense.weight ----- False\n",
      "Parameter: 12bert.encoder.layer.0.attention.output.dense.bias ----- False\n",
      "Parameter: 13bert.encoder.layer.0.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 14bert.encoder.layer.0.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 15bert.encoder.layer.0.intermediate.dense.weight ----- False\n",
      "Parameter: 16bert.encoder.layer.0.intermediate.dense.bias ----- False\n",
      "Parameter: 17bert.encoder.layer.0.output.dense.weight ----- False\n",
      "Parameter: 18bert.encoder.layer.0.output.dense.bias ----- False\n",
      "Parameter: 19bert.encoder.layer.0.output.LayerNorm.weight ----- False\n",
      "Parameter: 20bert.encoder.layer.0.output.LayerNorm.bias ----- False\n",
      "Parameter: 21bert.encoder.layer.1.attention.self.query.weight ----- False\n",
      "Parameter: 22bert.encoder.layer.1.attention.self.query.bias ----- False\n",
      "Parameter: 23bert.encoder.layer.1.attention.self.key.weight ----- False\n",
      "Parameter: 24bert.encoder.layer.1.attention.self.key.bias ----- False\n",
      "Parameter: 25bert.encoder.layer.1.attention.self.value.weight ----- False\n",
      "Parameter: 26bert.encoder.layer.1.attention.self.value.bias ----- False\n",
      "Parameter: 27bert.encoder.layer.1.attention.output.dense.weight ----- False\n",
      "Parameter: 28bert.encoder.layer.1.attention.output.dense.bias ----- False\n",
      "Parameter: 29bert.encoder.layer.1.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 30bert.encoder.layer.1.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 31bert.encoder.layer.1.intermediate.dense.weight ----- False\n",
      "Parameter: 32bert.encoder.layer.1.intermediate.dense.bias ----- False\n",
      "Parameter: 33bert.encoder.layer.1.output.dense.weight ----- False\n",
      "Parameter: 34bert.encoder.layer.1.output.dense.bias ----- False\n",
      "Parameter: 35bert.encoder.layer.1.output.LayerNorm.weight ----- False\n",
      "Parameter: 36bert.encoder.layer.1.output.LayerNorm.bias ----- False\n",
      "Parameter: 37bert.encoder.layer.2.attention.self.query.weight ----- False\n",
      "Parameter: 38bert.encoder.layer.2.attention.self.query.bias ----- False\n",
      "Parameter: 39bert.encoder.layer.2.attention.self.key.weight ----- False\n",
      "Parameter: 40bert.encoder.layer.2.attention.self.key.bias ----- False\n",
      "Parameter: 41bert.encoder.layer.2.attention.self.value.weight ----- False\n",
      "Parameter: 42bert.encoder.layer.2.attention.self.value.bias ----- False\n",
      "Parameter: 43bert.encoder.layer.2.attention.output.dense.weight ----- False\n",
      "Parameter: 44bert.encoder.layer.2.attention.output.dense.bias ----- False\n",
      "Parameter: 45bert.encoder.layer.2.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 46bert.encoder.layer.2.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 47bert.encoder.layer.2.intermediate.dense.weight ----- False\n",
      "Parameter: 48bert.encoder.layer.2.intermediate.dense.bias ----- False\n",
      "Parameter: 49bert.encoder.layer.2.output.dense.weight ----- False\n",
      "Parameter: 50bert.encoder.layer.2.output.dense.bias ----- False\n",
      "Parameter: 51bert.encoder.layer.2.output.LayerNorm.weight ----- False\n",
      "Parameter: 52bert.encoder.layer.2.output.LayerNorm.bias ----- False\n",
      "Parameter: 53bert.encoder.layer.3.attention.self.query.weight ----- False\n",
      "Parameter: 54bert.encoder.layer.3.attention.self.query.bias ----- False\n",
      "Parameter: 55bert.encoder.layer.3.attention.self.key.weight ----- False\n",
      "Parameter: 56bert.encoder.layer.3.attention.self.key.bias ----- False\n",
      "Parameter: 57bert.encoder.layer.3.attention.self.value.weight ----- False\n",
      "Parameter: 58bert.encoder.layer.3.attention.self.value.bias ----- False\n",
      "Parameter: 59bert.encoder.layer.3.attention.output.dense.weight ----- False\n",
      "Parameter: 60bert.encoder.layer.3.attention.output.dense.bias ----- False\n",
      "Parameter: 61bert.encoder.layer.3.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 62bert.encoder.layer.3.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 63bert.encoder.layer.3.intermediate.dense.weight ----- False\n",
      "Parameter: 64bert.encoder.layer.3.intermediate.dense.bias ----- False\n",
      "Parameter: 65bert.encoder.layer.3.output.dense.weight ----- False\n",
      "Parameter: 66bert.encoder.layer.3.output.dense.bias ----- False\n",
      "Parameter: 67bert.encoder.layer.3.output.LayerNorm.weight ----- False\n",
      "Parameter: 68bert.encoder.layer.3.output.LayerNorm.bias ----- False\n",
      "Parameter: 69bert.encoder.layer.4.attention.self.query.weight ----- False\n",
      "Parameter: 70bert.encoder.layer.4.attention.self.query.bias ----- False\n",
      "Parameter: 71bert.encoder.layer.4.attention.self.key.weight ----- False\n",
      "Parameter: 72bert.encoder.layer.4.attention.self.key.bias ----- False\n",
      "Parameter: 73bert.encoder.layer.4.attention.self.value.weight ----- False\n",
      "Parameter: 74bert.encoder.layer.4.attention.self.value.bias ----- False\n",
      "Parameter: 75bert.encoder.layer.4.attention.output.dense.weight ----- False\n",
      "Parameter: 76bert.encoder.layer.4.attention.output.dense.bias ----- False\n",
      "Parameter: 77bert.encoder.layer.4.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 78bert.encoder.layer.4.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 79bert.encoder.layer.4.intermediate.dense.weight ----- False\n",
      "Parameter: 80bert.encoder.layer.4.intermediate.dense.bias ----- False\n",
      "Parameter: 81bert.encoder.layer.4.output.dense.weight ----- False\n",
      "Parameter: 82bert.encoder.layer.4.output.dense.bias ----- False\n",
      "Parameter: 83bert.encoder.layer.4.output.LayerNorm.weight ----- False\n",
      "Parameter: 84bert.encoder.layer.4.output.LayerNorm.bias ----- False\n",
      "Parameter: 85bert.encoder.layer.5.attention.self.query.weight ----- False\n",
      "Parameter: 86bert.encoder.layer.5.attention.self.query.bias ----- False\n",
      "Parameter: 87bert.encoder.layer.5.attention.self.key.weight ----- False\n",
      "Parameter: 88bert.encoder.layer.5.attention.self.key.bias ----- False\n",
      "Parameter: 89bert.encoder.layer.5.attention.self.value.weight ----- False\n",
      "Parameter: 90bert.encoder.layer.5.attention.self.value.bias ----- False\n",
      "Parameter: 91bert.encoder.layer.5.attention.output.dense.weight ----- False\n",
      "Parameter: 92bert.encoder.layer.5.attention.output.dense.bias ----- False\n",
      "Parameter: 93bert.encoder.layer.5.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 94bert.encoder.layer.5.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 95bert.encoder.layer.5.intermediate.dense.weight ----- False\n",
      "Parameter: 96bert.encoder.layer.5.intermediate.dense.bias ----- False\n",
      "Parameter: 97bert.encoder.layer.5.output.dense.weight ----- False\n",
      "Parameter: 98bert.encoder.layer.5.output.dense.bias ----- False\n",
      "Parameter: 99bert.encoder.layer.5.output.LayerNorm.weight ----- False\n",
      "Parameter: 100bert.encoder.layer.5.output.LayerNorm.bias ----- False\n",
      "Parameter: 101bert.encoder.layer.6.attention.self.query.weight ----- False\n",
      "Parameter: 102bert.encoder.layer.6.attention.self.query.bias ----- False\n",
      "Parameter: 103bert.encoder.layer.6.attention.self.key.weight ----- False\n",
      "Parameter: 104bert.encoder.layer.6.attention.self.key.bias ----- False\n",
      "Parameter: 105bert.encoder.layer.6.attention.self.value.weight ----- False\n",
      "Parameter: 106bert.encoder.layer.6.attention.self.value.bias ----- False\n",
      "Parameter: 107bert.encoder.layer.6.attention.output.dense.weight ----- False\n",
      "Parameter: 108bert.encoder.layer.6.attention.output.dense.bias ----- False\n",
      "Parameter: 109bert.encoder.layer.6.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 110bert.encoder.layer.6.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 111bert.encoder.layer.6.intermediate.dense.weight ----- False\n",
      "Parameter: 112bert.encoder.layer.6.intermediate.dense.bias ----- False\n",
      "Parameter: 113bert.encoder.layer.6.output.dense.weight ----- False\n",
      "Parameter: 114bert.encoder.layer.6.output.dense.bias ----- False\n",
      "Parameter: 115bert.encoder.layer.6.output.LayerNorm.weight ----- False\n",
      "Parameter: 116bert.encoder.layer.6.output.LayerNorm.bias ----- False\n",
      "Parameter: 117bert.encoder.layer.7.attention.self.query.weight ----- False\n",
      "Parameter: 118bert.encoder.layer.7.attention.self.query.bias ----- False\n",
      "Parameter: 119bert.encoder.layer.7.attention.self.key.weight ----- False\n",
      "Parameter: 120bert.encoder.layer.7.attention.self.key.bias ----- False\n",
      "Parameter: 121bert.encoder.layer.7.attention.self.value.weight ----- False\n",
      "Parameter: 122bert.encoder.layer.7.attention.self.value.bias ----- False\n",
      "Parameter: 123bert.encoder.layer.7.attention.output.dense.weight ----- False\n",
      "Parameter: 124bert.encoder.layer.7.attention.output.dense.bias ----- False\n",
      "Parameter: 125bert.encoder.layer.7.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 126bert.encoder.layer.7.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 127bert.encoder.layer.7.intermediate.dense.weight ----- False\n",
      "Parameter: 128bert.encoder.layer.7.intermediate.dense.bias ----- False\n",
      "Parameter: 129bert.encoder.layer.7.output.dense.weight ----- False\n",
      "Parameter: 130bert.encoder.layer.7.output.dense.bias ----- False\n",
      "Parameter: 131bert.encoder.layer.7.output.LayerNorm.weight ----- False\n",
      "Parameter: 132bert.encoder.layer.7.output.LayerNorm.bias ----- False\n",
      "Parameter: 133bert.encoder.layer.8.attention.self.query.weight ----- False\n",
      "Parameter: 134bert.encoder.layer.8.attention.self.query.bias ----- False\n",
      "Parameter: 135bert.encoder.layer.8.attention.self.key.weight ----- False\n",
      "Parameter: 136bert.encoder.layer.8.attention.self.key.bias ----- False\n",
      "Parameter: 137bert.encoder.layer.8.attention.self.value.weight ----- False\n",
      "Parameter: 138bert.encoder.layer.8.attention.self.value.bias ----- False\n",
      "Parameter: 139bert.encoder.layer.8.attention.output.dense.weight ----- False\n",
      "Parameter: 140bert.encoder.layer.8.attention.output.dense.bias ----- False\n",
      "Parameter: 141bert.encoder.layer.8.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 142bert.encoder.layer.8.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 143bert.encoder.layer.8.intermediate.dense.weight ----- False\n",
      "Parameter: 144bert.encoder.layer.8.intermediate.dense.bias ----- False\n",
      "Parameter: 145bert.encoder.layer.8.output.dense.weight ----- False\n",
      "Parameter: 146bert.encoder.layer.8.output.dense.bias ----- False\n",
      "Parameter: 147bert.encoder.layer.8.output.LayerNorm.weight ----- False\n",
      "Parameter: 148bert.encoder.layer.8.output.LayerNorm.bias ----- False\n",
      "Parameter: 149bert.encoder.layer.9.attention.self.query.weight ----- False\n",
      "Parameter: 150bert.encoder.layer.9.attention.self.query.bias ----- False\n",
      "Parameter: 151bert.encoder.layer.9.attention.self.key.weight ----- False\n",
      "Parameter: 152bert.encoder.layer.9.attention.self.key.bias ----- False\n",
      "Parameter: 153bert.encoder.layer.9.attention.self.value.weight ----- False\n",
      "Parameter: 154bert.encoder.layer.9.attention.self.value.bias ----- False\n",
      "Parameter: 155bert.encoder.layer.9.attention.output.dense.weight ----- False\n",
      "Parameter: 156bert.encoder.layer.9.attention.output.dense.bias ----- False\n",
      "Parameter: 157bert.encoder.layer.9.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 158bert.encoder.layer.9.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 159bert.encoder.layer.9.intermediate.dense.weight ----- False\n",
      "Parameter: 160bert.encoder.layer.9.intermediate.dense.bias ----- False\n",
      "Parameter: 161bert.encoder.layer.9.output.dense.weight ----- False\n",
      "Parameter: 162bert.encoder.layer.9.output.dense.bias ----- False\n",
      "Parameter: 163bert.encoder.layer.9.output.LayerNorm.weight ----- False\n",
      "Parameter: 164bert.encoder.layer.9.output.LayerNorm.bias ----- False\n",
      "Parameter: 165bert.encoder.layer.10.attention.self.query.weight ----- False\n",
      "Parameter: 166bert.encoder.layer.10.attention.self.query.bias ----- False\n",
      "Parameter: 167bert.encoder.layer.10.attention.self.key.weight ----- False\n",
      "Parameter: 168bert.encoder.layer.10.attention.self.key.bias ----- False\n",
      "Parameter: 169bert.encoder.layer.10.attention.self.value.weight ----- False\n",
      "Parameter: 170bert.encoder.layer.10.attention.self.value.bias ----- False\n",
      "Parameter: 171bert.encoder.layer.10.attention.output.dense.weight ----- False\n",
      "Parameter: 172bert.encoder.layer.10.attention.output.dense.bias ----- False\n",
      "Parameter: 173bert.encoder.layer.10.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 174bert.encoder.layer.10.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 175bert.encoder.layer.10.intermediate.dense.weight ----- False\n",
      "Parameter: 176bert.encoder.layer.10.intermediate.dense.bias ----- False\n",
      "Parameter: 177bert.encoder.layer.10.output.dense.weight ----- False\n",
      "Parameter: 178bert.encoder.layer.10.output.dense.bias ----- False\n",
      "Parameter: 179bert.encoder.layer.10.output.LayerNorm.weight ----- False\n",
      "Parameter: 180bert.encoder.layer.10.output.LayerNorm.bias ----- False\n",
      "Parameter: 181bert.encoder.layer.11.attention.self.query.weight ----- False\n",
      "Parameter: 182bert.encoder.layer.11.attention.self.query.bias ----- False\n",
      "Parameter: 183bert.encoder.layer.11.attention.self.key.weight ----- False\n",
      "Parameter: 184bert.encoder.layer.11.attention.self.key.bias ----- False\n",
      "Parameter: 185bert.encoder.layer.11.attention.self.value.weight ----- False\n",
      "Parameter: 186bert.encoder.layer.11.attention.self.value.bias ----- False\n",
      "Parameter: 187bert.encoder.layer.11.attention.output.dense.weight ----- False\n",
      "Parameter: 188bert.encoder.layer.11.attention.output.dense.bias ----- False\n",
      "Parameter: 189bert.encoder.layer.11.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: 190bert.encoder.layer.11.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: 191bert.encoder.layer.11.intermediate.dense.weight ----- False\n",
      "Parameter: 192bert.encoder.layer.11.intermediate.dense.bias ----- False\n",
      "Parameter: 193bert.encoder.layer.11.output.dense.weight ----- False\n",
      "Parameter: 194bert.encoder.layer.11.output.dense.bias ----- False\n",
      "Parameter: 195bert.encoder.layer.11.output.LayerNorm.weight ----- False\n",
      "Parameter: 196bert.encoder.layer.11.output.LayerNorm.bias ----- False\n",
      "Parameter: 197bert.pooler.dense.weight ----- False\n",
      "Parameter: 198bert.pooler.dense.bias ----- False\n",
      "Parameter: 199classifier.weight ----- True\n",
      "Parameter: 200classifier.bias ----- True\n"
     ]
    }
   ],
   "source": [
    "# We can check whether the model was correctly updated\n",
    "for index, (name, param) in enumerate(model.named_parameters()):\n",
    "     print(f\"Parameter: {index}{name} ----- {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 285
    },
    "executionInfo": {
     "elapsed": 24489,
     "status": "ok",
     "timestamp": 1719387042249,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "QyleqOHICBjj",
    "outputId": "f2dc589c-1b8f-40e7-ef93-3ba0f3d3a3a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\2122135607.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 12:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.472500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4012673795223236,\n",
       " 'eval_f1': 0.8140417457305503,\n",
       " 'eval_runtime': 55.2558,\n",
       " 'eval_samples_per_second': 19.292,\n",
       " 'eval_steps_per_second': 1.213,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model_id = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Encoder block 10 starts at index 165 and\n",
    "# we freeze everything before that block\n",
    "for index, (name, param) in enumerate(model.named_parameters()):\n",
    "    if index < 165:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Trainer which executes the training process\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJRMWsLdA913"
   },
   "source": [
    "### [BONUS] Freeze blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ADvKJjNaFAot"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 21:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.406000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 21:03, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.408300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:40]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 19:10, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.409900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 18:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.411000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 18:15, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.413000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 17:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.416800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 17:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.421800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 17:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.429100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 16:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.444900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:41]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 15:59, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.468900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:44]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 15:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.509500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3529723155.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='534' max='534' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [534/534 14:43, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.663700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#This code is experimenting with gradual unfreezing of the transformer layers in a BERT model, to see how different layers affect performance on a classification task.\n",
    "'''Training with Layer-Freezing Strategy:\n",
    "The loop runs 12 iterations (from index = 0 to index = 11), progressively freezing more layers in the model during each iteration.\n",
    "Layer Freezing Logic: In each iteration:\n",
    "It re-loads a fresh version of the BERT model (bert-base-cased).\n",
    "Freezes the layers of the model based on the current index (which corresponds to the layer number being unfrozen).\n",
    "Layers that have an index less than or equal to the current index are frozen.'''\n",
    "scores = []\n",
    "for index in range(12):\n",
    "    # Re-load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "    # Freeze encoder blocks 0-index\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"layer\" in name:\n",
    "            layer_nr = int(name.split(\"layer\")[1].split(\".\")[1])\n",
    "            if layer_nr <= index:\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # Train\n",
    "    trainer = Trainer(\n",
    "      model=model,\n",
    "      args=training_args,\n",
    "      train_dataset=tokenized_train,\n",
    "      eval_dataset=tokenized_test,\n",
    "      tokenizer=tokenizer,\n",
    "      data_collator=data_collator,\n",
    "      compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate\n",
    "    score = trainer.evaluate()[\"eval_f1\"]\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1712321357732,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "dWYlHFNdLQtk",
    "outputId": "a28d645b-7633-4bdf-cc51-b7a937d36508"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8477443609022557,\n",
       " 0.8431924882629108,\n",
       " 0.8428974600188147,\n",
       " 0.839622641509434,\n",
       " 0.8398104265402844,\n",
       " 0.8387096774193549,\n",
       " 0.8358490566037736,\n",
       " 0.832391713747646,\n",
       " 0.8320754716981132,\n",
       " 0.8174904942965779,\n",
       " 0.7953667953667953,\n",
       " 0.7049742710120068]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "executionInfo": {
     "elapsed": 1383,
     "status": "ok",
     "timestamp": 1712388601684,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "bf3PIvKhOBJ-",
    "outputId": "aadf0ff7-e4ab-4d62-87b5-cbf1ecdea8da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.5-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.59.1-cp313-cp313-win_amd64.whl.metadata (111 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp313-cp313-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\aitra\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aitra\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\aitra\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aitra\\anaconda3\\envs\\llm\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aitra\\anaconda3\\envs\\llm\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.5-cp313-cp313-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 2.9/8.1 MB 16.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 7.9/8.1 MB 25.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.1/8.1 MB 13.3 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.3.3-cp313-cp313-win_amd64.whl (226 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.59.1-cp313-cp313-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 26.9 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.9-cp313-cp313-win_amd64.whl (73 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   ------------- -------------------------- 2/6 [fonttools]\n",
      "   -------------------------- ------------- 4/6 [contourpy]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   --------------------------------- ------ 5/6 [matplotlib]\n",
      "   ---------------------------------------- 6/6 [matplotlib]\n",
      "\n",
      "Successfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.59.1 kiwisolver-1.4.9 matplotlib-3.10.5 pyparsing-3.2.3\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAGJCAYAAABo5eDAAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAenxJREFUeJzt3Qd4FNXXBvCTXiCF3pv0HnqVIkgVwYKAIogKoqgoioIgCAooKGJBsAD+sYHYFekC0kF67723FBLS53veq7PfZrPpye7M5P09z8DuZHf23pktZ+6ce6+HpmmaEBERERFZlKe7C0BERERElJcY8BIRERGRpTHgJSIiIiJLY8BLRERERJbGgJeIiIiILI0BLxERERFZGgNeIiIiIrI0BrxEREREZGkMeImIiIjI0hjwklvcunVLnnzySSlZsqR4eHjICy+8oNZfvnxZHnzwQSlSpIhaP2PGDDF7ncic1qxZo44j/jeaihUrymOPPZYn2z516pSq97vvvpsn27cSHAMci+x444031H62On4vklEw4KVc8+WXX6ovtLSWzZs32x47efJk9finn35avvrqK3n00UfV+hdffFGWLVsmo0ePVuu7dOmS6+XEa//yyy95sl1ndXIGP5Jp7afY2FixYuCY1rJgwQLJ75zto8KFC0vz5s3lm2++cXfxTCe995v9YsSTGVcF6vb7ITg4WOrXry/vvfeexMXFue17kSgveefp1ilfmjhxolSqVCnV+ipVqthu//XXX+rHfPz48Skeg/U9e/aUl19+Oc/Khy9gtCL36tUrV7ebVp3SEhYWJi+99FKq9b6+vmJFzz//vDRp0iTV+hYtWrilPEbfR9evX5eFCxdK//79JTw8XIYNG+bu4pkGAit78+fPlxUrVqRaX7NmzRy9zueffy7JycnZeu7YsWNl1KhR4i5+fn7yxRdfqNt4f/3444/qe3fbtm25ehKa1e9ForzCgJdyXdeuXaVx48bpPubKlStSq1Ytp+tDQ0PFjNKqU1rKlCmjgpnMiomJkcDAQDGrO++8U51o5FfR0dFSoECBLO0jtIrdcccd8u233zLgzQLHzxWuLiHgzejzltXPmI+PT7bL6O3trRZ3wWvb749nnnlGmjVrpk6ypk+fLqVLl872tnESEB8fL/7+/ln+XsxIYmKi2r5VGwYo7zClgdxy6fbkyZOyePFi2yU1PR1C0zSZOXOmbb0OLRDI/SpXrpxqmUBr8TvvvJOqdQX3P/jgA6lbt676si1WrJhKi/jnn3/U37FNBB7/+9//bK+RUS4kvrCfeOIJKVGihNomLv3h+RnVCbmQ2dWuXTupU6eObN++Xdq0aaN+hF977bVMlUd/flqXcbGvs7Jf7XM6P/vsM6lcubJ6LFoi0RqUm/A6zz77rEo5Qf3xOrVr15alS5emeuz58+fVfsAPMx6HqwoIEPFDqztx4oT07t1bpQdgH6KlCcfI0blz51SLPwLS4sWLq9SatC7tbtmyRb2nQkJC1Dbbtm0rGzZscJqfeeDAAXn44YelUKFC0rp16yzvD/yo47mZCYwyW1ekzKB81apVU++fUqVKyf333y/Hjx9Pc9v4XA4ZMkSV56efflLrEhISZMKECVK1alW1HeTdo44ILHOjrPrn6vvvv5dJkyZJ2bJl1et06NBBjh07JjmV3mfs119/le7du9veW3jPv/nmm5KUlJRuDm9WPivOcniz8v7H/kHDAvYJXufTTz/NUV6wp6en2id6PQCfAbTM4nsBZcH3xCuvvJLqs6GXG+k3KC8eizKn972Yme8x+/2J/hz6/sTnSq/rkSNHVOCOzyO+719//XX1fj179qy6Woh0DeQPI13DHr4nxo0bJ40aNVLPxWcfJ5yrV69OswyZ+f47dOiQPPTQQ6osAQEBUr16dRkzZkyq767HH39c1V0/xnPnzs3WcaPMYwsv5bqIiAi5du1ainX4wsAPIi4h4rIiAgr8gOmX9Bs0aGDL77r77rtlwIABKVpdEFTgS+Kpp56S8uXLy8aNG1We78WLF1N0bMMXKAI6tDKjowRaA9atW6daePDjgNfA+qZNm6ofcMAXWFpu376tfgTwA4svdARVixYtUj90CBaHDx+eZp3whZceBAyO+wk/unoLEy5pox59+/ZVX+j4csxMeQBfsKinva+//lrlRyOgy+p+BbQyRkVFqcfieE6dOlUFSgheMtPShec61hf0Doq69evXq6AKLU5BQUHy4YcfygMPPCBnzpxRj4ULFy6oY4g64zjWqFFD1eOHH35Q9UJghg6QLVu2VPeRKoDn4gf13nvvVY+77777bMcYQRS2j8chyMHxxKVYR1iHY4IfSQQCCBLmzZsnd911l3qfoUz2ENQhIEQaDX6Es7KPbty4ofb5vn37ZM6cOek+L7N1RcB2zz33yKpVq9T7Cu8XvCaCVLyOs88CnoMfZ7T8/fzzzyoQBAQcU6ZMsX2eIiMj1Ynljh071Gc4p2XVvf3222o/43I7vlvwvnvkkUfUiUdOOfuMAb5DChYsKCNGjFD/47gjOEIdp02bluF2c/JZycz7f+fOneqkCycrOOnAMUIqWUbfORnRT3rwOjjpxTFBefAZw/fc3r175f3331dBpmM/COwjnJzge6lo0aKqbGl9L2b2e0yHzxhO1FAOBIg4UdL16dNHlQ3vEwTWb731lvo7TgDwucQJPAJxvH8QpOLkBnAskdLRr18/GTx4sDpe+Jx17txZtm7dqlLOsnpM9+zZo4Jm3EdZcTKEffr777+rkzb9/Y8TPP0kAftjyZIl6rcLZWKnvjykEeWSefPm4Rfd6eLn55fisRUqVNC6d++eaht47LBhw1Kse/PNN7UCBQpoR44cSbF+1KhRmpeXl3bmzBl1/6+//lLPf/7551NtNzk52XYb2xo4cGCm6jRjxgy1za+//tq2Lj4+XmvRooVWsGBBLTIyMsM6OYPHOttP48ePV39v27atuj979uxsl8fehg0bNB8fH+3xxx/P8n49efKkes0iRYpoN27csD3u119/Vet///33dOu6evXqNN8XWC5evGh7LO77+vpqx44ds63bvXu3Wv/RRx/Z1g0YMEDz9PTUtm3bluaxfuGFF9Tz1q1bZ/tbVFSUVqlSJa1ixYpaUlJSin36/fff2x4XHR2tValSRa1H+fXtVq1aVevcuXOK91NMTIza5t13321bh+OI5/br1y/dfZPRPkIdJ02a5PT9Y/8ezmxd586dqx43ffr0NPebfrynTZumJSQkaH369NECAgK0ZcuWpXh8/fr1M/1+t5fZsur7pGbNmlpcXJztsR988IFav3fv3ky/Jr5THH/u0vqM6cfU0VNPPaUFBgZqsbGxtnU4BjgWuqx8VvT3iL3Mvv979OihynL+/HnbuqNHj2re3t6ptukMyo3P/tWrV9WC15s8ebLm4eGh1atXTz3mq6++Uu8/++ME2F94DXyn2Jcbj92/f3+q13L2vZjZ7zF9fwYHB2tXrlxJsQ19/w0ZMsS2LjExUStbtqyqx9tvv21bf/PmTfUetv/M4LH27yv9cSVKlEjxPZmVY9qmTRstKChIO336dIrt2n9fPPHEE1qpUqW0a9eupXhM3759tZCQEKfvPcodTGmgXIeUBLQY2S84g80unPnjrBmXdtH6pS8dO3ZULRt///23ehw6XeCs2VnniOxe5vvzzz/V5TC0Auhw9o6WKQy3s3bt2mzXC/lyjvvJvmUbLRmDBg3KcXkuXbqk8kLRYvHJJ59keb/at6TgsTo8F9DCkRloIXOsLxb71hrA69u3NNarV09dltRfBy1PaF3q0aOH01xx/VhjX6Hl0T6VAK11aHnBZUpcFtUfh9Yo+9xZtLLrVwB0u3btkqNHj6oUBbQM6vsLKTJoIcb+ckyxGTp0aKb2jbN9hBZVHGe01iNNJz2ZrSs+I2h9e+6559Lcb/aXfNFC/ccff6jtd+rUKcXfkWu/f/9+tU+yIrNl1eEzYJ+vmdX3XXqcfcYAl6IdW93xumiVxiXrjOTks5LR+x+fzZUrV6oUHPs8W6QdoLU6s/C+ResiFjwX6RzoQIpWfP37AS2nuHpi//2AVlNwvPSPq0WZzdXN6vcYWrjTar22v5Ll5eWlvhMQg6PF1P69itQC+/2Px+rvK3xucUUFVwTxfFylyOoxvXr1qvoOwNUQXC1z9tlCufAZxHcXbtvvV7Qs4wqGs9em3MGUBsp1+DHLqNNaVuAHFZeK0vrCQy4Y4NIRfgAcA6icOH36tLokjUuqznp34+/ZhcADP27pdWpz7JiR1fLgCxz5ZPiRxGVS/MBndb/qHL/E9S//mzdvSmYgrzq9+qb1Ovpr6a+DHxZc+kOOY3qwL3BS4ch+X2Eb+B8/+I4BH34g7emB3cCBA9N8Tfxg2f8oOhutJCv7CMcO20RvfgTaaR2rzNYVnxHUKzM5wUhXQPCBk1U9t9MeLqEjRxK5wNg2LrEjJQkBWnoyW9bcet+lx9lnDBDIYxQFXKbHe80ejkdGclLmjN7/+FwiJcB+1Buds3VpQd4sLrWDngOP1AP79/vBgwcz/f2Qlfd6Vr/H0tu24/5CPi7qhu9Xx/U4UbWHVBrk9uIkBilm6b1eRsdUD3zT+17CdxdSNpALjCUz+5VyDwNeMjycfSMnEJ0lnMEPrhXZtzJl18iRI2XTpk2qRcj+xyw7+xUtIs5kJjc1K1z1Olmlt94ih9Mxv8++pTK3jyFaj9HKirxCPX/WFdDihI5HyFVEwIsgwh5yIRFAo4PX8uXLVT4k8jtnz56dKn/cqO8HZ8cHAQlaK9GqiqAera2oO1reXn311UwNQ5aTMrvyc5beCSjqiRMwjNjgDDqw5fZ7PS3pbdvZ/srMPkSfBuQMo6Uc35Po24Dn4UTPWQfO3Dgu+nsH+eJpnThndMJI2ceAlwwPPzhoacqodRCPQ6csXJpKr5U3K+kNFSpUUK2g+KKyb43QL2vi766UlfJgLE10PMOCH/Ds7lejQYsTghF0skoP9sXhw4dTrXfcV/gf28IPl/17w/G5+mVmvLYr9xla6QHHKqd1RR3Q2QutWRl1nkLHGqRkoJMbUhtwqduxZRifM6QEYEH5EASjM1t6AW9my+ouGP0ALYG4IqJ3cAKMNmAECMwQgDsbqSI3Rq/Q4b2ye/dudcKV2zPCGeF7FR0kMeQfjrN9/bI7XjC2Bel9L+G7Cx0RccXNbN+7VsAcXjI8XNZFKyWCWWetMXpAgDwvBC3otZzeWTiGn8HzMqNbt24qBxb5lDq83kcffaRa85wFknkps+XBly6CDrQkOPZ4zup+NRr8QKJVBpdj9eHmnB1r7Cu0iqKO9nmLuJSI3tN6viEeh1Ef8AOoQ66m4yVHjMyAIADDEzkLPnG5Mi+gdRcwbFNaMltXfEaQL/jxxx9nqqUKP8o4cUJLL9IV7Fs3HS8P4/2HS+oZzdSV2bK6i96SZ78/kM9sn//uTnrLLPLY8b61D3Zz0lfC2fcDRj7B5BqOkFKBY5ZdRvhedXaccTJo/77MCgSzOEHC8GIYUcOe/hp4TXwGkcfrLDDOq+8Q+hdbeCnX4UvXWccODEWknwVnBS43/fbbb6qlCZegEHjgyxZD5CBIQUcX5Gu1b99e/ShjGB/knyGnED/QGC4Kf8MQMIDn4xK/Prg68rWc5RQCOtJgeBu8LsbrxA8yXhPjrqLlFGfrrpTZ8ugdcfAFjEt3zo5DZvdrbsFxcDZtMi7hZfUyHob5wmV0/DDqQyZhKDV0tMEwSuikgrzX7777TnXkQWcYtEYiZw8tdfjB0VuWMCQRAkB0GMQ+1YdTcpyAAI/HZXtsD+NmYh8jBxRBATrwoOVXz4nMjX2EKxU4PujAg2Gz0HkoLZmtK+qIWccw3BaCTnS8wTHH5wHDYCEn1xFOLjAsFJ6LOuL9BwhMkeqA9w1eDycfeN/on7OcltVd8PlAfiYuOaN8aP3D+8HdKTX20IqO93+rVq3U2NNoMcR7GPmj6FyZG/BdimHG0MqP9zdeC6+D73asx4lydvtqGOF7Fd97aN3FMHhIFcL7D+k4eF+ndzUlPfjtQWfMhg0bqjritwXfoxguTT8uGD4N+xO/Ofjuwevhs46UGXwOcZvySC6N9kCU7rBkWPD37AxLpg9bNHr0aDVUFIbtKVq0qNayZUvt3XffVcPZ2A81g+GUatSooR5XrFgxrWvXrtr27dttjzl06JAaPgbD1OD1Mhqi7PLly9qgQYPUa2KbdevWTVGXjOrkTEaPxZBJtWvXznZ50hr2zPE4ZGa/2g9T5ch+KLXsDktm//y0jr/jMFyAoX8wPBmOMYa9u+OOO9Rz7YcaOn78uPbggw9qoaGhmr+/v9a0aVPtjz/+SLV9bOvee+9VQz1hHwwfPlxbunRpimHJdDt37tTuv/9+NUwRXhdle+ihh7RVq1alGjIJQz5lhrN9hOOB9zGGJbN/j6e1PzJbVwx7NGbMGDUMGIaqK1mypHoenp/e8f7kk0/U+pdfflndf+utt9Rr4PXwWUqrrM5kpqz6Plm0aFGK9Xr5nH0GszosWVqfMQy51bx5c1Wv0qVLa6+88ooals3x/ZDWsGSZ+aykNSxZZt//eL81aNBAvU8qV66sffHFF9pLL72k9mdmhyXLCI7lO++8o/YT3uuFChXSGjVqpE2YMEGLiIjIsNzpfddl5nssvf2Z1mcsrbo5Hm8MFYah2FA+1A37Eu/BnBxT2Ldvn3bffffZ3tvVq1fXXn/99VR1x/4qV66c7TPYoUMH7bPPPnO6Dyl3eOCfvAqmiYiIyDXQGp+doeKI8gPm8BIREZkM8mjtIcjF+LbOhpAjIhG28BIREZkMcs2RA4t8fIxbO2vWLNVhENMOY4xbIkqJndaIiIhMBp1y0fkPox1g4gjMkobOnAx2iZxjCy8RERERWRpzeImIiIjI0hjwEhEREZGlMYfXCUxWgBlsMPh1bk+pSEREREQ5h6zcqKgoNYlURpPWMOB1AsFuuXLl3F0MIiIiIsrA2bNnpWzZsuk+hgGvE/q0htiBmEozryVrmoRHREloSJB4mrxF2Up1sVp9WBdjslJdrFYf1sWYrFQXq9Un2cV1iYyMVA2UmZmOmgGvE3oaA4JdVwW8SZqHBAdb481ulbpYrT5WqYsWGy/hQ2dJUHKSBH06TLwC/MTMrHJcrFgf1sWYrFQXq9Un2U11yUz6KQNeIjIXTST59BXbbSIiooxwlAYiIiIisjQGvERERERkaQx4iYiIiMjSGPASERERkaUx4CUiIiIiS+MoDURkLh4iniVC1YyIuE1ERJQRBrxEZCoe/r4S8t3LcjMiSt0mIiLKCFMaiIiIiMjSGPASERERUbZpmibnz5+XZcuWyeI/fhcjYkoDEZmKFpcgkcM/F0lKEu2jp0SY1kBE5DLh4eGyb98+tezdu1ctuH3z5k3194oVK0r/Rx4Wo2HAS0TmkqxJ0uHztttERJT7YmNj5dChQ7aAVg9uz5075/Txnp6eUq1aNales6bqVOzp5SVGwoCXiIiIKJ9KSkqSEydO2IJa/f+jR4+qvzlTtmxZqVu3rlrq1Kmj/q9Ro4b4+vmpDsUIfo2GAS8RERFRPsizvXTpUqpUhP3798vt27edPqdQoUIpglr8jyU0NNTp45M14151Y8BLREREZCGRkZFO82yvX7/u9PH+/v5Sq1atVMFt6dKlxcPDGgOeM+AlIiIiMqG4uDg5fPhwqjzbM2fOOH28p6enVKlSJVU6QuXKlcXLYDm3uY0BLxEREZGBoRPYqVOnUrTW4v8jR45IYmKi0+eUKVPGFtDW/S+4rVmzpgQEBEh+xICXiEzHIyRQ5aMREVnN5cuXnebZRkdHO318SEhIitZaLLVr15bChQu7vOxGxoCXiEzFI8BXQn9+7d+phQM4Bi8RZR1OmNFqigUjEaS3ZOYxmX2cs8egw9ievfvk6JHDKri9evWq0zL7+fmpFlrH4BYtuVbJs81LDHiJiIjIkEEpgr+TJ0+qy/n6cvLUKTl/7rxoWnK2A1D8zagQvCKn1jHPFrm33t4M27KLe46IiIjcFtDaB7OOS1rDZbkCOnGhkxf+T2/J8WO8vaVM2XLSuFFDqVe3rhotITAw0G31tioGvERkuqmFo179UiQxSbR3H+fUwkQGDmivXbvmNJDVW20zCmjR2olL9piuVl/KV6ggIaGFpXChEPHx9s5WAJrR41w5cQLGrkWKVqGQIPFkaoJ1A96ZM2fKtGnT1GDI9evXl48++kiaNm2a5uNnzJghs2bNUkNuFC1aVB588EGZMmWKGkPO0dtvvy2jR4+W4cOHq+cRkQUka5K4+5TtNhG5L6DFuK72AazjEhMTk2FAi7FeK1WqlCKo1Zdy5cqJr2/Kk1oGiGS6gHfhwoUyYsQImT17tjRr1kwFpZ07d1ZjyhUvXjzV47/99lsZNWqUzJ07V1q2bKmG43jsscfUB2b69OkpHrtt2zb59NNPpV69ei6sERERkfUC2rSWtEYOcAxonQWzekCLzlhElg54EaQOHjxYBg0apO4j8F28eLEKaBHYOtq4caO0atVKHn74YXUfH5Z+/frJli1bUjzu1q1b8sgjj8jnn38ub731lotqQ0REZK6A9saNG+mmHGQU0IKzgFZvsWVAS5LfA974+HjZvn27SjnQIWemY8eOsmnTJqfPQavu119/LVu3blVpDydOnJA///xTHn300RSPGzZsmHTv3l1tKzMBL2YqwWI/JZ9+2cQV80Lr44mqYVLE3KxUF6vVxyp1sR9/V9XF5OPxWuW4GKk+eG18p+NyOgI2+8Vx3W3Hx8TEpHhMfHyC+Ph4i5enp/qN8vjvf3Xbw8N22zOz6538Lavbycpz9PLiwv+Va9flyqWLcur0aTn9X2CLBqKMlCpV6t9AFkFshQpSwaGF1llKob3c/owa4T2Wm6xUH83FdcnKe8ttAS8S2TE0SIkSJVKsx/1Dhw45fQ5advG81q1bq52J2UWGDh0qr732mu0xCxYskB07dqiUhsxCDvCECRNSrQ+PiJIkzXX5QeGRGX/xmIWV6mK1+pi+LrHxtpsRUbdEEv7/vpmZ/rhksT4JCQkSExMtMdExEnP79v/fjolWHZmi/7uN4BPLbT0Q/e9xeIzt7/pj1Trcjzb0sFNGU6JkSSlfrryUK19eype3/7+ClC1bNt2A9nZcglrcIb99Zswk3EV1iYqMMk+ntaxYs2aNTJ48WT755BOV83vs2DHVIe3NN9+U119/Xc6ePavur1ixIsMzTntoZUYusX0LL85aQ0OCJDg4SPIagne8OUKDC5p+8Ggr1cVq9bFKXTTfeAn/73ZIUEHxDDT35VIzHxe0Dp4/f17OnTunvn/PnzsnFy5ckGvXb0hCfNy/AWgaLawIeF0BHZ4KFCigFgz1ZLttf99+/X+31d8DAiQuIVEC/HwF7Uj6RAX2i/0EBup+GuuT83K93e00H5+cLJ5e3lK1apUUKQcIbLPye2kEZv7MWL0+movr4uVhghZejLCAoT8whZ493C9ZsqTT5yCoRfrCk08+qe5jIGZ8cQ4ZMkTGjBmjUiSuXLkiDRs2tD0Hrch///23fPzxx+oSF17TEfKLnOUYqUtPLjhgejuEfrnLzKxUF6vVxyp10VB2fx9BBGL2uhj1uOBHCyf+CGTTW8LD9VOP7MN3srOANCvr0ntMTgbqt9JoAJaqiwE/Mzlhpfoku7guWXkNtwW8OOtu1KiRrFq1Snr16qXW4SwU95999lmnz0HLgOPYeHoAiy/oDh06qHmn7aFDXI0aNeTVV191GuwSkblgOuFCf47n1MI57KiUUTCbmdxOCA4OVpe99QUdmHz8/KVokcISVLBghkEpfgvM3qpFRMbn1pQGpBEMHDhQGjdurDqhYVgytNjqozYMGDBADTiNHFvo0aOHGtmhQYMGtpQGtPpiPYLZoKAgNQWfPXypFilSJNV6IiKrQaMBZq7KKJiNjY3N1PYKFy6cIph1XPD9jIDXqi2JRGQdbg14+/Tpo76cx40bpyaeCAsLk6VLl9o6smFyCfsW3bFjx6qWAPyPvLFixYqpYHfSpElurAURUd5Deha+J9MLZPG9mNncWHx/oq9CesEspzclIqvw0OzH+CEFuWshISESERGRqvUiL1ipRcRKdbFafaxSFy0+QSLHfatGaQmd9Kh4+fma/rhcuXZDbkdHyYX/OoA5Wy5evKiC3oygUQD9INJrmUXaQV51VLLK+wxYF2OyUl2sVp9kF9clK/GaqUZpICKSJE0Stxyx3TYDtCugQy3GDtcXDOyv30ZAm5m2B6RuIVhNL5jFmKk+Pj4uqRcRkVkw4CUiygUYF9Y+iLW/jQWdbtODIDW9QBYL0r3Y+ZaIKOsY8BIRZbJDGNIK0gpo8beMUg0QtN5xxx0pFsxeFVq4qFSrcod4M5glIsoTDHiJiP4TFRVlC2QdA1rct5+C3BnkkDkGtFgwyH+FChWcjvet57w5DrlIRES5hwEvEeUb6PSFfFnHYFZfMGpMepBOgJmp0gpqMYwXx5QlIjIeBrxEZCmY/ctZCy2W06dPZzhsF4LWtAJaDOPFDmFERObDgJeIXAqjESAfFsOKocU1o8XxcYnRsXIj8oJEJsbJ1blfyKlzZ1MEtTdv3kz39RGwVqxY0WlAiyU0NNRl+4KIiFyDAS9RPoBpYmfNmiV79+8Xb09PFXBmFFhmNgDN6mPw2rnmue+cri5evHiarbSYUIEjHRAR5S8MeIksDAHmvHnz1BTcmKXLDJADi4DUfvH29k61LiAwUKpUqSKVnbTSFixY0N3VICIiA2HAS2RRy5cvl5dffln27t2r7iMgfKD3QxISVDBVAOksoHS2uOJxmen0ZaWZiYiIKO8x4CWymP3796tAd+nSpeo+clLRwvv0M89ITGy8JaYWvjVpkUhComjj+8r6yNvS9u/dUj0oQB4uV1w6FC8kTQoFibeneetIRES5iwEvkUVcvnxZxo0bJ1988YXKk0XnrGHDhqlgFyMPoFUUAa/pJWmS8Pd+2+3awYGC+P1g1G0Zd+C0vH7gtAR6eUq7oiHyQNliMqhCCQ4VRkSUz3GkcyKTw5S1kyZNUvmsn332mQp277//fjlw4IC8//77Kti1skJ+PvJkxZKCkFb7b11MUrL8efmmPLfrmMQn62uJiCi/YsBLZFIIbL/66iupXr26jB07Vo3E0KRJE/n777/lxx9/VAFwfvH0HaVtwa4OAfD3zWqKnxe/5oiI8jv+EhCZ0Jo1a1RwO2DAADVzGGb/+uabb2Tz5s1y5513Sn5TP7SgNC5UMMUXWpWCAdKySLAbS0VEREbBgJfIRA4fPiw9e/aU9u3by44dOyQoKEimTJkihw4dkocfflg8PfPvR/q5ymVEH+H30fLF5UZ8grRas1tORce6uWRERORu+ffXkchErl27Js8995zUqVNHfvvtNzV81zPPPCPHjh2TUaNGSUBAgOR3vcsWlZJ+PtK/XHH5X+PqsrFdmMQlJ0uLNbtkx80odxePiIjciAEvkYHFxsbKtGnTVD7uxx9/rGYwu+eee9TYujNnzlQzitG/Ary85ETXpjK/SXU1KkO1oEDZ1C5Mygf4SZu1u2XJpRvuLiIREbkJA14iA9I0TRYsWCA1a9aUV155RSIiIiQsLExWrlwpv//+u1qfb/n7SOjicSLfvahuOwa99kOQFff3ldVt6qmxeXts3Cefn7zohgITEZG7MeAlMpiNGzdKixYtpF+/fnLq1CkpXbq0mh74n3/+kQ4dOkh+h4DWI8BXxN83U+PrBnp7yU8tasnQSqVlyI6jMnb/SXVCQURE+QcnniAyiOPHj6t83B9++EHdL1CggGrdfemll9Rtyj4vDw/5KKyyVCzgJyP3npTTMXEyp1E18c3HnfyIiPITBrxEbnbz5k1566235KOPPpKEhAQ10sLjjz8uEydOlFKlSrm7eIajxSdK9PRfROITRHv1QRG/lGkNaUFr8MvVykm5AD8Z8M9huXA7XrX8hvjwa5CIyOrYvEHkJvHx8fLBBx+oDmnTp09Xwe7dd98tO3fulM8//5zBblqSkiV+2U6R1fvU7azqU664rGhdV3aE35LWa3bJ2RgOW0ZEZHUMeIlcDPmjP//8s9SuXVteeOEFuXHjhrq9ZMkSWbZsmdSrV8/dRbS8NsVC1bBlUYlJ0nz1LtkdfsvdRSIiojzEgJfIhbZt2yZt27aV+++/X42hi2HFPv30U9m1a5d06dIlU52wKHfUDA6Uze0bSEl/X7lz7W5Zcfmmu4tERER5hAEvkQucPn1aHnnkEWnatKmsW7dO/P39ZcyYMSroHTJkiHh7M4/UHRDsrm1bX+4sGiLdNuyTL09dcneRiIgoD/BXligPRUZGqql/33//fYmLi1PrHn30UZk0aZKUK1fO3cUjESno7SW/tqgtz+w8KoO2H1EjOIyrWZ6t7UREFsKAlygPYEY0dDwbP368XL16Va1DKsN7770njRo1cnfxyIG3p4d82rCqVCzgL2P2n5LTMbHqvg+HLSMisgS3f5tjetSKFSuqS7zNmjWTrVu3pvv4GTNmSPXq1SUgIEC1kL344otq+lUdWtOaNGkiQUFBKj+yV69ecvjwYRfUhOjfDml//PGH1K1bV5555hkV7FarVk1++eUXWb16NYNdA0OL7ms1ystXTarL12euyD0b9ktkQqK7i0VERGYPeBcuXCgjRoxQrWA7duyQ+vXrS+fOneXKlStOH//tt9+qgfnx+IMHD8qcOXPUNl577TXbY9auXSvDhg2TzZs3y4oVK9RQT506dZLo6GgX1ozyI3Q869ixo/To0UMOHTokRYoUUWPr7tu3T3r27MlL5LnF30dCfhot8uVzqaYWzg39y5eQpa3ryOYbkdJm7W65cPvfVBQiIjIvtwa8GHt08ODBMmjQIKlVq5bMnj1bAgMDZe7cuWlOudqqVSt5+OGHVaswAllMv2rfKrx06VJ57LHH1DBPCKC//PJLOXPmjGzfvt2FNaP85Pz58+o93LBhQ/nrr7/E19dXRo4cqTqkPfvss+Ljk/tBWX6GEwfP0AIiIYF5dhJxV/FCsqFdmFyPT1DDlu2P5AkzEZGZebtz0H0EoaNHj7atwwxTaCHbtGmT0+e0bNlSvv76axXgorf7iRMn5M8//1SdgNISERGh/i9cuHCaj0FnIr1Dkd7RCJI1TS2uuAyu/5/1YfSNxUp1yag+t27dknenTVN5uTExMWrdQ336yOTJk6VSpUrqviveP/nx2LiiLrWCA9VYvfds3C+t1uySH5vXkvbFQnP9dax0XKxWH9bFmKxUF6vVR3NxXbLyG+u2gPfatWuSlJQkJUqUSLEe93E52Bm07OJ5rVu3VjsTHYOGDh2aIqXBXnJyshrYH63CderUSbMsyPudMGFCqvXhEVGSpLnuMnR4pHUGv7dSXRzrg/ftd99+I5MnvSVXLl9W65o0aSpvTposjZs0UfdvRkSJUZn+2CCvdu5f6mb443eJ5OHUwIEi8mvYHTJo7ynpun6ffFirnPQumfbJc74+LhauD+tiTFaqi9XqE+6iukRFRllzlIY1a9ao1rNPPvlEdXDDJePhw4fLm2++Ka+//nqqxyOXF/mT69evT3e7aGVGLrF9Cy86xIWGBElwcJDkNQTveHOEBhc0fZ6nlerirD7IC39l5EjZs2eP+jtacqe8/bY8+OCDhq+vVY6NdjtewpfuVLdDnr1HPAP98vT1CiFVqk09GbrzmDy9/4xc1zxkVPVyubYPrXJcrFgf1sWYrFQXq9VHc3FdvDxM0MJbtGhR8fLyksv/tZDpcL9kyZJOn4OgFukLTz75pLqPnvDojIaB+zGIP1IidMidRG/5v//+W8qWLZtuWfz8/NTiyBO5gi44YMn2uYkmf7NbqS729Tlw4IAKdJEjDqGhoTJ27Fj1PnP23jEiqxwbza7srqqLn5eXzG1UTSoG+svYA6flzO04mRlWVQ1nllNWOS5WrA/rYkxWqovV6pPs4rpk5TXc1mkNHXswRNOqVatSpCDgfosWLZw+B3mS9kEtIGh2zBtBEPLzzz+rDkR6LiVRduAEbMQLwyWsfn0V7GJGNFxVwNWFl156yTTBLuUcvsDH16qgAt+5py5Lz0375VZikruLRURERk9pQBrBwIEDpXHjxqoTGsbYRYsterzDgAEDpEyZMirHFjDcE0Z2aNCggS2lAa2+WK8HvkhjwPBlv/76qxqL99Klf6cKDQkJUWP3EmUWxs7FezAq6t8cofvuu0/eeecdqVq1qruLRm40qGJJKRPgKw9sPijt1u6WP1rVUVMUExGRcbk14O3Tp48amH/cuHEqMA0LC1OtaHpHNgwnZt+ii0vIaGXB/xgKqlixYirYxTStulmzZqn/27Vrl+K15s2bp4YrI8oIrhJMmzZNjfmM2/XDwtTJWLu2bd1dNDKITiUKy7q29aX7hn3SYvVOWdKqrtQIRhc3IiIyIg9NzwWgFJ3W0CKMIc2Cg4NdMqwGevUXCgkyf/6OyeuC4fKefvpp21jQuD1+4ltSrEghU9bHSsfGvtPajW7/jqoSuniceOVxp7X0nI2Jla4b9smF2/Hya8vacmfRkHx7XKxYH9bFmKxUF6vVJ9nFdclKvOb2qYWJjOLGjRtqpj8Eu7iy8OGHH8rHM2eqvF0iZ8oF+sv6tmESFlpAOq7bIwvPOp8lkoiI3Iu/5EQicvToUenevbv6v2DBgmrK6m7duhlq4gj6j5+3BH/7kkRGRavb7hbq6y1LW9eVJ7Yfkb5bD8nZ23HyUtWyph9eiIjIStz/a0FkgPGd77//frl586aUL19eDWeHIe/ImDw8PcWrZCGRAG912wh8PT1lfuPqUj7AT0buPSmnouPkg7DK4sWgl4jIEIzxa0HkJujM2KlTJxXsYuSPLVu2MNilbEGL7qQ6leTTBlVl9skL8sCmAxLDYcuIiAyBAS/lSxjz+dVXX5XHH39cEhIS5KGHHpLVq1enOekJGYeWkCgxs5eKfLla3TaaIXeUkt9a1JGVV25K+7/3yJXYeHcXiYgo32PAS/kOxnrGVMBTp05V9zGW83fffcdxms0iMVnivl8v8utWdduIupUqLGvb1pfTMbHSYs0uORp1291FIiLK1xjwUr5y4cIFadu2rZqJD7P9ffXVVzJx4sRUM/gR5VSjQkGyqX2Y+Hp6SIs1O2XT9Uh3F4mIKN/irzzlGzt37lQz+m3fvl2KFi2qprHu37+/u4tFFlapQIBsaBcmtYID5a6/98hP56+5u0hERPkSA17KF3777Tdp3bq1mqGvZs2aqnMa7hPltcK+PrK8dT25t3QReXDzAfng6Hl3F4mIKN9hwEuWhokE3333XenVq5fExMTI3XffLRs3bpQ77rjD3UWjfMTfy1O+a1pDXq5WVl7Yc1xe3H2cYzwTEbkQx+Ely8LoC88884x88cUX6v7QoUPV7Gk+Pj7uLhrlQ5hmc2rdO6RCoL88v+uYmqDiqybVxY/540REeY7ftGRJGFe3S5cuKthFh7QZM2bIJ598wmCX3G5Y5dLyU4ta8uelG9Jx3V65Fpfg7iIREVkeA16ynGPHjknz5s3lr7/+UtMEI393+PDhnOrVSlMLz3lO5IPHDTG1cHb0LF1UVrepJ0dv3ZbWa3fLqdtx7i4SEZGlMeAlS/n777/VjGlHjhyRcuXKyYYNG6R79+7uLhbl9tTClUqIlC9mmKmFs6NZ4WDZ1C5M3e6y7ahsvRHl7iIREVmWeX8tiBz873//k44dO8qNGzekSZMmaiSGevXqubtYRGmqXDBA1retL5UCfeWudXvktwvX3V0kIiJLYsBLlpgm+LXXXpPHHntMdVTr3bu3rF27VkqVKuXuolEewHTCt79cJbJgvSGnFs6qon4+8lODKtKlRCG5b9N++eT4BXcXiYjIchjwkqlhqLGHHnpIpkyZou6PGTNGFixYwGmCrSwxWWLnrxZZuMGwUwtnVYCXpyxsVlOeq1JGhu06Jq/uPcFhy4iIcpE5e3wQicjFixfl3nvvlX/++UeNvoARGQYMGODuYhFli5eHh8yoX1kqBvrJiD0n5ExMnHzZuLr4ebFdgogop/hNSqa0a9cuNU0wgt0iRYqoaYIZ7JIVvFC1rCxqXlN+uXBdOq3fKzfjOWwZEVFOMeAl0/n999/VtMDnzp2TGjVqqM5pd955p7uLRZRrHihTTFa1qSv7IqOl1Zrdcjo61t1FIiIyNQa8ZKppgqdPny49e/aU6Oho6dChg5omuHLlyu4uGlGua1kkRA1bFpecLM3X7JIdNzlsGRFRdjHgJVPA6AuYGvill15Sge+QIUNkyZIlUqhQIXcXjSjPVAsKVEFvuQA/abN2tyy5dMPdRSIiMiUGvGSKaYK7du0qn332mZot7b333pPZs2dzmmDKF4r7+6pZ2e4qHio9Nu6Tz09edHeRiIhMh6M0kKEdP35c7rnnHjl06JAUKFBAvvvuO+nRo4e7i0Xu5OstQZ8MlahbMep2flDA20t+blFbnt91TIbsOKpGcJhYqwKnyyYiyqT88WtBprRu3Tq577775Pr161K2bFnVWS0s7N+pWCn/8vDyFO8aZUUiotTt/DRs2cdhVaRioL+8su+knI6JlS8aVRNfE0+vTETkKvymJEOaP3++6pSGYLdx48aydetWBruU76FFd2T1cvJd0xqy8NxV6bp+n0RYYLY5IqK8xoCXDDdN8NixY2XgwIGqo9oDDzzAaYIpBUwnHLtgncjPWywxtXB29C1XXFa0ris7wm9J6zW75GwMhy0jIkoPA14y1DTBffv2lUmTJqn7o0ePlu+//14CAwPdXTQyksRkuf3ZMpH5aywztXB2tCkWKhvbhUlUYpK0WLNLdoffcneRiIgMy+0B78yZM6VixYri7+8vzZo1U5eu0zNjxgypXr26BAQESLly5eTFF1+U2NjYHG2T3O/SpUvSrl07WbRokRp9Yd68eTJ58mTxZH4iUZpqBv87bFkJP1+5c+1uWXH5pruLRERkSG6NJhYuXCgjRoyQ8ePHy44dO6R+/frSuXNnuXLlitPHf/vttzJq1Cj1+IMHD8qcOXPUNl577bVsb5Pcb8+ePWqa4G3btknhwoVl5cqV8thjj7m7WESmUCrAT9a2rS93Fg2Rbhv2yZenLrm7SEREhuPWgBezZg0ePFgGDRoktWrVUmOr4vL13LlznT4es2q1atVKHn74YdWC26lTJ+nXr1+KFtysbpPca/HixeqYnj17VqpVq6amCW7Tpo27i0VkKgW9veTXFrVlUIUSMmj7EZl48LSaoIWIiNw8LFl8fLxs375d5WnqcPm6Y8eOsmnTJqfPadmypXz99dcqwEWL4IkTJ+TPP/+URx99NNvbhLi4OLXoIiMj1f/JmqaWvKb/MOF/s2ckZrYu+PuHH3wgL7/8suqodtddd8n3ixapmdNcsc8zKz8eG6OzD+RUXQz0fnHncfH0EJnVoIpUCPSTsQdOy6noWHXfx8VpQVZ5nwHrYkxWqovV6qO5uC5Z+f53W8B77do1SUpKkhIlSqRYj/uYZMAZtOziea1bt1Y7MzExUU03q6c0ZGebMGXKFJkwYUKq9eERUZKkuW5g9/BI63Q6Sa8uGH1h9KuvyJfz/m117z9ggEx7d7qIp7fcjIgSI8ovx8YUYuNtNyOibokk/P99M8ut4zK0VCEpLJoMP3hWTkfFyJy6FSXI20tczfTvMzusizFZqS5Wq0+4i+oSFRllzYkn1qxZozoyffLJJ6oz2rFjx2T48OHy5ptvyuuvv57t7aJFGHm/9i286BAXGhIkwcFBktcQvOPNERpc0PQzJ2VUl/DwcHn0kX6ycsUK9fepU6fKiyNGGLbe+enYmIXmGy/h/90OCSoonoF+YmZ5cVyeCgmSqoVD5IHNB+S+XSfk95a1pXSAa/aTVd5nwLoYk5XqYrX6aC6ui5eHCVp4ixYtKl5eXnL58uUU63G/ZMmSTp+DoBbpC08++aS6X7duXYmOjpYhQ4bImDFjsrVN8PPzU4sjTw8PteQ1vdnfw0Wv5666IAUF0wSjwyHyqtEJsWfPnmJk+eXYmInm5yMFpz8ut6Jvi4efj6nrkpfHpWOJQrK+XZh027BXWq7ZLUta15HawQUkr1nlfQasizFZqS5Wq0+yi+uSlddwW6c1X19fadSokaxatcq2DrmcuN+iRYs0x2l1HKYKAa5+VpGdbZJrbNiwQbXKI9gtU6aMrF+/3vDBLhkTphP2CbtDpE75fDW1cHbUDSkgm9s3kEK+3tJqzS5ZfUVvGyciyl/c+muBNILPP/9c/ve//6lA6Omnn1YtthhhAQYMGJCiA1qPHj1k1qxZsmDBAjl58qSsWLFCtfpivR74ZrRNcr1vvvlGdUpDjjVOSNDpsEGDBu4uFlG+UCbAT9a1rS9NCwVJ5/V75ZszKa+AERHlB27N4e3Tp49cvXpVxo0bpyYeCAsLk6VLl9o6nZ05cyZFiy6mnEUzOf4/f/68FCtWTAW7+sxcmdkmuQ5a19944w2VYw3333+/zJ8/XwoUyPvLqmRdWmKSxP6+VeR2nGi9W4v4mKorglsE+3jL4lZ1ZMiOo9J/22E5ExMno6qXM32+IBFRZnloHKwxFXRaCwkJkYiICAkODnbJsBoYnaBQSJD583f+q4u/r7c8PmiQmhoYXn31VVPOnGbFY2P2umi34+VGt39HVQldPE68TN5pzZXHBV/3Ew6elgkHz8hTlUrJx2FVxBvjmeUiq7zPgHUxJivVxWr1SXZxXbISr7FphHIdZrUbNKC/mkTC29tbPvvsM6aUEBkAWnTfqFVRKgT6q9bec7fjZEGzmmriCiIiK2PAS7kKQ8V16tBBzp07q6YJ/vHHH6Vdu3buLhYR2RlUsaSUCfCVBzYflHZrd8sfrepISX9fdxeLiCjPmOv6Mhne+PHjVbBbtWpV2bx5M4NdIoPqVKKw6sx2ITZeWqzeKYciY9xdJCKiPMOAl3INZr5btnSpuj1n7lwV9BKRcYWFFpTN7cOkgLeXtFyzS9Zdi3B3kYiI8gQDXso1yNm9efOmFCpUSJo3b+7u4hBRJpQP9Jf1bcMkLLSAdFy3R74/d9XdRSIiynUMeCnX/Pnnn+r/9nd1sI2LTETGF+rrLUtb15WHyhaTPlsOyrtHzqoRHYiIrIKd1ijXA96Od3dyd1HIyny9pODkR9XUwrhNucPX01PmN64u5QP8ZOTek3I6Jk5m1K8sXiYfJomICBjwUq7ARCC7du1Swx7d1aGDu4tDFubh5SU+zauLRESp25R78PmdVKeSGrbsmV1H5WxMnHzbtIYEctgyIjI5pjRQrsBsdtC0aVMpWrSou4tDRDkw5I5S8luLOrLyyk25a90euRoX7+4iERHlCANeytV0hq5du7q7KJQPphaOW7pD5K+96jbljW6lCsvatvXlVHSstFi9S45G3XZ3kYiIso0BL+VYfHy8rFixQt3u2q2bu4tDVpeQJDFTfxL56E91m/JOo0JBsql9mPh4ekiLNTtl0/VIdxeJiChbGPBSjm3YsEGioqKkePHi0rBhQ3cXh4hyUaUCAbKhXZjUCg6Uu/7eI39cvO7uIhERZRkDXsrVdAZPT76liKymsK+PLG9dT/qWKyaLL95wd3GIiLKMozRQrgW83ZjOQGRZ/l6eMq9xdXcXg4goW3LUHHfs2DFZtmyZ3L79b2cGDlSe/5w6dUoOHDigJpq4++673V0cIiIiotwJeK9fvy4dO3aUatWqqVa9ixcvqvVPPPGEvPTSS9nZJJnUkiVL1P8tW7ZUUwoTERERWSLgffHFF8Xb21vOnDkjgYGBtvV9+vSxjcdK+QPTGYiIiMiSObzLly9XqQxly5ZNsb5q1apy+vTp3CobGVxsbKysWrVK3WbASy7j6yUFxvWV6BhOLUxERHnYwhsdHZ2iZVd348YN8fPzy84myYTWrl2r8rfLlCkjdevWdXdxKJ/AdMK+7eqItKphiamFJ7zxhtSsVkW8PD3ll19+cXdxLD91cnr7GH0S8BhMkw5r1qyRooVCJDw8XN3/8ssvJTQ0NFOv5bgtIjJhwHvnnXfK/PnzbffxoU5OTpapU6dK+/btc7N8ZJJ0BrwHiKzqscceU+9xLL6+vlKlShWZOHGiJCYm5mi7Bw8eVNt57/0Zcv7CBc5U6OZAsly5cqpPSp06dZz+HWl7R44cyZVtEZEJUhoQ2Hbo0EH++ecfNcvWK6+8Ivv371ctvJiEgPIH5u+SO2hJSRL/9wGRmNuidW4o4u2a0RW7dOki8+bNk7i4OPXeHzZsmPj4+Mjo0aOzvK2kpCQVtB0/flzd79qtuxQODRbPbJ44JiQkqLJQzmC0mZIlS6b594CAALXkxraIyAQtvDhjxVlu69atpWfPnirF4f7775edO3dK5cqVc7+UZDhHjx5Vw9LhRxYnP0QuE58k0RMXiLz7q7rtKkjXQgBToUIFefrpp9VINb/99pv6G4Lgl19+WaX3FChQQJo1a6Yuh+v0S+F4fK1atdS2Hn/8cenRo4f6e7HCoSqlAXC1DK2+6COBx4WFhaXoDKy3cC5cuFDatm0r/v7+8s0336hW6F69esnkyZOlRIkS6vX0VuiRI0dK4cKF1TYRtNt79dVX1Yg7SFO744475PXXX1cBtO6NN95QZfjqq6+kYsWKEhISIn379lWzK+r0K3xo+Q7w95f6dWrL5EmTbH8/e/asPPTQQ6pMKAd+N1CPtNy8eVMeeeQRKVasmAow0T9EL3elSpXU/w0aNFD7oV27dur+tm3b1NCIRYsWVWXEvtmxY0eqbaPVFS3p2C7q+8MPP2S69dgxpQH7Q2/5t1+cbQvvB9xHv4fGjRur/Y3RbQ4fPpziNd566y01a2VQUJA8+eSTMmrUKLX/icjFAS++CBHgXLlyRcaMGSPff/+9au3Ah7RUqVI5LA6ZrXW3TZs26ouZKL9BwIQrXPDss8/Kpk2bZMGCBbJnzx7p3bu3ahHGiaEuJiZG3nnnHfniiy/UFbEPP/zQFsTtP3REpTTABx98IO+99568++67aludO3eWe++9N8W2AIHQ8OHDVVoEHgN//fWXXLhwQf7++2+ZPn26jB8/Xu655x41ZOCWLVtk6NCh8tRTT8m5c+ds28HnF4EcxtPGa3/++efy/vvvp3gttEQj9/WPP/5QC/L33377bdvf0cqN+wiW9+3fL59+/oUUL1HC9puB8uF11q1bp64CFixYUO0fff85wnZQHgx7iPrNmjVLBbKwdetW9f/KlStV8PrTTz+p+wjABw4cKOvXr5fNmzerIBlXn+wDc33bDzzwgOzevVsF1Qje8RrZgSAbZcCCfdq8eXOV8pce/G7i+OIKKUY7womPDicukyZNUu+T7du3S/ny5VXdiSgXaNlQtGhR7ciRI5pVRUREYAYN9b8rJCUna9duRqj/zaJTp05qH7333numr0t6rFQfq9QlOSZOu9buNbUkRse65DUHDhyo9ezZ89/XT07WVqxYofn5+Wkvv/yydvr0ac3Ly0s7f/58iud06NBBGz16tLo9b9489XnZtWtXisf8/PPPar39cSldurQ2adKkFI9r0qSJ9swzz6jbJ0+eVM+ZMWNGqjJWqFBBS0pKsq2rXr26duedd9ruJyYmagUKFNC+++67NOs6bdo0rVGjRrb748eP1wIDA7XIyEjbupEjR2rNmjVTt7Ee++Lzzz93+j776quvVDmw33RxcXFaQECAtmzZMqdl6NGjhzZo0CCnf9Prv3PnzjTroMqRlKQFBQVpv//+u20dnjd06NAUj0M9nn76aafbXvXXX+r+9Rs3bMcxJCTE6es9//zzav9fuXLF6bZWr16t7q9cudL2nMWLF6t1t2/ftpVl2LBhKbbbqlUrrX79+lpOWeXzb7W6WK0+SS6uS1bitWylNPTv31/mzJmTG/E2mRBSWPTLtczfpfwCLZtomUQKAS6JowMTLvfv3btX5eQiLQB/1xe0guo5uoDObvXq1Uv3NSIjI1ULbatWrVKsx33HVkhcFndUu3Zt8fwvNQKQ2mA/ggrySosUKaKu0OmQGoHtI10D5R47dqwaY90eLt3bX8nB1Tx9GygXUjrSSm1CSyrSn/B8fd8grQHDGtrvH3tIGUFrOS7lo4/Ixo0bJSOXL1+WwYMHq5ZdpDQEBwfLrVu3UtWlRYsWqe5nt4VX99lnn6nfRKSsIA0jPfbvAf2qqL4vkd7QtGnTFI93vE9E2ZOt3h7ICZs7d666pNSoUSOVs2YPl9LIunDZFJcikUtXvXp1dxeHyCUwAg0uLyNwLV26tLocDQiqEEjiEjT+t4fgzj4FIjdHM3H83gXHjmt4PWfrkHMLSMPAZf0JEyaotAMEigg0cck9o+3q28ioExf2D34ncLneUVrBIU4oMKY7UqdWrFihgml0EkSaR1qQzoBZQJGWgTxr5D8jmE0rbSK3rF69Wp577jn57rvvMjyhcdyX+vtB35dEZLCAd9++fdKwYUN123GIFg5PZX0cjozyIwSY6JTlCJ2n0MKLVrqM8jczglZJBNPIc0WnKx3u50VLH1pOERwir1SX1cmD0KKKoBedsdDJyhF+K9CKjI5YqF9mIRhGEIsF+xUd7xDw4oQDsM/tYR998skntqtO6Ch37dq1VNtFfu+AAQNS3McxzA60XD/44IPy2muvqY7bOYUGBOQF25cP94nITQEvzmgpf0IaHIcjI/p/SGVAKymCFLSMIni6evWqCgDR4te9e/csbQ+BHTqbYcQbXNJHxzb09HfWQppTCFZxyR+tuk2aNJHFixfLzz//nKVtIMUDIz0g9QDBaIuWLeXEydNy5vRJGfzkk2rfTJs2TY3MoI8+gaAanc3wHMcZO2HcuHGqVRgpGkiXQDpJzZo11d8QOCPAxsgVeC5eHy3TqAtGkkCqB1JDsB+dtT4vWrRIPQajDGGfohNcdlL0MOkORtnA8R4yZIhcunTJ9rfsDkeGlmKkZaB8GMEBJwrouIjRJIgoZ3I8gKXe29fZlxZZD3pO4wcSPzL6cEBELuXjJYGv3C8xt2PVbSNAUIqRal566SU5f/68GlEAPfYxQkJWPf/88xIREaG2hVZjDGOG3FAEdLkNoz+8+OKLapQJBJYIzjGKAXKTswLPQYoHAlXkIJcoUVKefnqo+huG38KoEQiK0QqKURMwfBvSFNJq8UXgjJEfMLQXgla08CIoB7wORrhA8IzXw9/QpwBBKwJPtChj0gcMz4ah4hwhfQPbeuaZZ1QOLVIRsI+zCjnDhw4dUgta5e392z8u63BycOLECVVu5DhjKDcMN6ePTEFEOZCdXnHo/TphwgQtODhY8/T0VAt6rk6cODFFD+HM+Pjjj1XPVvTybdq0qbZly5Y0H9u2bVvVG89x6datm+0xUVFRqpdrmTJlNH9/f61mzZrarFmzslQmjtKQtqlTp6p907VrV9PXJTOsVB/WxZisVBer1ccIdenYsaPWv39/S9Qlt1ipLlarT5KBR2nIVgsv8r1wNo1xF/XexBj7EK0COCvFOIKZgcs1I0aMkNmzZ6uB2mfMmKE6TqCnKi5bOcIlMPsOCOigUL9+fTXmpQ7bQ6eqr7/+WvUsXr58uTqTxxk4WjMoZ5jOQESUNzBWM34P8TuIDpBofUbncHTcI6Icyk5EXapUKe3XX39Ntf6XX35RY0hmFlp07cccROswnj9lypRMPf/9999X4yzeunXLtq527dqqpdlew4YNtTFjxmS6XGzhdS48PFzz9vZW++b48eOmrktmWak+VqlLcmKiFrvxoHZtxQ4tMSFBMzurHBcr1sfVdYmJiVHjNxcuXFiNfdygQQPtxx9/zJVt87gYl5Xqk2S1Ft4bN25IjRo1Uq3HOvwtM9BSi2F87Oehx/iRmK4TQ+VkBlqZMUuO/fA8SPRHvhtmr0GrLnK7MJKE48xB9pC7hkWHDg+QrGlqyWt6vhf+N/LgNMtXrFBD0qEnccVKlZzuG7PUJbOsVB+r1EWLS5Rbr3317+0WNSXZYSgws7HKcbFifVxdFz9/f/U96yg3fod4XIzLSvXRXFyXrHw2shXwIo3g448/Vh0H7GEd/pYZGC4Gw8pgYHR7uI9OABlBEj+GR3PsXfvRRx+pjgvoRIfODQiiMVUmpsBNy5QpU1RHBkfhEVGSpLlu2K3wyFtiZL/++qv6v32HjnIzIuV0nWarS1ZZqT6mr0vs/6c1RUTdEknI23FWXcX0x8XC9WFdjMlKdbFafcJdVJeoyPRjkRwHvFOnTlW9eZFbpM9ag1ZZjHuo53jmNQS6mEHIcWxKBLwYVxGtvBhfEr2DMWA5WnvReuwMWpmR+2vfwotevqEhQRIc/P+zC+UVnAnhzREaXNCw49qijKtWrlS37+vVUwqFBJm2LllhpfpYpS6ab7yE/3c7JKigeAb6iZlZ5bhYsT6sizFZqS5Wq4/m4rp4eeRxCy8GREfHMgzyrbfGYrgZvXNYZmDYHiTlY2gXe7if0RiGmNoWw8pgWBrHcRExADjGkdTHvsQ4mBjDEgOWpxXwYkYeLI48PTzUktf0Zn8PF71eduzctUuNM4n0kbZt2qRZTjPUJSusVB+r1EWzK7vZ62Kl42LF+rAuxmSlulitPskurktWXiPb4/BiHMXMjsaQ1jiLGFgcg7P36tXLNr0i7mNMyPRg4HDk3Pbv3z/F+oSEBLXYzyUPCKw5dWPO6C33OGlwdnJAREREZFTe2R1kHXPE2w8HpgeiGFYFU0FmBtII8FjMKoPUBAxLhtbbQYMGqb9j5iIE1sixdUxnQJBcpEiRFOsxiDlan/UZdpDSsHbtWpk/f75Mnz49O1Wl/3A4MiIiIspXAS8C0E8//TTVeoydiw5jmQ14+/Tpo6bgxGw5uFyOaTQxXaTekQ0zejm21iKVAmP+YnxdZ5DqgJxczFiDESMQ9KIleujQf2f9oazDeMfIi4auXbu6uzhEREREeR/wIhCtVKlSqvUILvG3rED6QlopDBhSzBGGxEpv2kbk/6IFmnIPTi6QEoJOgujMR+RWPl4S8Pw9cvt2nGGmFiYiImNL2XyaSWjJ3bNnT6r1u3fvTpVmQObHdAYyEg9vL/Hv1VykW0N1m4iIKE8C3n79+snzzz8vq1evVmPpYsF0vsOHD1cTQZB14NgizQQY8BIREVG+SWl488035dSpU9KhQwc1uQPgkjc6mU2ePDm3y0hu9M8//6hJQkJCQmxjLhO5k5aULAl7TopE3xateU0RtvISEVFeBLwYUmzhwoXy1ltvqTFuMSIC8juRw0vWTGfo1KmT+Pj4uLs4RCLxiXJrxNx/by8ex4CXiIjybhxeqFq1qlpw2Xvv3r1qWLBChQrlZJNkMMzfJSIionyZw/vCCy+osXABwS7Gvm3YsKHqwe9sZAUyJ8x6h5QG6NKli7uLQ0REROS6gPeHH36Q+vXrq9u///67nDhxQk0x/OKLL8qYMWOyVxIyHL2zGmbEy2i6ZyIiIiJLBbzoxKQHQLjk/dBDD0m1atXk8ccfV6kNZA1MZyAiIqJ8G/BiJrQDBw7Yhqy6++671XpMK+zlxQ4kVpCYmCjLli1TtxnwEhERUb7rtDZo0CDVqluqVCnx8PCQjh07qvVbtmyRGjVq5HYZyQ02bdokERERaiKRJk2auLs4RERERK4NeN944w2pU6eOnD17Vnr37i1+fn5qPVp3R40alf3SkOHSGdBZja32ZCjenhIwpLPcjo1Tt4mIiPJsWLIHH3xQ/X/u3Dk16YSnp6cMHDgwu5sjg2H+LhmVh4+3+Pe9U25HRKnbREREGclx80itWrXUrGtkHTiJ2bNnj0pX6dy5s7uLQ0REROTegFfTtJxuggxmyZIl6v/mzZurHF4io00tnHjonMjRi+o2ERFRRng9kFJhOgMZWnyiRD0z+9/bnFqYiIhc0cL72muvSeHChXO6GTKIuLg4WblypbrNgJeIiIisIMctvKNHj86dkpAhrF+/Xm7duqUmFgkLC3N3cYiIiIhyLFfH9MEwZZhtjcyfztC1a1c18gYRERGR2eVqRHPjxg353//+l5ubJBdj/i4RERHl65SG3377Ld2/nzhxIqflITfC8Tt06JCaaEKfPY+IiIgoXwW8vXr1UmOzpjcUGf5O5h6OrFWrVhIaGuru4hARERG5PqWhVKlS8tNPP6mZ1ZwtO3bsyJ1SkVswnYFMwdtT/Ae0F+nTilMLExFRpmTp16JRo0ayffv2NP+eUesvGdft27flr7/+UrcZ8JKRYTrhgMc6iPRtzamFiYgoU7L0azFy5EiJjo5O8+9VqlSR1atXZ2WTZBBr1qyR2NhYKVu2rNSpU8fdxSEiIiJyT8BbpkwZqVSpUpp/L1CggLRt2zY3ykVuTGdgHjYZmZacLEmnrojcihatdgERL860RkREuZjSULVqVbl69artfp8+feTy5ctZ2QQZENJQmL9LphGXKJFPfCQyfK66TURElKsBr2N+LoKk9FIcyByOHDmihiTz8fGRDh06uLs4RERERLmKXZzJ1rqLdJSCBQu6uzhERERE7gt4kdvpmN+Z03zPmTNnSsWKFcXf31+aNWsmW7duTfOx7dq1s5XBfunevXuKxx08eFDuvfdeCQkJUXnFTZo0kTNnzuSonFbGdAYiIiKyMu+spjQ89thj4ufnp+6jV//QoUNVUGkPY/VmxsKFC2XEiBEye/ZsFezOmDFDOnfuLIcPH5bixYunejy2Gx8fb7t//fp1qV+/vvTu3du27vjx49K6dWt54oknZMKECRIcHCz79+9XATWlduvWLVm7dq26zYCXiIiIJL8HvAMHDkxxv3///jl68enTp8vgwYNl0KBB6j4C38WLF8vcuXNl1KhRqR5fuHDhFPcXLFgggYGBKQLeMWPGqMBt6tSptnWVK1fOUTmtbNWqVZKQkCB33HGHVKtWzd3FISIiInJvwDtv3rxce2G01GISi9GjR9vWeXp6SseOHWXTpk2Z2sacOXOkb9++thZmzPaGgPmVV15RLcU7d+5Uw6jhNTAtclri4uLUoouMjPx3e5qmlrymdwbE/8niWthf0LVrV0EpcjpxiDvrkhesVB+r1MX+ParqYvLJbqxyXKxYH9bFmKxUF6vVR3NxXbLy/e+2aYquXbsmSUlJUqJEiRTrcf/QoUMZPh+5vvv27VNBr+7KlSvqEv3bb78tb731lrzzzjuydOlSuf/++9WEGGmNETxlyhSV/uAoPCJKkjTXjUkbHnlLXAlvyMWL/83fvbNte7kZEWXauuQ1K9XH9HVJSBLp2VTdjIi5LZLw/2lOZmb642Lh+rAuxmSlulitPuEuqktUZObjFtPOy4lAt27dutK06b8/fHoLL/Ts2VNefPFFdTssLEw2btyo0iXSCnjRAoxcYvsW3nLlykloSJAEBwe5JPDEmyM0uKBLJ33Yu3evXLhwXuU339O9qwQEBJi2LnnFSvWxVF2e72GduljouFitPqyLMVmpLlarj+biunh5mKCFt2jRouLl5ZVq4grcL1myZLrPxdi/yN+dOHFiqm16e3tLrVq1UqyvWbOmrF+/Ps3toROe3hHPnqeHh1rymt7s7+Gi19MtXbJE/X/XXXdJgcBAU9clr1ipPqyLMVmpLlarD+tiTFaqi9Xqk+ziumTlNdw2Dq+vr680atRIdZqyb6HF/RYtWqT73EWLFqmcW8dOc9gmhiDDKA+OEytUqFAhl2tgfhyOjEw7tfClmyJXItRtIiIiQ6c0II0AIz80btxYpSZgWDK03uqjNgwYMEDKlCmjcmwd0xnQCa1IkSKptjly5Eg15XGbNm2kffv2Kof3999/lzVr1risXmYQHh4uGzZssHVYIzLV1MIPv/fv7cXjRAK93F0iIiIyOLcGvAhMr169KuPGjZNLly6pfFsEqHpHNkwWgZEb7KH1FukJy5cvd7rN++67T+XrIkh+/vnnpXr16vLjjz+qsXnp/61YsUJ1GqxRo4YakoyIiIjIqtzeae3ZZ59VizPOWmURwGY0dNbjjz+uFkob0xmIiIgov3BbDi+5D3Kll/zXYY0BLxEREVkdA958CBNyYDSMggULMtWDiIiILI8Bbz5OZ8Csds6GYyMiIiKyEga8+RDzd4mIiCg/cXunNXL9lM5btmxRtzkcGZmSl6f49WwmcXHx6jYREVFGGPDmM8uWLVOjXNSrV0/Kli3r7uIQZZmHr7cEDu8hcRFR6jYREVFG2DySzzCdgYiIiPIbBrz5CCaawMQewICXzApXKJLDo0UiYjIck5uIiAgY8OYjW7dulRs3bkhISIi0aNHC3cUhyp7YBIm4f4rIYx+p20RERBlhwJsP0xk6d+4s3t7MfSQiIqL8gQFvPsL8XSIiIsqPGPDmExcvXpQdO3ao2126dHF3cYiIiIhchgFvPqF3VmvcuLGUKFHC3cUhIiIichkGvPkE0xmIiIgov2LAmw8kJCTI8uXL1W0GvERERJTfsKt+PrBx40aJjIyUokWLqpQGIlPz8hTfzg0kPj6BUwsTEVGmMODNR+kM6Kzm5eXl7uIQ5QimEy7w6gMSz6mFiYgok9g8kg8wf5eIiIjyMwa8FnfmzBnZt2+feHp6SqdOndxdHKIcw3TC2u14kdh4Ti1MRESZwoDX4pYsWaL+b968uRQpUsTdxSHKudgECe8+UaTf+5xamIiIMoUBr8UxnYGIiIjyOwa8FhYXFycrV65UtxnwEhERUX7FgNfC/v77b4mJiZFSpUpJWFiYu4tDRERE5BYMePNBOkPXrl3Fw8PD3cUhIiIicgsGvBbG/F0iIiIiBryWdezYMTly5Ih4e3tLx44d3V0cIiIiIrfhNEUWH46sdevWEhIS4u7iEOUeLw/xaVNbEhIS1W0iIqKMMOC1KKYzkFV5+PpIwTf6yU01tbCPu4tDREQmYIiUhpkzZ0rFihXF399fmjVrJlu3bk3zse3atVMdsByX7t27O3380KFD1d9nzJgh+QVGZli9erW6zYCXiIiI8ju3B7wLFy6UESNGyPjx42XHjh1Sv3596dy5s1y5csXp43/66Se5ePGibcG0uV5eXtK7d+9Uj/35559l8+bNUrp0aclPEOxiDN7y5ctLrVq13F0cIiIiovwd8E6fPl0GDx4sgwYNUsHZ7NmzJTAwUObOnev08YULF5aSJUvalhUrVqjHOwa858+fl+eee06++eYb8fHxybfpDByOjKxGux0vN+8aK3LfO+o2ERGRoXN44+PjZfv27TJ69GjbOk9PTzWqwKZNmzK1jTlz5kjfvn2lQIECtnXJycny6KOPysiRI6V27doZbgOtoVh0kZGR/25H09SS17T/XgP/J+fCtvSAt0vXri4pv+Pr51ZdjMBK9bFKXfR66Ldd/R7PbVY5LlasD+tiTFaqi9Xqo7m4Lln5/ndrwHvt2jVJSkqSEiVKpFiP+4cOHcrw+cj1RUoDgl5777zzjhqO6/nnn89UOaZMmSITJkxItT48IkqSNNe1kIZH3srxNo4cPiynTp0SX19fadCoierY4w65URcjsVJ9TF+X2P9v1Y2IuiWSYI1WXtMfFwvXh3UxJivVxWr1CXdRXaIio/LHKA0IdOvWrStNmza1rUOL8QcffKDygTN7OR8tzMgjtm/hLVeunISGBElwcJDkNZwJ4c0RGlwwxykIG9atVf+3bddOypYuKa6Wm3UxAivVxyp10XzjJfy/2yFBBcUz0E/MzCrHxYr1YV2MyUp1sVp9NBfXxcvDJC28RYsWVR3OLl++nGI97iM/Nz3R0dGyYMECmThxYor169atUx3e0GFLh1bkl156SY3UgNZPR35+fmpx5OnhoZa8pjf7e+TC6+nj73bv1s0lZc/LuhiBlepjlbpodmU3e12sdFysWB/WxZisVBer1SfZxXXJymu4tdMaLrs3atRIVq1alSL/FvdbtGiR7nMXLVqk8m779++fYj1yd/fs2SO7du2yLRilAfm8y5YtEytDyzQCfuBwZEREREQGSWlAKsHAgQOlcePGKjUBrbBovcWoDTBgwAApU6aMyrN1TGfo1auXFClSJMV63Hdch1Ea0GJcvXp1sTKcKCQkJEiVKlWkatWq7i4OERERkSG4PeDt06ePXL16VcaNGyeXLl2SsLAwWbp0qa0j25kzZ9TIDfYOHz4s69evl+XLl7up1MbE2dUoX/DyEO9m1SQxkVMLExGRSQJeePbZZ9XizJo1a1KtQ0ut/dBEGXGWt2s19sORMeAlK8N0wkFTBnBqYSIiMs/EE5Q7kLd84cIFCQgIkLZt27q7OERERESGwYDXIvTW3Q4dOoi/v7+7i0NERERkGAx4LYLpDJSvphbuNkGk73ROLUxERObJ4aWcuXnzpmzcuFHd7tq1q7uLQ5T3YhPcXQIiIjIRtvBaAEarwPjFtWrVkooVK7q7OERERESGwoDXApjOQERERJQ2Brwmh5ZdfTphBrxEREREqTHgNbnt27eriTuCgoKkVatW7i4OERERkeEw4LVIOsPdd98tvr6+7i4OERERkeFwlAaTY/4u5TueHuJdv6IkJiap20RERBlhwGtiV65ckW3btqnbHI6M8gsPPx8Jev/Jf6cW9uPUwkRElDGmNJjYsmXLRNM0CQsLk9KlS7u7OERERESGxIDXxJjOQERERJQxBrwmlZiYqFp4gQEv5SeYTjj8vskiAz/k1MJERJQpzOE1qS1btqgphQsVKiTNmjVzd3GIXEqLiHF3EYiIyETYwmvydIbOnTuLtzfPW4iIiIjSwoDXpJi/S0RERJQ5DHhN6Pz587Jr1y7x8PBQLbxERERElDYGvCa0dOlS9X+TJk2kePHi7i4OERERkaEx4DUhpjMQERERZR57O5lMfHy8rFixQt1mwEv5kqeHeFUvI0lJnFqYiIgyhwGvyWzYsEGioqKkWLFi0qhRI3cXh8jlMJ1w8KynObUwERFlGlMaTJrO0LVrV/H05OEjIiIiyggjJpNh/i4RERFR1jDgNZFTp07JgQMHVMtup06d3F0cIrfQYuMlot+7IkNmqdtEREQZYQ6viSxZskT937JlSzWlMFG+pIkkXw633SYiIsoIW3hNhOkMRERERFnHgNckYmNjZdWqVeo2A14iIiIikwW8M2fOlIoVK4q/v780a9ZMtm7dmuZj27Vrp6bUdVy6d++u/p6QkCCvvvqq1K1bVwoUKCClS5eWAQMGyIULF8TM1q5dK7dv31b1qVevnruLQ0RERGQabg94Fy5cKCNGjJDx48fLjh07pH79+tK5c2e5cuWK08f/9NNPcvHiRduyb98+8fLykt69e6u/x8TEqO28/vrr6n88/vDhw3LvvfeKVdIZEOATERERkUk6rU2fPl0GDx4sgwYNUvdnz54tixcvlrlz58qoUaNSPb5w4cIp7i9YsEACAwNtAW9ISIhtJjLdxx9/LE2bNpUzZ85I+fLlxYyYv0tERERkwoAX0+Ru375dRo8ebVuHIbc6duwomzZtytQ25syZI3379lXpC2mJiIhQraKhoaFO/x4XF6cWXWRkpPo/WdPUkte0/14D/yc7+fvRo0fl2LFj4uPjI+3vusslZcqrupiNlepjlbpooolnhWKSnJysbhv585CfjosV68O6GJOV6mK1+mgurktWvv/dGvBeu3ZNkpKSpESJEinW4/6hQ4cyfD5yfZHSgKA3vc5eyOnt16+fBAcHO33MlClTZMKECanWh0dESZLmuvSB8MhbTtf/+NPP6v/mLVqo8mBKVaNLqy5mZaX6WKIuMx5X/0XEx+PMWazAEsfFovVhXYzJSnWxWn3CXVSXqMgo86Q05AQCXXROQ7qCM+jA9tBDD6kzjVmzZqW5HbQwI4/YvoW3XLlyEhoSJMHBQZLXUD68OUKDCzrNz12z+i/1/709ekihkLwvT17WxWysVB/WxZisVBer1Yd1MSYr1cVq9dFcXBcvD5O08BYtWlR1OLt8+XKK9bhfsmTJdJ8bHR2t8ncnTpyYbrB7+vRp+euvv9Js3QU/Pz+1OPL08FBLXtOb/T2cvB7q+ffateo2RqJwRXnyqi5mZKX6sC7GZKW6WK0+rIsxWakuVqtPsovrkpXXcOsoDb6+vtKoUSPb+LKAvDzcb9GiRbrPXbRokcq77d+/f5rBLnJfV65cKUWKFBGzWr16taonhm2rUaOGu4tDZIyphQd9KPL8F5xamIiIzJHSgFSCgQMHSuPGjVVqwowZM1Srpj5qA8bQLVOmjMqzdUxn6NWrV6pgFsHugw8+qIYk++OPP1SO8KVLl2wjPCDINhMOR0bkZGrh0/8NW2ju/mpERJRfAt4+ffrI1atXZdy4cSowDQsLk6VLl9o6smEoMYzcYA/j6q5fv16WL1+eanvnz5+X3377Td3GthxbSzFxhZlyYTgcGREREZHJA1549tln1eLMmjVrUq2rXr26begLR7j0n9bfzObgwYMqBxn5xe3bt3d3cYiIiIhMye0zrVHa9NZdBLuYXIOIiIiIso4Br4ExnYGIiIgo5xjwGhTGAl63bp263bVrV3cXh4iIiMi0DJHDS6lhOLXExESpVq2aVKlSxd3FITIODxHPEqFqCEPcJiIiyggDXoNiOgORcx7+vhLy3ctqim3cJiIiyghTGgyIw5ERERER5R4GvAa0e/duuXjxohqZoU2bNu4uDhEREZGpMeA1IL11t2PHjmoMXiL6f1pcgkQ+PUtk5P/UbSIioowwh9eAmM5AlI5kTZIOn7fdJiIiyghbeA3mxo0bsmnTJnWbw5ERERER5RwDXoNZvny5Gm6pTp06Ur58eXcXh4iIiMj0GPAaDNMZiIiIiHIXA14DQcvukiVL1G0GvERERES5gwGvgfzzzz9y7do1CQ4OlpYtW7q7OERERESWwFEaDGTJf+kMnTp1Eh8fH3cXh8iwPEIC1QQtREREmcGA10CYzkCUMY8AXwn9+bV/pxYO4NTCRESUMaY0GMSVK1dk27Zt6naXLl3cXRwiIiIiy2DAaxCr/1ql/m/YsKGUKlXK3cUhIiIisgwGvAaxYvly9T/TGYjSh+mEo178QmTst5xamIiIMoU5vAaQmJhoa+FlwEuUgWRNEnefst0mIiLKCFt4DWDz5s0SEREhhQsXlqZNm7q7OERERESWwoDXQMORde7SRby8vNxdHCIiIiJLYcBroOHIunbt6u6iEBEREVkOc3gNkL/bpEkTuX79hnTu3NndxSEiIiKyHAa8bubt7S2ffvaZ3AiPlMKhwe4uDhEREZHlMOA1CA8PD3cXgcg8/H1EOEADERFlEgNeIjIVTCdc6M/xnFqYiIgyjZ3WiIiIiMjSGPASERERkaUZIuCdOXOmVKxYUfz9/aVZs2aydevWNB/brl07le/quHTv3t32GE3TZNy4cVKqVCkJCAiQjh07ytGjR11UGyLKS1p8gkSNni/y1iJ1m4iIyPAB78KFC2XEiBEyfvx42bFjh9SvX18Nz3XlyhWnj//pp5/k4sWLtmXfvn1qsobevXvbHjN16lT58MMPZfbs2bJlyxYpUKCA2mZsbKwLa0ZEeSJJk8QtR0S2n1C3iYiIDN9pbfr06TJ48GAZNGiQuo8gdfHixTJ37lwZNWpUqsdj+l17CxYskMDAQFvAi9bdGTNmyNixY6Vnz55q3fz586VEiRLyyy+/SN++fVNtMy4uTi26yMhI9X+ypqklr6HM+v/JYm5WqovV6mOVuuj10G+74jOal6xyXKxYH9bFmKxUF6vVR3NxXbLy/e/WgDc+Pl62b98uo0ePtq3z9PRUKQibNm3K1DbmzJmjgli04sLJkyfl0qVLahu6kJAQlSqBbToLeKdMmSITJkxItT48IkqSNNcNFxYeeUuswkp1sVp9TF+X2HjbzYioWyIJ/3/fzEx/XCxcH9bFmKxUF6vVJ9xFdYmKjDJHwHvt2jVJSkpSra/2cP/QoUMZPh+5vkhpQNCrQ7Crb8Nxm/rfHCHgRlqFfQtvuXLlJDQkSIKDgySv4UwIb47Q4IKmH4/XSnWxWn2sUhfNN17C/7sdElRQPAP9xMysclysWB/WxZisVBer1UdzcV28PEzSwptTCHTr1q0rTZs2zdF2/Pz81OLI08NDLXlNb/b3cNHr5SUr1cVq9bFKXTS7spu9LlY6LlasD+tiTFaqi9Xqk+ziumTlNdzaaa1o0aKqw9nly5dTrMf9kiVLpvvc6Oholb/7xBNPpFivPy872yQiIiIi63FrC6+vr680atRIVq1aJb169VLrkpOT1f1nn3023ecuWrRIdTTr379/ivWVKlVSgS22ERYWZktRwGgNTz/9dJaSrvXOa65IukYeCprmTX92Z6G6WK0+VqmLdjteohL/7WTqGRkpXonmTmmwynGxYn1YF2OyUl2sVp9kF9dFj9PsOzOnSXOzBQsWaH5+ftqXX36pHThwQBsyZIgWGhqqXbp0Sf390Ucf1UaNGpXqea1bt9b69OnjdJtvv/222savv/6q7dmzR+vZs6dWqVIl7fbt25kq09mzZ7HnuHDhwoULFy5cuIixF8RtGXF7Dm+fPn3k6tWraqIIdCpDq+zSpUttnc7OnDmjRm6wd/jwYVm/fr0sX77c6TZfeeUVlfIwZMgQCQ8Pl9atW6ttYmKLzChdurScPXtWgoKCXJJ0rXeSw2sGBweLmVmpLlarD+tiTFaqi9Xqw7oYk5XqYrX6RLq4LmjZjYqKUnFbRjwQ9eZ5iSjDNwiGTouIiLDEm90qdbFafVgXY7JSXaxWH9bFmKxUF6vVJ9LAdXH7TGtERERERHmJAS8RERERWRoDXgPAGMDjx493Ohaw2VipLlarD+tiTFaqi9Xqw7oYk5XqYrX6+Bm4LszhJSIiIiJLYwsvEREREVkaA14iIiIisjQGvERERERkaQx4iYiIiChXrFmzRk3ahYm/4Msvv5TQ0FB3F4sBb2557LHH1AF+++23U6z/5ZdfXDJbmyvNnDlTKlasqGaua9asmWzdujXdx0+aNElatmwpgYGBhnjTZ7cup06dkieeeEIqVaokAQEBUrlyZdUbNT4+Xsx6bO69914pX768enypUqXk0UcflQsXLogZ66KLi4tTMzbic7dr1y4xY13wWJTffnH8bjHTcVm8eLF6LD43hQoVkl69eolRZKU++g+5s2Xbtm1itmNz5MgR6dmzpxQtWlRNEoBZSVevXi1GkNW67NixQ+6++271G1OkSBE10+qtW7fEKr+RmHW2e/fu6jHFixeXkSNHSmJiorjTpk2bxMvLS5XLFDKcfJgyZeDAgZq/v78WGhqq3bhxw7b+559/VvM8W8WCBQs0X19fbe7cudr+/fu1wYMHqzpfvnw5zeeMGzdOmz59ujZixAgtJCREM2tdlixZoj322GPasmXLtOPHj2u//vqrVrx4ce2ll17SzHpscFw2bdqknTp1StuwYYPWokULtZixLrrnn39e69q1q/rc7dy5UzNjXSpUqKBNnDhRu3jxom25deuWZsa6/PDDD1qhQoW0WbNmaYcPH1bPW7hwoWYEWa1PXFxcimOC5cknn9QqVaqkJScna2Y7NlWrVtW6deum7d69Wzty5Ij2zDPPaIGBgapeZqrL+fPn1Xts6NCh2qFDh7StW7dqLVu21B544AHDlz0zv5GJiYlanTp1tI4dO6rvtD///FMrWrSoNnr0aM2dnnjiCW348OFawYIF1THQrV69Wn3/3rx5U92fN2+eIX77rROJGSDgveeee7QaNWpoI0eOTDPgxZd/rVq11AcCP2rvvvtuiu1g3aRJk7RBgwapN1G5cuW0Tz/9NMVjzpw5o/Xu3Vu9gfAhv/fee7WTJ0+6oJaa1rRpU23YsGG2+0lJSVrp0qW1KVOmZPhco7zpc6MuuqlTp6ofO6vUB0G8h4eHFh8fr5mxLvghwGcQPzRGCXizUxd8D7z//vua0WS1LgkJCVqZMmW0L774QjOinH5m8DkpVqyYOjkxW12uXr2qPiN///23bV1kZKRat2LFCs1MdcFvJBof8Djdnj17VF2OHj2quVJe/Ebie83T01O7dOmSbR1OIIODg9VJmDtERUWpGAUnGH369FFxi9EDXqY05CI07U+ePFk++ugjOXfuXKq/b9++XR566CHp27ev7N27V9544w15/fXXVX6Lvffee08aN24sO3fulGeeeUaefvppOXz4sPpbQkKCdO7cWYKCgmTdunWyYcMGKViwoHTp0iXPL61j+6hDx44dbes8PT3VfVzaMJPcqgvmCy9cuLBYoT43btyQb775Rl1a8/HxEbPV5fLlyzJ48GD56quv1GU/I8jJcUEKAy7NNmjQQKZNm+b2y5fZqQsuM58/f149DvVA2kzXrl1l3759YoXPzG+//SbXr1+XQYMGidnqgvdW9erVZf78+RIdHa3eX59++qm6XN6oUSMxU12QxuTr66sep0P6DKxfv15cJa9+I/HcunXrSokSJWzrEAdERkbK/v37xR2+//57qVGjhnoP9e/fX+bOnYuWPTEyBry57L777lP5g8jtdDR9+nTp0KGDCnKrVaum8n6fffZZ9WNmr1u3birQrVKlirz66qsqv0rPq1q4cKEkJyfLF198oT4ANWvWlHnz5qn8HuSX5aVr165JUlJSig8d4P6lS5fETHKjLseOHVMnN0899ZSYuT54jxUoUED9AOJ99Ouvv4rZ6oIvWnyehg4dqk4WjSK7x+X555+XBQsWqM893l84kX7llVfEbHU5ceKE+h8n92PHjpU//vhD5fC2a9dOnWCZ/Ttgzpw5KvAoW7asmK0uyDteuXKlalhBAwryTfEbtXTpUnWMzFSXu+66S/0Nv6UIOm/evCmjRo1Sf7t48aK4Sl79RuK5zrap/80d5syZowJdQIMbGn/Wrl0rRsaANw+888478r///U8OHjyYYj3ut2rVKsU63D969Kj6kOjq1auX4kupZMmScuXKFXV/9+7dKtDCFxRadrGghTE2NlaOHz8u7oRgQy8TFjPLqC5otcKHvHfv3qpV0cz1QecH/OgtX75cXaUYMGCAoc/UndUFJx5RUVEyevRoMZO0jsuIESNUUIjvAjwGV31QR7RkmakuODmHMWPGyAMPPKBaDnGCju+1RYsWiZm/A3AVb9myZaojq9E5qws+48OGDVMturhaiI5V6EzYo0cPlwaJuVGX2rVrq99cfE5wdQe/mehcjKDQvtXX3azyG3n48GH1funXr5+67+3tLX369FFBsJF5u7sAVtSmTRt11o8fX7Q6ZZXj5WT8OOg/HOh1ih8NXHp2VKxYMclLaGlGQIRLx/ZwH18wEydOlJdfflnMICd1wSgG7du3V5f+P/vsMzF7ffBcLLjqgCsG5cqVk82bN0uLFi1cVPqc1+Wvv/5Sl/0c529Ha+8jjzyifgzdIbc+M+jpjUvOGCkElxDNUhekMECtWrVs63CM7rjjDnU1wZ1yemwQuOOqCEY6cbfsfmbQ4o7WUIzQAJ988omsWLFCfV70FlKzHJeHH35YLXgcrljhdxMt1nivGb3sGcFzHUd60F8Df3O1OXPmqO+j0qVL29bhBAqf7Y8//liMyjinPhaD/Lvff/89Rd4Oggnk3NrDfQQa+JBkRsOGDVWLMM7KkfJgv4SEhEheQo4Ugu1Vq1bZ1iEQx30ER45lMrLs1gUtu2h501uqjNJ6kFvHRj+xcmdLYnbq8uGHH6qrHxiGDMuff/5pSwHCkD9mqoszqBPea3i8meqCx+NHUO+DoPdDQOBeoUIFcaecHBv8uOPzj6sh7sx3z0ldYmJi1P+O32G4r38PmPEzg1ZdtJ7is480DQxVZpaypwXPRb8f/Uov4MQEJyr2J5OukJiYqPK+0Zquf99iwfcvAuDvvvtODMvdveasNEpDz549U6x79NFH1VBl+m7evn276mmJHr0YnufLL7/UAgICVA/G9Hpn169fXxs/fry6HR0drYaSadeunepde+LECdUj8rnnntPOnj3rkiFX/Pz8VNkPHDigDRkyRA25Yt971NHp06dVb/kJEyaoXp24jQW9PN0pq3U5d+6cVqVKFa1Dhw7qtv3QREaQ1fps3rxZ++ijj9SxwLBkq1atUkP5VK5cWYuNjdXcKTvvM3sYtcQoozRktS4bN25U3wG7du1Sw999/fXXaiSAAQMGaO6WneOCYYswUgOG80OPbgxlhB719sM3mu19tnLlSvX+OnjwoGYUWa0LRmkoUqSIdv/996v3Gn6TXn75Zc3Hx0fdN9txwXcZfmNRj48//lj9tn7wwQeaq+XFb6Q+LFmnTp3UsVm6dKn6TnDHsGQ///yzGmUqPDw81d9eeeUVrXHjxoYdpYEBbx4GvPjRxRvD2bBk+FIpX768Nm3atBTPySjgBQRY+PHDOHz4YN1xxx1qrL+IiAjNFfDFgrKjbhiCBYFTRvsG+8BxwYfC3bJSF3xondXDSOeNWakPhu1p3769VrhwYfU+qlixohrHEsG8EWT1fWbUgDerdcGPdrNmzdQPBE6Ya9asqU2ePNntJyHZPS4YugtjVSPIDQoKUmOJ7tu3TzOK7LzP+vXrp04OjSarddm2bZsKovAdgGPTvHlzNQSWGeuCBibUA4+vV6+eNn/+fM1d8uI3Eo0SGF8cgTx++/GZwrB/rnbPPfeosZud2bJliyo3TjSMGPB64B93tzITEREREeUVYyQgEhERERHlEQa8RERERGRpDHiJiIiIyNIY8BIRERGRpTHgJSIiIiJLY8BLRERERJbGgJeIiIiILI0BLxERERFZGgNeIso3KlasKDNmzMjVbT722GPSq1evdB/Trl07eeGFF8SKMlP/7Dh16pR4eHjIrl270nzMmjVr1GPCw8Nz5TVze3tEZBwMeInIcBB0pLe88cYb2drutm3bZMiQIbleXiIiMjZvdxeAiMjRxYsXbbcXLlwo48aNk8OHD9vWFSxY0HYbs6MnJSWJt3fGX2fFihXLg9JSTmTl+BERZRdbeInIcEqWLGlbQkJCVKuufv/QoUMSFBQkS5YskUaNGomfn5+sX79ejh8/Lj179pQSJUqogLhJkyaycuXKdFMasN0vvvhC7rvvPgkMDJSqVavKb7/9Zvs7ArEnnnhCKlWqJAEBAVK9enX54IMPnJZ5woQJKqAODg6WoUOHSnx8fJr1i4uLk5dfflnKlCkjBQoUkGbNmqnL6enBZfYnn3zS9hp33XWX7N692/Z3tHqHhYXJV199peqJ/da3b1+JioqyPSY5OVmmTp0qVapUUfutfPnyMmnSJNvf9+7dq7aLuhYpUkS1ht+6dSvF/hgxYoSEhoaqv7/yyisqYLWH15gyZYptn9WvX19++OGHVGkDjscvLTjeLVu2FH9/f6lTp46sXbs23f30448/Su3atdV2sR/ee++9VPv+1VdflXLlyqnHYF/MmTPH6bZiYmKka9eu0qpVK7X/cUyfffZZKVWqlCpPhQoVVF2JyPgY8BKRKY0aNUrefvttOXjwoNSrV08FZt26dZNVq1bJzp07pUuXLtKjRw85c+ZMuttBoPrQQw/Jnj171PMfeeQRuXHjhi14K1u2rCxatEgOHDigWppfe+01+f7771NsA6+JciCY++677+Snn35S200LgqZNmzbJggUL1Ov27t1blffo0aNpPgePuXLligoUt2/fLg0bNpQOHTrYygoI+n/55Rf5448/1ILgEPtIN3r0aHX/9ddfV/X59ttv1QkCREdHS+fOnaVQoUIq9QN1xgkDyqpD8Pjll1/K3LlzVZCK1/75559TlBMB4Pz582X27Nmyf/9+efHFF6V///6pAlXH45eWkSNHyksvvaSOaYsWLdQxvX79utPHYr/gWCLQR/COkwDUFWXWDRgwQB2jDz/8UL32p59+muKKgQ4B7t13363eAytWrFBBPp6DEyIcf1xx+Oabb1RQTUQmoBERGdi8efO0kJAQ2/3Vq1ejSVH75ZdfMnxu7dq1tY8++sh2v0KFCtr7779vu4/tjB071nb/1q1bat2SJUvS3OawYcO0Bx54wHZ/4MCBWuHChbXo6GjbulmzZmkFCxbUkpKS1P22bdtqw4cPV7dPnz6teXl5aefPn0+x3Q4dOmijR492+prr1q3TgoODtdjY2BTrK1eurH366afq9vjx47XAwEAtMjLS9veRI0dqzZo1U7ex3s/PT/v888+dvsZnn32mFSpUSO0D3eLFizVPT0/t0qVL6n6pUqW0qVOn2v6ekJCglS1bVuvZs6e6j/KhDBs3bkyx7SeeeELr169flo7fyZMn1ePefvvtVK/3zjvvpNjWzZs31f2HH35Yu/vuu1NsB/ugVq1a6vbhw4fV41esWOH0NfXtHTx4UKtXr546znFxcba/P/fcc9pdd92lJScnp1t2IjIeJk0RkSk1btw4xX208KJFb/HixSoHODExUW7fvp1hC6996yLSC5AugJZU3cyZM1WLJraD7eGyNlIH7OGyPVIidGiJRHnOnj2rLnvbQ8sjUgOqVauW6lI70gScQeoCtuf4d5QHrbo6tDYi3UOHS+96XdCaiddAq7Az+DvqgX2gw6V8tHCiNROX8LFfkX6hQ94tjoOe1nDs2DGVBoCWUXvYZw0aNEj3+KUF+9Lx9VDWtOqAtBZ7qAPSWLDPMeKDl5eXtG3bNt3XRPmbNm2q8sfxePsRKfA3pLagRf6ee+6RTp06ZaoeROReDHiJyJTsAzNATiwuPb/77rsqLxP5ow8++GC6ubTg4+OT4j7ySxHkAVIOsF1cykfghWBy2rRpsmXLlmyXG4ErgihcfrcPpsDZpXX9OQheneX54lJ7ZuqC/ZHX9HxfnHQgP9ke8mXTO36ukNl90L17d5ULjLSPunXr2tYjjeTkyZMqrQTpHkif6NixY4ocZSIyJga8RGQJGzZsUC1w6ICmB18YyzWn20SHqWeeeca2zr5F1b4FFq2tekC1efNmFbyiY5QjtHSitREtr3feeWemyoFA69KlS6qFM7s5o+iQh/Ih3xid3xzVrFlT5boil1cPRlF/T09P1aKJTnAIuhHst2nTRv0dreh6PjHUqlVLBbZoDc+oFTWzsC8dX88+r9ixDiizPdxHazpOLhC84gQA+cQIVNOC3GIcP7SG4yQD9dLhCkCfPn3UghMqtPQil7lw4cK5Ul8iyhsMeInIEhDQobMYOjWhZROdlfTWzZxsEx2wli1bpkYdwAgI6NCF2/bQiozRHMaOHauC7PHjx6ugDMGiIwRf6BiHzlNoOUYAfPXqVRWIIr0CrYuOEJyhhRkTPGCUBWzjwoULqiUVAX5m0gOQkoDRCTCygq+vr7rUj9dFxzKUHWVCuQcOHKhSQ/C35557Th599FFbx7bhw4erYBD7pUaNGjJ9+vQUkzSgBRwt4uiohn3funVriYiIUEEnAkVsO6uQUoLXQzD7/vvvy82bN+Xxxx93+lh0bsPoHG+++aYKSNEx8OOPP5ZPPvlE/R0nCygDno8OaEjhOH36tDr5QGutPVwpwIkJRq1A0KvXF0E/jhmOLTr2YeQQ+1Z2IjImBrxEZAkIRhDIoEW2aNGiKriLjIzM0TafeuopNToAgicE0f369VOtvbikbQ8tgQjK0BKJPFk8Lr3JMebNmydvvfWWCtDOnz+vytu8eXOVE+oMXvvPP/+UMWPGyKBBg1QwikALr6cHo5mBkwC0EmO0CQTMCN4whBogBxmBPYJaBI24/8ADD6j9qkN5kceLoBEBH/Y3Am4EtToEmxg6DaM1nDhxQgWDaAHG6BbZgQAbC/JvkaqCURKwv5zB62AEBdQP5UD9Jk6cqFr+dbNmzVJlwXHEaA8Ymi2tsiHAtg96EdDjhAOjaaDFGPsJx8XZiQ0RGYsHeq65uxBERERERHmFp6VEREREZGkMeImIiIjI0hjwEhEREZGlMeAlIiIiIktjwEtERERElsaAl4iIiIgsjQEvEREREVkaA14iIiIisjQGvERERERkaQx4iYiIiMjSGPASERERkVjZ/wHy15FzAtJeDgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create Figure\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# Prepare Data\n",
    "x = [f\"0-{index}\" for index in range(12)]\n",
    "x[0] = \"None\"\n",
    "x[-1] = \"All\"\n",
    "y = [\n",
    "    0.8477443609022557,\n",
    " 0.8431924882629108,\n",
    " 0.8428974600188147,\n",
    " 0.839622641509434,\n",
    " 0.8398104265402844,\n",
    " 0.8387096774193549,\n",
    " 0.8358490566037736,\n",
    " 0.832391713747646,\n",
    " 0.8320754716981132,\n",
    " 0.8174904942965779,\n",
    " 0.7953667953667953,\n",
    " 0.7049742710120068\n",
    "][::-1]\n",
    "\n",
    "# Stylize Figure\n",
    "plt.grid(color='#ECEFF1')\n",
    "plt.axvline(x=4, color=\"#EC407A\", linestyle=\"--\")\n",
    "plt.title(\"Effect of Frozen Encoder Blocks on Training Performance\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.xlabel(\"Trainable encoder blocks\")\n",
    "\n",
    "# Plot Data\n",
    "plt.plot(x, y, color=\"black\")\n",
    "\n",
    "# Additional Annotation\n",
    "plt.annotate(\n",
    "    'Performance stabilizing',\n",
    "    xy=(4, y[4]),\n",
    "    xytext=(4.5, y[4]-.05),\n",
    "    arrowprops=dict(\n",
    "        arrowstyle=\"-|>\",\n",
    "        connectionstyle=\"arc3\",\n",
    "        color=\"#00ACC1\")\n",
    ")\n",
    "plt.savefig(\"multiple_frozen_blocks.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf785lzMjwiy"
   },
   "source": [
    "## Few-shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8ybeQ3j6kOk4"
   },
   "outputs": [],
   "source": [
    "'''This imports the sample_dataset utility function from the SetFit library.\n",
    "SetFit is a framework for efficient few-shot fine-tuning of Sentence Transformers.\n",
    "The sample_dataset function helps reduce a dataset to a smaller few-shot subset by randomly sampling examples.'''\n",
    "from setfit import sample_dataset\n",
    "\n",
    "# We simulate a few-shot setting by sampling 16 examples per class\n",
    "'''This code is taking the training split of the tomatoes dataset and creating a few-shot version of it by sampling 16 examples per class. That smaller dataset will then be used to train a model in a low-data setting.'''\n",
    "sampled_train_data = sample_dataset(tomatoes[\"train\"], num_samples=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3418,
     "status": "ok",
     "timestamp": 1719390247822,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "1Y55TDrmSqHm",
    "outputId": "84ec7a9c-92ed-4277-dfff-f0cb4cc918d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#A SetFitModel is essentially a sentence transformer (for embeddings) wrapped with a classification head (so it can be fine-tuned on classification tasks).\n",
    "from setfit import SetFitModel\n",
    "\n",
    "# Load a pre-trained SentenceTransformer model\n",
    "'''SetFitModel.from_pretrained(...) is a class method that loads a model checkpoint.\n",
    "\"sentence-transformers/all-mpnet-base-v2\" is a pre-trained Sentence Transformer hosted on the Hugging Face Hub.\n",
    "all-mpnet-base-v2 is one of the strongest general-purpose embedding models, built on Microsoft’s MPNet architecture.\n",
    "It encodes text into dense vector representations (embeddings).\n",
    "Under the hood:The Sentence Transformer backbone (all-mpnet-base-v2) is loaded. A classification head is added on top, but initially untrained.\n",
    "You can then fine-tune this model with few-shot learning'''\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "630de7830edb4104938bef1c304129c0",
      "736c3004e2ec48b2b3a8974e465eb838",
      "54c6443169d84a3c9983d8c5b125c3e7",
      "bbee47201aac4b8f96aa772cc751d1d8",
      "9ad6e36197894003809d3ae6fa1b6a2c",
      "4708f67e67194377a94ec08aa07fe688",
      "c03a1bf3d2214c3680ab0b983dd582ef",
      "e777041b66fa484090714094d9516f35",
      "c5d50176a0a049769731d1c6fdb37f0f",
      "aa6d83c964d741178534cd0fae6ccec9",
      "318f91cb3cdd4f6e88681121de392207"
     ]
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1719390248689,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "zZ10SpAXdNvC",
    "outputId": "f9930b86-e077-4233-f7d7-16c5ec1e640d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca99435df4e74c65b6f53d83804bbc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''Contrastive learning phase: Model takes sentence pairs (generated via num_iterations) and learns embeddings by pulling similar pairs together and pushing different pairs apart.\n",
    "Classifier training phase: A lightweight classifier head (often logistic regression) is trained on top of embeddings to perform your target classification task.\n",
    "Evaluation: On test_data, trainer computes the F1-score to track model performance.'''\n",
    "from setfit import TrainingArguments as SetFitTrainingArguments\n",
    "from setfit import Trainer as SetFitTrainer\n",
    "\n",
    "# Define training arguments\n",
    "args = SetFitTrainingArguments(\n",
    "    num_epochs=3, # The number of epochs to use for contrastive learning\n",
    "    num_iterations=20  # The number of text pairs to generate\n",
    ")\n",
    "args.eval_strategy = args.evaluation_strategy\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=sampled_train_data,\n",
    "    eval_dataset=test_data,\n",
    "    metric=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8HDP5-9wRYZS"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\851602375.py:4: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2bd2079cfb4c0a931a687b9b16fde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from setfit import SetFitTrainer\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    train_dataset=sampled_train_data,\n",
    "    eval_dataset=test_data,\n",
    "    metric=\"f1\",\n",
    "    num_epochs=3, # The number of epochs to use for contrastive learning\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "executionInfo": {
     "elapsed": 37840,
     "status": "ok",
     "timestamp": 1719390288851,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "3yaXeQJadWIS",
    "outputId": "3bb59731-1360-4f4f-eaae-989a8547ca7c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 1280\n",
      "  Batch size = 16\n",
      "  Num epochs = 3\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='240' max='240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [240/240 16:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.299600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.110400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training loop\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2455,
     "status": "ok",
     "timestamp": 1719390291303,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "PyRxiY32R3Jd",
    "outputId": "51d00a55-8086-4456-b3c2-d8720342bcc8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8450433108758422}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on our test data\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1719390291304,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "-aKIHJpCQdAm",
    "outputId": "e2f786a4-78a4-4b92-d464-e3336d92edb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  display: none;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  overflow: visible;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".estimator-table summary {\n",
       "    padding: .5rem;\n",
       "    font-family: monospace;\n",
       "    cursor: pointer;\n",
       "}\n",
       "\n",
       ".estimator-table details[open] {\n",
       "    padding-left: 0.1rem;\n",
       "    padding-right: 0.1rem;\n",
       "    padding-bottom: 0.3rem;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table {\n",
       "    margin-left: auto !important;\n",
       "    margin-right: auto !important;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(odd) {\n",
       "    background-color: #fff;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:nth-child(even) {\n",
       "    background-color: #f6f6f6;\n",
       "}\n",
       "\n",
       ".estimator-table .parameters-table tr:hover {\n",
       "    background-color: #e0e0e0;\n",
       "}\n",
       "\n",
       ".estimator-table table td {\n",
       "    border: 1px solid rgba(106, 105, 104, 0.232);\n",
       "}\n",
       "\n",
       ".user-set td {\n",
       "    color:rgb(255, 94, 0);\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td.value pre {\n",
       "    color:rgb(255, 94, 0) !important;\n",
       "    background-color: transparent !important;\n",
       "}\n",
       "\n",
       ".default td {\n",
       "    color: black;\n",
       "    text-align: left;\n",
       "}\n",
       "\n",
       ".user-set td i,\n",
       ".default td i {\n",
       "    color: black;\n",
       "}\n",
       "\n",
       ".copy-paste-icon {\n",
       "    background-image: url(data:image/svg+xml;base64,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHZpZXdCb3g9IjAgMCA0NDggNTEyIj48IS0tIUZvbnQgQXdlc29tZSBGcmVlIDYuNy4yIGJ5IEBmb250YXdlc29tZSAtIGh0dHBzOi8vZm9udGF3ZXNvbWUuY29tIExpY2Vuc2UgLSBodHRwczovL2ZvbnRhd2Vzb21lLmNvbS9saWNlbnNlL2ZyZWUgQ29weXJpZ2h0IDIwMjUgRm9udGljb25zLCBJbmMuLS0+PHBhdGggZD0iTTIwOCAwTDMzMi4xIDBjMTIuNyAwIDI0LjkgNS4xIDMzLjkgMTQuMWw2Ny45IDY3LjljOSA5IDE0LjEgMjEuMiAxNC4xIDMzLjlMNDQ4IDMzNmMwIDI2LjUtMjEuNSA0OC00OCA0OGwtMTkyIDBjLTI2LjUgMC00OC0yMS41LTQ4LTQ4bDAtMjg4YzAtMjYuNSAyMS41LTQ4IDQ4LTQ4ek00OCAxMjhsODAgMCAwIDY0LTY0IDAgMCAyNTYgMTkyIDAgMC0zMiA2NCAwIDAgNDhjMCAyNi41LTIxLjUgNDgtNDggNDhMNDggNTEyYy0yNi41IDAtNDgtMjEuNS00OC00OEwwIDE3NmMwLTI2LjUgMjEuNS00OCA0OC00OHoiLz48L3N2Zz4=);\n",
       "    background-repeat: no-repeat;\n",
       "    background-size: 14px 14px;\n",
       "    background-position: 0;\n",
       "    display: inline-block;\n",
       "    width: 14px;\n",
       "    height: 14px;\n",
       "    cursor: pointer;\n",
       "}\n",
       "</style><body><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.7/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\" data-param-prefix=\"\">\n",
       "        <div class=\"estimator-table\">\n",
       "            <details>\n",
       "                <summary>Parameters</summary>\n",
       "                <table class=\"parameters-table\">\n",
       "                  <tbody>\n",
       "                    \n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('penalty',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">penalty&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;l2&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('dual',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">dual&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('tol',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">tol&nbsp;</td>\n",
       "            <td class=\"value\">0.0001</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('C',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">C&nbsp;</td>\n",
       "            <td class=\"value\">1.0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('fit_intercept',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">fit_intercept&nbsp;</td>\n",
       "            <td class=\"value\">True</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('intercept_scaling',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">intercept_scaling&nbsp;</td>\n",
       "            <td class=\"value\">1</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('class_weight',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">class_weight&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('random_state',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">random_state&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('solver',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">solver&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;lbfgs&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('max_iter',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">max_iter&nbsp;</td>\n",
       "            <td class=\"value\">100</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('multi_class',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">multi_class&nbsp;</td>\n",
       "            <td class=\"value\">&#x27;deprecated&#x27;</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('verbose',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">verbose&nbsp;</td>\n",
       "            <td class=\"value\">0</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('warm_start',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">warm_start&nbsp;</td>\n",
       "            <td class=\"value\">False</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('n_jobs',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">n_jobs&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "\n",
       "        <tr class=\"default\">\n",
       "            <td><i class=\"copy-paste-icon\"\n",
       "                 onclick=\"copyToClipboard('l1_ratio',\n",
       "                          this.parentElement.nextElementSibling)\"\n",
       "            ></i></td>\n",
       "            <td class=\"param\">l1_ratio&nbsp;</td>\n",
       "            <td class=\"value\">None</td>\n",
       "        </tr>\n",
       "    \n",
       "                  </tbody>\n",
       "                </table>\n",
       "            </details>\n",
       "        </div>\n",
       "    </div></div></div></div></div><script>function copyToClipboard(text, element) {\n",
       "    // Get the parameter prefix from the closest toggleable content\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${text}` : text;\n",
       "\n",
       "    const originalStyle = element.style;\n",
       "    const computedStyle = window.getComputedStyle(element);\n",
       "    const originalWidth = computedStyle.width;\n",
       "    const originalHTML = element.innerHTML.replace('Copied!', '');\n",
       "\n",
       "    navigator.clipboard.writeText(fullParamName)\n",
       "        .then(() => {\n",
       "            element.style.width = originalWidth;\n",
       "            element.style.color = 'green';\n",
       "            element.innerHTML = \"Copied!\";\n",
       "\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        })\n",
       "        .catch(err => {\n",
       "            console.error('Failed to copy:', err);\n",
       "            element.style.color = 'red';\n",
       "            element.innerHTML = \"Failed!\";\n",
       "            setTimeout(() => {\n",
       "                element.innerHTML = originalHTML;\n",
       "                element.style = originalStyle;\n",
       "            }, 2000);\n",
       "        });\n",
       "    return false;\n",
       "}\n",
       "\n",
       "document.querySelectorAll('.fa-regular.fa-copy').forEach(function(element) {\n",
       "    const toggleableContent = element.closest('.sk-toggleable__content');\n",
       "    const paramPrefix = toggleableContent ? toggleableContent.dataset.paramPrefix : '';\n",
       "    const paramName = element.parentElement.nextElementSibling.textContent.trim();\n",
       "    const fullParamName = paramPrefix ? `${paramPrefix}${paramName}` : paramName;\n",
       "\n",
       "    element.setAttribute('title', fullParamName);\n",
       "});\n",
       "</script></body>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7NbUeSn-QSe"
   },
   "source": [
    "## MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1601,
     "status": "ok",
     "timestamp": 1719387317479,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "Z35PD47AXXnv",
    "outputId": "45667c3b-9421-4406-e6e1-a406d803b8fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#This code loads BERT for MLM with its tokenizer, so you can run masked token prediction or fine-tune BERT on a downstream MLM task.\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load model for Masked Language Modeling (MLM)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "5333a40e5cbc4bb5b89628ecaa6608b2",
      "c7a67733b49e48df88a34329bd090349",
      "40f95e431f264d3eb034f09f3f2e9e09",
      "827d0cdd85c14d13b78da5c0d7f054c7",
      "e8403bc0d63b4ec0aa679505a962dc34",
      "bb38873892e847c6b646c9e3cc4c753d",
      "496cfba4c0d04e399c6fde17772fa62c",
      "e21c06a4d4ac4f5987b8e4f34c4b9ac8",
      "c39ac7f5c60e4bf98e64174f21959d9a",
      "1caf892f43c94d50bb31f12b830462c6",
      "526eb0a0a0d246578aa2fb726c7385e4",
      "83160c120ff64c42af6d38bb3fb932b5",
      "aa3c27c5a1254b19a7cfd04f28aee5d0",
      "c350f4d70d5646fc973aa9e68711f830",
      "1ca86de4b7174993942d43b0222f698f",
      "cac994c5cc274c14ba2163c73fa4ef8c",
      "4a1c759888cf472aab34c96be48e95b1",
      "76aeb09a52f04f97aa006927b60bad84",
      "60f082d7f4234bfa985c9241a7e1e9c0",
      "8d84f304aca74bd99d7d624b29630495",
      "71ea5effe952447fa40b578e64e1b868",
      "933e2bdff863421abba4ba875a817dc0"
     ]
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1719387318933,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "zgLardIvEFTG",
    "outputId": "f40b1e95-9a82-437e-ffd8-5e0fdb4be138"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f16b304d1354ca3813b2eb7b55b742a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8530 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79831821c2e54cbf9d77515c529cde09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1066 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_train = tokenized_train.remove_columns(\"label\")\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = tokenized_test.remove_columns(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "x7Yfg_2TECs_"
   },
   "outputs": [],
   "source": [
    "'''A data collator is a helper that:Takes raw tokenized examples from your dataset. Batches them together.\n",
    "Applies on-the-fly masking for MLM (instead of masking everything ahead of time).'''\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "'''For each batch: Pick 15% of tokens at random. Replace them as follows (again, standard BERT trick):\n",
    "80% → [MASK] token\n",
    "10% → random word from vocab\n",
    "10% → left unchanged\n",
    "Model’s loss is computed only on these masked tokens.'''\n",
    "\n",
    "# Masking Tokens\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "wqnA-ROy7ZRG"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForWholeWordMask\n",
    "\n",
    "# Masking Whole Words, it masks entire words instead of just random subword tokens\n",
    "data_collator = DataCollatorForWholeWordMask(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "OduApRY03iOE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\3188571048.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Training arguments for parameter tuning\n",
    "'''TrainingArguments:\n",
    "A Hugging Face helper class where you define all your training hyperparameters and settings.\n",
    "Arguments explained:\n",
    "\"model\" → output directory where checkpoints and logs will be saved.\n",
    "learning_rate=2e-5 → small learning rate, standard for fine-tuning BERT.\n",
    "per_device_train_batch_size=16 → batch size per GPU/CPU during training.\n",
    "per_device_eval_batch_size=16 → batch size for evaluation.\n",
    "num_train_epochs=10 → train for 10 full passes through the dataset.\n",
    "weight_decay=0.01 → regularization to avoid overfitting.\n",
    "save_strategy=\"epoch\" → save model checkpoint after every epoch.\n",
    "report_to=\"none\" → disables logging to WandB/Comet/MLflow by default.'''\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=10,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "\n",
    "'''Trainer:\n",
    "High-level API that abstracts away the whole training loop (forward pass, backward pass, optimizer, evaluation).\n",
    "Arguments:\n",
    "model=model → the MLM model (e.g., BertForMaskedLM) you loaded earlier.\n",
    "args=training_args → hyperparameters/settings you just defined.\n",
    "train_dataset=tokenized_train → your training dataset, already tokenized.\n",
    "eval_dataset=tokenized_test → your evaluation dataset.\n",
    "tokenizer=tokenizer → ensures Trainer can handle padding, truncation, etc.\n",
    "data_collator=data_collator → defines how batches are formed (e.g., with random masking if you used DataCollatorForLanguageModeling or whole-word masking if you used DataCollatorForWholeWordMask).'''\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 389
    },
    "executionInfo": {
     "elapsed": 731739,
     "status": "ok",
     "timestamp": 1719388053472,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "v-cRB3QmEiXl",
    "outputId": "3945078d-51b8-4ba9-9bd4-937cb4e7b858"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5340' max='5340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5340/5340 4:07:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.106300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.885400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.806700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.707700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.667000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.622400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.570500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.509100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.509700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.483900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Save pre-trained tokenizer\n",
    "tokenizer.save_pretrained(\"mlm\")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save updated model\n",
    "model.save_pretrained(\"mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1511,
     "status": "ok",
     "timestamp": 1719388054975,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "HfxN1p8TOg2v",
    "outputId": "e9bcca59-cebb-4297-e675-b3d2a555d36e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> What a horrible idea!\n",
      ">>> What a horrible dream!\n",
      ">>> What a horrible thing!\n",
      ">>> What a horrible day!\n",
      ">>> What a horrible thought!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load and create predictions\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "preds = mask_filler(\"What a horrible [MASK]!\")\n",
    "\n",
    "# Print results\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1719388054975,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "ogk1hJ4zOlAU",
    "outputId": "316b16d0-065b-41e8-c35c-3ce3be481469"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> What a horrible mess!\n",
      ">>> What a horrible movie!\n",
      ">>> What a horrible film!\n",
      ">>> What a horrible thing!\n",
      ">>> What a horrible story!\n"
     ]
    }
   ],
   "source": [
    "# Load and create predictions\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"mlm\")\n",
    "preds = mask_filler(\"What a horrible [MASK]!\")\n",
    "\n",
    "# Print results\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sDqoG2NyeJO"
   },
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Here are a number of interesting datasets you can also explore for NER:\n",
    "* tner/mit_movie_trivia\n",
    "* tner/mit_restaurant\n",
    "* wnut_17\n",
    "* conll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "KGDvXU-ZzJ3J"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298,
     "referenced_widgets": [
      "e8567cb28aff4974acf098b8fc4184ea",
      "a0960ca186e9491fbff044109a197da2",
      "1a56e5d0d26e49ad9bd6a919515de403",
      "2fe1e77e27014f62964c94b62b66f049",
      "2df3bbada30f48068cf8bd414f40d895",
      "4457b446302442e4bcab06fc10eb15ae",
      "23db3d47ec284ebfa6da24d9a58d0881",
      "a2795e523b5a436d9f94b059e7252597",
      "027f9ea43b6647ab91915020a2e25771",
      "15c3f91714a74762989e35950d6c12a5",
      "19b10fadedbd48288026ff8b21148c8f",
      "e4c076fd3de24dba84f4d566d4fca48a",
      "79c5f352cf7f4bf7ace576348e3c55d6",
      "690b323dd52e48b4b130e63d377b4831",
      "dbb5c772fc674bedaee90064d8e4163b",
      "4fe4a9b1182e4383b9ffa8b0b0c0f321",
      "6d12abf106954c2f9008c8505ed6fce3",
      "ff5ebeb9078d4f8b975a8aa59977fae3",
      "befc22d4789e49eb81793e4ce1210f41",
      "18c3194ae3b8497fb325a424dfb95901",
      "848c37b4a0224b4e994896ab75d695e4",
      "bc720c483d74483790aa184e927bd742",
      "7c9beb946034455e8f4e68cf9d3ed451",
      "18f06f815052464aa05c788246507c1b",
      "b58c221705ed490f801fe1cea0a50b60",
      "c51e7b34c1c049c99753132ddf06a8bf",
      "ca05b822e80640728b7b98d75fc0a569",
      "e70e0557dffe4ac4883151f464d88fb3",
      "caab4f8ab71343dc9d949a4b387ff97d",
      "114e90e1dc0b49149e94d9352a389352",
      "95784022cda24c588e9986f5267e3922",
      "1309c3d2eb8d4bd7b8aa8d7176b225d3",
      "67d416d3edb747598eefad5de3969330",
      "ec5ff2eceeaf425bb25a1580b1c86f35",
      "322b02e7719b4795a0afcbbe99074678",
      "376dbc09d30742cf9ea209d51b10bb46",
      "453b539e6093481dabbd4bacc7b746e6",
      "bb87ce6727e349df960b276b31477d42",
      "ddf0589272894ee8bc63371aebae6c47",
      "ce8a5082960345b28883dc1128550675",
      "69a2c842c8e04a00bccfd9534ead0a3d",
      "e4b0a87f816c4762b01e6952a4fb91f3",
      "4956e10177f74c30a4dcec1aee5d84d1",
      "930cc9ef722e4ad09fc5aec2bdb58f51",
      "d133524c37da40a688ae41d0be4fb608",
      "5068f7a72b444b368e3064b523d37316",
      "20fc8f2be16b48a3a330cb07558c4a7d",
      "b6871ea8d9ee464ca3d68d62c0264f65",
      "2ba02b04fbee4db093abeffdc880ce1b",
      "f4bce89187a74fda9f6b8412a886be56",
      "2118f618461a4f45b864b8ca51561ecd",
      "ca9ae83245b3466d961f8741b5b5ca22",
      "c9aa5057f4c64210a50738ab0ae8d92f",
      "bbc7a519dee04caca31b77c03b599cc1",
      "5f33680650354398bd53b210141050ad",
      "ed3c3a380c3a46e9b6eabe8e0df7f7a1",
      "a1cfaa8574e34145a95b170954a6af0f",
      "c94da6e530da46b284bee469474890cf",
      "9e0395ec942f4b42b7983147701806f8",
      "4730ed0d0d9a43d0b8b6a46f3351a91e",
      "6eaad3c842a14b5b86fa908a18d86f20",
      "9437374ca4e44034a7852cc05a345c18",
      "31d743d1493746cdae66dff187dee185",
      "4a2604352c6946e5b6ce02da0e82f00c",
      "243f45f71e3a4385ac447112e2778915",
      "20c7a8103eb44b61b2f221cc5ab5527b"
     ]
    },
    "executionInfo": {
     "elapsed": 19287,
     "status": "ok",
     "timestamp": 1719388740030,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "YOQlMioIGY33",
    "outputId": "3f5e52d8-e736-411c-e468-d63b48828109"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\aitra\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\conll2003\\9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98 (last modified on Sat Aug 23 19:04:56 2025) since it couldn't be found locally at conll2003, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "# The CoNLL-2003 dataset for NER\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1719388740030,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "hOc5J7YgIPl8",
    "outputId": "648a5ac4-0e61-44fa-fea7-60d1af1853fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '848',\n",
       " 'tokens': ['Dean',\n",
       "  'Palmer',\n",
       "  'hit',\n",
       "  'his',\n",
       "  '30th',\n",
       "  'homer',\n",
       "  'for',\n",
       "  'the',\n",
       "  'Rangers',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 22, 38, 29, 16, 21, 15, 12, 23, 7],\n",
       " 'chunk_tags': [11, 12, 21, 11, 12, 12, 13, 11, 12, 0],\n",
       " 'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][848]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1719388740030,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "hEPNSdADeAWD",
    "outputId": "fdb397ea-8105-44b2-b016-c29b69f61f7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-MISC': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {\n",
    "    'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4,\n",
    "    'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8\n",
    "}\n",
    "id2label = {index: label for label, index in label2id.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 886,
     "status": "ok",
     "timestamp": 1719388740915,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "OtRCaz15AyjC",
    "outputId": "b22d94ad-b666-4f4c-a76b-07102752a88c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1719388740915,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "QOrlt03kIt_f",
    "outputId": "4f58559f-5d3e-43ed-a2f9-e6a5d13f5ba0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Dean',\n",
       " 'Palmer',\n",
       " 'hit',\n",
       " 'his',\n",
       " '30th',\n",
       " 'home',\n",
       " '##r',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Rangers',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split individual tokens into sub-tokens\n",
    "token_ids = tokenizer(example[\"tokens\"], is_split_into_words=True)[\"input_ids\"]\n",
    "sub_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "sub_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "2374a01e18eb42af8f9e9448c7500f3f",
      "052d9ca5cba84efda37ddcc4e7122dd3",
      "a11d42f341fa451b85d1042c45041aee",
      "b0dfa37334294bd387caf4193ddb4fa8",
      "c6a90623259049c89167579c116b8a2b",
      "079ab07a92cb4e2985d04dd8428b4286",
      "ab1366b8f75c4d35a70b00a555d45fff",
      "d44f9fd0544f4dca964aa7aa0e792f77",
      "eaf47f4519cb4839a8b33c4c87f1eb2f",
      "99ffc8e69feb461095323a52cd4153c5",
      "c558a4dfcec34484899578274ca46d38",
      "463e4b109eaf4868a6abf5c979f279ba",
      "d4ccf5b35fab4accbce478de57fd52ef",
      "dfa3d3c4f64e487eadc1c288cdbf0463",
      "d997aee4803042c29b88025d9500ec12",
      "8505564f6157421c99e675ab7065dc9b",
      "b907032d76ea427385e59eb1f7c6a5a7",
      "a79dd7dc444243dcbe3d056f9125857b",
      "e1ec63869ec24a5eb4e2bef5c646695c",
      "3816be92ddad4795b714158f1ec8f1db",
      "4fa53ee583bd4a77a685b4b0ed01ea50",
      "53790691595e41c29a84a4e02dbbcbfd",
      "15b5d717e0824b2292b90b4d679e2ca9",
      "02d997424a4b4e26b60023e938edb44e",
      "def65a08a1264375aa007ce1d9205f3a",
      "5ce6635fc9ed4406a63a87baf1e28f17",
      "05679dbc7df549bd9788accb2f7a62d1",
      "e648eec3fd584b6ca370d5c2d804b092",
      "58a9e06b27d34500ab344ad0b77281a6",
      "7f44960348fa48109a8dc0b9f6780f52",
      "889fd835725e4f2480fc8c1dd329299e",
      "8000c805502a4657bc2c657f63af75d0",
      "1fef2fd92da448c8a509ed6fccf71ac7"
     ]
    },
    "executionInfo": {
     "elapsed": 4419,
     "status": "ok",
     "timestamp": 1712752415862,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "7s5A3mzj6TXf",
    "outputId": "b92cc409-96bd-43a0-9e3b-955003157f36"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1a6eba230c4740b2cfcdf9ce127d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8a954abfde4045b58931d804ac2a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1cfcfc056b4e819a79123a892ea6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3453 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def align_labels(examples):\n",
    "    token_ids = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = examples[\"ner_tags\"]\n",
    "\n",
    "    updated_labels = []\n",
    "    for index, label in enumerate(labels):\n",
    "\n",
    "        # Map tokens to their respective word\n",
    "        word_ids = token_ids.word_ids(batch_index=index)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "\n",
    "            # The start of a new word\n",
    "            if word_idx != previous_word_idx:\n",
    "\n",
    "                previous_word_idx = word_idx\n",
    "                updated_label = -100 if word_idx is None else label[word_idx]\n",
    "                label_ids.append(updated_label)\n",
    "\n",
    "            # Special token is -100\n",
    "            elif word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            else:\n",
    "                updated_label = label[word_idx]\n",
    "                if updated_label % 2 == 1:\n",
    "                    updated_label += 1\n",
    "                label_ids.append(updated_label)\n",
    "\n",
    "        updated_labels.append(label_ids)\n",
    "\n",
    "    token_ids[\"labels\"] = updated_labels\n",
    "    return token_ids\n",
    "\n",
    "tokenized = dataset.map(align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1712752662929,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "gIWOMzE-AhTu",
    "outputId": "66cfe4c4-406c-4362-d93f-7999078040f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]\n",
      "Updated: [-100, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "# Difference between original and updated labels\n",
    "print(f\"Original: {example['ner_tags']}\")\n",
    "print(f\"Updated: {tokenized['train'][848]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "bA9nE7E6g97p"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load sequential evaluation\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Create predictions\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=2)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Document-level iteration\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "\n",
    "      # token-level iteration\n",
    "      for token_prediction, token_label in zip(prediction, label):\n",
    "\n",
    "        # We ignore special tokens\n",
    "        if token_label != -100:\n",
    "          true_predictions.append([id2label[token_prediction]])\n",
    "          true_labels.append([id2label[token_label]])\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"f1\": results[\"overall_f1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "m0S4ZajdCaJl"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Token-classification Data Collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 170605,
     "status": "ok",
     "timestamp": 1712753138553,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "S76kRYZtBr5c",
    "outputId": "8c2e866b-e9a9-49e3-ef08-56aab5aacdb4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\AppData\\Local\\Temp\\ipykernel_2308\\1046598781.py:14: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='878' max='878' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [878/878 33:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.232300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=878, training_loss=0.16677095026524572, metrics={'train_runtime': 2018.2841, 'train_samples_per_second': 6.957, 'train_steps_per_second': 0.435, 'total_flos': 351240792638148.0, 'train_loss': 0.16677095026524572, 'epoch': 1.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=1,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 14521,
     "status": "ok",
     "timestamp": 1712753153056,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "ds5osPr9T0pq",
    "outputId": "28e6836f-5eae-43c2-a37f-b92acb9a42b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aitra\\anaconda3\\envs\\llm\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='216' max='216' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [216/216 01:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.14675407111644745,\n",
       " 'eval_f1': 0.9028946697271105,\n",
       " 'eval_runtime': 107.6315,\n",
       " 'eval_samples_per_second': 32.082,\n",
       " 'eval_steps_per_second': 2.007,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on our test data\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2674,
     "status": "ok",
     "timestamp": 1712753204334,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "Q0PAXyzT-N45",
    "outputId": "d29825c0-2be2-48dd-c685-fad114afb369"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': np.float32(0.9826962),\n",
       "  'index': 4,\n",
       "  'word': 'Ma',\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9714426),\n",
       "  'index': 5,\n",
       "  'word': '##arte',\n",
       "  'start': 13,\n",
       "  'end': 17},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.9533223),\n",
       "  'index': 6,\n",
       "  'word': '##n',\n",
       "  'start': 17,\n",
       "  'end': 18}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Save our fine-tuned model\n",
    "trainer.save_model(\"ner_model\")\n",
    "\n",
    "# Run inference on the fine-tuned model\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"ner_model\",\n",
    ")\n",
    "token_classifier(\"My name is Maarten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAfBhqVwC61e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOzKODTW4KPEpkvmaAMgqlD",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3-llm] *",
   "language": "python",
   "name": "conda-env-anaconda3-llm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
