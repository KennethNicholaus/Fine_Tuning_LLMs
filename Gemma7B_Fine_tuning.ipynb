{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3wk7M5nnVgc"
   },
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqkFWxkVnVgc"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# We're installing the latest Torch, Triton, OpenAI's Triton kernels, Transformers and Unsloth!\n",
    "!pip install --upgrade -qqq uv\n",
    "try: import numpy; get_numpy = f\"numpy=={numpy.__version__}\"\n",
    "except: get_numpy = \"numpy\"\n",
    "!uv pip install -qqq \\\n",
    "    \"torch>=2.8.0\" \"triton>=3.4.0\" {get_numpy} torchvision bitsandbytes \"transformers>=4.55.3\" \\\n",
    "    \"unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo\" \\\n",
    "    \"unsloth[base] @ git+https://github.com/unslothai/unsloth\" \\\n",
    "    git+https://github.com/triton-lang/triton.git@05b2c186c1b6c9a08375389d5efe9cb4c401c075#subdirectory=python/triton_kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJq3z_gYnVgd"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527,
     "referenced_widgets": [
      "8c039ec5fb594077aa9947c2683ca1ef",
      "73107ec68ea84a12914293008d2f2cd9",
      "18524360ea164f8794178e7dd4ece59c",
      "9990ddfd1aa94f07b43545d1c8bca2b4",
      "22c213a5fb574eeea5f9a7efab5b1ba7",
      "b372d6ed1c204203be1fac53f2093c62",
      "376cd15963c84026a4ba2a2c212b813e",
      "38f281294af847129355dfa86416ae0c",
      "aa5d182dec464a709c6f3ce95b415304",
      "8f39efb61c224ae18db657ce38efd085",
      "2a2612b9d72c49089ebb79bb28c0c415",
      "607d1555851348b7813f6a3db1844109",
      "c4e07ba599fc462792e39b6f3841ec46",
      "29d35da050f94c17a8b09331e16d9c23",
      "19983e4ce30944c7a57abfe01e463eb0",
      "a0713b54fa2b47c2b726042051640522",
      "f1237a5c19014663b8ec6475ff81091d",
      "585b94dcbd1c4a1595c7c6b110ead7ef",
      "4b3e58cb5db14f4988a3eb953b98e248",
      "ae71957fa4f04efb9e8f207f1d9de48c",
      "9643968ed03642429372c2dac797031b",
      "48bb950cb7224cf681b8892d9bae389d",
      "be2ea37136c24ffab3758cc90ec310c6",
      "d827c81f690044e2b3002e81be8ccc86",
      "88d58b3bc15f4d029f361a5f012f0dfe",
      "18bfa19f04a2490ba5c4097a3d956a07",
      "5b94be536a47455bb802b9e9efb3bc37",
      "ebb49ff5feff47aca6953a77806bfcc0",
      "f3c6916566f0483082b75a6232501001",
      "65d2db12df6942b98bda16b738191f34",
      "341e656e22e24cf0a54484dc1131ac0b",
      "98122f1f5c974405aec8cee21d511235",
      "60ee8e94b3794c6085a03a96058d03ee",
      "d9b1cfdaa58f4a579addc1bfb41e3622",
      "69176a4379e74670a765be4b916e718a",
      "cd691c6e5bf746f3870c3b059f04778d",
      "42db3b1fa57a4d85ad46f5641e3daddd",
      "dd8e22c3182a486b968acfb24757a567",
      "5f96703d9fd64ee7b52b02662e7afffc",
      "51c530ca4981460c99501f5f90f3a182",
      "2d762276a54c4ecb89649d1d58997069",
      "41e0eae9d175446e86c5c84f850b362f",
      "ded74fd1bf114fe1a7c3d1bc0b6dd6ab",
      "f999d6c9069249b9ae9e1a32a3a0a80f",
      "28bf8cd1a1f04fb099ffc36700ead6ad",
      "c5996543c5c346a99000c70e810f8e8c",
      "477177141b7349e9b3e01fdfd845bfbb",
      "686d7f8f60554cdba30eeda79db4501f",
      "ba25ca3bc967493c8d9f53670d6245b9",
      "bd365bd853fd417aa7b7096ea1e9540c",
      "fd15ab7222824c9abcce3a17cc0209af",
      "b2df64020b764343914f9acc97d86076",
      "1b7009babefe4108be77c969c97c6c56",
      "2e560b107cbf4f9ea1b34bf3a3094678",
      "74ebde2ac07d49f0ba65b7d70cea09f1",
      "323c5d1ee6fd4fc99951adda4afb572c",
      "444905088dc045faa382e6fdec70574a",
      "65f08647736f42c285980f4580b8c3f2",
      "ddd55b7ba1164e809f9406bf2f9de9a4",
      "09d35ff962e24e0791932a5d60a8a911",
      "6db32b388f734fd598644ddfef4632f1",
      "50baccf35989487f9bc9049ff4303f4d",
      "479f8e8afeab4bc3ac20363e7dfef770",
      "8ec51fbe49f74f82b0f13c658f5d6bf8",
      "8d925b65a79240f0bad9cd8add2bfec7",
      "cb7de23470ce4dbbbb3a636d1aa0af9c",
      "14dd75fc40d94565b05931f6d9519b8a",
      "6378d55aada8467688da8d1da0c123ce",
      "ff74e51179ab471b898e11008c91629e",
      "3821f16f51ab4f3ebedb06c94d3846ce",
      "1f050ac26f114a36b2c8fbf810084bf5",
      "99dfd860e52240838e9c55238884fcee",
      "a17f3673fb6c4971bd53489a80c12b03",
      "72de77c20e1c4e3982aefb8a6868fed6",
      "ccb4d40f2ede4676a334aed9855aabf7",
      "54355aab70f34cbc8465048d8cdd8cf2",
      "6c279fe5cb444673a65f1caba4648fc4",
      "b1683c2194bf4d34bd61434fcca06c32",
      "d73ccf7259b9439299a1d17cd22b822b",
      "ad6e28f080ef4ee8bb6ec726669df8c5",
      "0c95cd53486241a689301dee6bd3c2d3",
      "ecb9b5a306cc4244a12f8bdd7c65e498",
      "add72aaf688a4ad8bfe7b5ffda08d21d",
      "70f86aee84a143159feded54e0b0e2ee",
      "d22ce9627bdf41f59e74bd46c8e0d921",
      "024cba3b43c840238940ef161521c7cb",
      "83dd0a7d75d544f1a64fb265822b1dc6",
      "28b1a6aef393405ba325d29e470b9332",
      "157cdd563d2145388b8288d7ed981f6f",
      "d302c13ddea44894bad6494309771580",
      "d307e2839dae4480b07e25b1db2ff9e1",
      "8cd9481d509d40d398acda0fe597c999",
      "bb816edcb65640688306f1b099a1a088",
      "c0615e2ed6c246d3bd64e50002f1b5cf",
      "72986da11c5c400b8f3fcf73cebf8af8",
      "83fd58564b7d46c38cff553df21a69c6",
      "75ead08eb8124736800f59c455785cba",
      "040250e6afb74feeb107c69e50a985bc",
      "5d6c9f818ec94c5d9f8b325839371963",
      "097cf7aa8f4344dd84af6021e12ee829",
      "7322a242ad4744168de44963be435725",
      "30fd12adf3a14aad813b0d9b29670596",
      "3a1670c82c4544578816944852a3a48f",
      "fd443c983f1a409aa6be506aea521e9a",
      "88e50815be2a48e2a434b78ea4b98bd2",
      "5e42a9d44ffe44eebf95d3bc0fd0f752",
      "bb24af1cff464e35912adcb7fb2bd070",
      "332a4aedcef1459b8a553a9c8a27a72d",
      "3f023c6bb6604ae9b4c6eea1fd12a905",
      "1be08746d9294ea49380a48182acfaa1",
      "54608166730a4e4aa836a2588faa0f5b",
      "e75762e5993c440da2c0fb38056a56c4",
      "1447cc59ce834e9b950c9f78d557f11c",
      "7b312cbc61c342eda30999be93bda78b",
      "57f520767b4a4cc2bfe993457f9f6799",
      "11ada4258a894a27a4e096257ecac8ff",
      "f527df8dc8734cbcac2bfe27faaa7dfa",
      "b1cdcb9c0b9a463bbbc4a16b64f24e12",
      "0c6c7e5a315e44c0a545515626ef3606",
      "6d1644394190402baf9a58b00b1b3de8",
      "3c88be2e8d5b4559b7c1928e7a46e847"
     ]
    },
    "id": "QmUBVEnvCDJv",
    "outputId": "62fa5df0-0119-443b-84b8-ecc19401ee3b"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "max_seq_length = 128\n",
    "dtype = None  # Automatically detected\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True,  # <--- Enables CPU offloading\n",
    ")\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/gemma-7b\",\n",
    "    dtype = dtype,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    full_finetuning = False,\n",
    "    device_map = \"auto\",  # <--- Auto-splits between GPU/CPU\n",
    "    quantization_config = bnb_config,  # <--- Adds offloading config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVqtZVxxnVgf"
   },
   "source": [
    "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_LK81NRnVgg",
    "outputId": "5243c491-bc32-4a97-f326-fa6862da448a"
   },
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-sFShVvnVgg"
   },
   "source": [
    "### Gemma 7B Reasoning Effort Levels\n",
    "\n",
    "The `gpt-oss` models from OpenAI, including **Gemma 7B**, provide a feature called **Reasoning Effort**. This feature allows you to control the trade-off between the model's reasoning performance and response latency by adjusting how many tokens the model uses to \"think.\"\n",
    "\n",
    "There are three distinct reasoning effort levels available:\n",
    "\n",
    "- **Low**  \n",
    "  Optimized for tasks that require very fast responses and do **not** need complex or multi-step reasoning.\n",
    "\n",
    "- **Medium**  \n",
    "  Balances performance and speed, suitable for most typical use cases.\n",
    "\n",
    "- **High**  \n",
    "  Provides the strongest reasoning capabilities for tasks that demand complex, multi-step reasoning. This comes at the cost of increased response time (latency).\n",
    "\n",
    "You can adjust this parameter when applying the chat template or during inference to fine-tune the behavior of the Gemma 7B model based on your needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxCi64FnnVgh",
    "outputId": "26150958-7208-4dbd-ce07-bdac6748465b"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"Solve x^5 + 3x^4 - 10 = 3.\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, max_new_tokens=64, streamer=streamer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlAzq_RinVgh"
   },
   "source": [
    "Changing the `reasoning_effort` to `medium` will make the model think longer. We have to increase the `max_new_tokens` to occupy the amount of the generated tokens but it will give better and more correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaPPyXN1nVgh",
    "outputId": "ff594b71-a82c-4203-fa6e-f9fd14b210a0"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Original messages (chat-style)\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Solve x^5 + 3x^4 - 10 = 3.\"},\n",
    "]\n",
    "\n",
    "# Convert chat messages to a single prompt string manually\n",
    "# You can build more complex formatting if needed\n",
    "prompt = messages[0][\"content\"]\n",
    "\n",
    "# Tokenize the prompt normally\n",
    "inputs = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "# Stream output\n",
    "streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, max_new_tokens=64, streamer=streamer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0iuyJt7nVgh"
   },
   "source": [
    "Lastly we will test it using `reasoning_effort` to `high`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QrjUXjN8nVgh",
    "outputId": "9db0a3e3-5aae-40b6-8acb-a9b393d0d176"
   },
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = \"Solve the equation: x^5 + 3x^4 - 10 = 3\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(**inputs, max_new_tokens=64, streamer=streamer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6BnnYcbnVgh"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91gfk9L3nVgh"
   },
   "source": [
    "The `HuggingFaceH4/Multilingual-Thinking` dataset will be utilized as our example. This dataset, available on Hugging Face, contains reasoning chain-of-thought examples derived from user questions that have been translated from English into four other languages. It is also the same dataset referenced in OpenAI's [cookbook](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers) for fine-tuning. The purpose of using this dataset is to enable the model to learn and develop reasoning capabilities in these four distinct languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183,
     "referenced_widgets": [
      "8b8eb63337fb428fb0702ab599e2d402",
      "ec16474c3bb2416ea72cda7801911a36",
      "ba78a415e8b8469ea3ca3f4f5fe2d419",
      "2a4965d875f640cf8a10998614308c10",
      "b6bbb3fd3245428c9a56ccb007bdd1ab",
      "f14e045ddcf54eef958e92c7a8616d50",
      "8d0635071af84cf1ac18e9a052087e32",
      "d1c64a303c6541f4a5463748383cecc1",
      "470ed5fc391f4c8fbe4d4f07d5aa3e23",
      "a9bd7392477840acbab43d9263955647",
      "0017ec22a7504941934db02a385dce85",
      "9a430ca8b86e4f279122b45267a038c0",
      "e75b2c318d464bb8b4debc68621cb533",
      "907f9b49253f46638f2c1ecc79116698",
      "8e7481889c1d4d70bbf4f5b0dc849bdc",
      "ad65c3013d2d4cedba1fd98ef835b3b5",
      "e3a9a9b8868e40c3b754b4fb6a299906",
      "29f0d621132742188596ce3a7dfb1704",
      "737f0b3c8edd40c69ac7025c6ee00723",
      "7e5c3cad61f9447dbfdc25e3487223b7",
      "297f17e5d1e743c7acea1d15731d255e",
      "30893988a2a4460696d92911a4ebede7",
      "c13b432ba06341c09746c52307f866aa",
      "1bce340c0f8848fe85db3beaf8dc1ed7",
      "8f3aa28ce7c14c3a97629855721d0c25",
      "62fafca550a7466fb478a161a1e5c541",
      "4f93b270ee7b4eec95113b56214eada8",
      "03a7eaea40cf4eb69b0f0d1e495e631c",
      "3986d3adb14d48e1b5939e68f9d3ffc5",
      "8cb4d60568bf4572a37870b8a1b510b2",
      "97e57af4fdd84d8baeb52fea57b3ab14",
      "602a471c56e54731a847d1b29f72e999",
      "9518e8ada50747818ad94bf81118a964"
     ]
    },
    "id": "62QfuPXBnVgi",
    "outputId": "dfe615ff-591a-4a3d-fb3f-3198626cdd6b"
   },
   "outputs": [],
   "source": [
    "# Define a custom formatter for Gemma-7B\n",
    "def formatting_prompts_func(examples):\n",
    "    convos = examples[\"messages\"]\n",
    "    prompts = []\n",
    "\n",
    "    for convo in convos:\n",
    "        # Just use the last user message as prompt\n",
    "        user_messages = [msg[\"content\"] for msg in convo if msg[\"role\"] == \"user\"]\n",
    "        prompt = user_messages[-1] if user_messages else \"\"\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    return { \"text\": prompts }\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset (e.g., multi-lingual logic questions)\n",
    "dataset = load_dataset(\"HuggingFaceH4/Multilingual-Thinking\", split=\"train\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoVaL_i-nVgj"
   },
   "source": [
    "To format our dataset, we will apply our version of the GPT OSS prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "97ef826a71cf4db6b2487e3ceb610574",
      "5d2597a3407840eeae41ad02a008eae2",
      "33296d2012e3437dac6393b1e447d89a",
      "fd92fe1fac8245faad1d0b4df340eacd",
      "34380cffc7ac48908baaa8103d26b952",
      "f1cb00038b094d079dd924ce3c523a2c",
      "04b1a6ba8ec54e6d8ff2f9406d0e708f",
      "ad39a8481898489b858c2e797faa564a",
      "322de8a1e48a4c7bbe033561f12191de",
      "04bc14d9112242259867abad6efc53c3",
      "2e9287b93e93412b9f2b12cd98d69ab6"
     ]
    },
    "id": "FW-l11GBnVgj",
    "outputId": "ebb65aba-e7d8-4873-99cd-b2b17cc994ad"
   },
   "outputs": [],
   "source": [
    "from unsloth.chat_templates import standardize_sharegpt\n",
    "dataset = standardize_sharegpt(dataset)\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1EmcWNinVgj"
   },
   "source": [
    "Let's take a look at the dataset, and check what the 1st example shows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNgkE5QxnVgj",
    "outputId": "53cd5d37-8372-4d77-8a75-8a07b4fb2c8a"
   },
   "outputs": [],
   "source": [
    "print(dataset[0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ3i-AMFnVgj"
   },
   "source": [
    "What is unique about GPT-OSS is that it uses OpenAI [Harmony](https://github.com/openai/harmony) format which support conversation structures, reasoning output, and tool calling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtdsxyl6nVgk"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "0fc33d9d7b2e486ea16c7e9655d1f078",
      "c4568dd761a140b6bb9d5996a98a22d4",
      "587e44e5af14403582c0b87ef85813b4",
      "1adeb75bbdaa4ef388c82f786916509a",
      "1cce8185eab94b189fee6a7efb0eb3dc",
      "4362a20e703c42d4b0b92dc410d62889",
      "227eab802b6543d8b6915da6fed18c6e",
      "bbd94cb3957e4b0b9fde5ef117753d43",
      "17ee69f3ffdd4985b436803c99a80b3d",
      "5df9512f00d842d5bba5da9f97d703ac",
      "aa886d9ac13d40c2a90625943b782168"
     ]
    },
    "id": "O-XZLeLYnVgk",
    "outputId": "1ffe6822-e7a2-4c69-c764-59933ef359ca"
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 30,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BF6so1JTnVgk",
    "outputId": "80374b48-e979-4f6f-ce86-4ba19d728d21"
   },
   "outputs": [],
   "source": [
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aFaejiSonVgk",
    "outputId": "f9768c59-df45-4b80-b150-2e99036837ae"
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_G3eBV3EnVgk",
    "outputId": "7c86ff1e-b5b5-47f6-bbc4-eec30a219e46"
   },
   "outputs": [],
   "source": [
    "# @title Show final memory and time stats\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
    "print(\n",
    "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
    ")\n",
    "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
    "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
    "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
    "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuK0hVOsnVgk"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! You can change the instruction and input - leave the output blank!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdVCmTuBnVgl",
    "outputId": "266de72f-20d3-4253-fffe-6b2764a5a7d9"
   },
   "outputs": [],
   "source": [
    ", max_new_tokens = 64, streamer = TextStreamer(tokenizer))\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "prompt = (\n",
    "    \"You are a helpful assistant that can solve mathematical problems.\\n\"\n",
    "    \"Solve the following equation step-by-step in French:\\n\\n\"\n",
    "    \"x^5 + 3x^4 - 10 = 3\\n\\n\"\n",
    "    \"Solution:\"\n",
    ")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "streamer = TextStreamer(tokenizer)\n",
    "\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.0,\n",
    "    do_sample=False,\n",
    "    streamer=streamer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e1j8KRb4AwO"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    " Currently finetunes can only be loaded via Unsloth in the meantime - They are working on vLLM and GGUF exporting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ds7ByU7e4KF7"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"finetuned_model\")\n",
    "# model.push_to_hub(\"hf_username/finetuned_model\", token = \"hf_...\") # Save to HF"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
