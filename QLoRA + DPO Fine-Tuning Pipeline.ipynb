{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpBVeU0XX8Uk"
   },
   "source": [
    "<h1 Fine-tuning Generative Models</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Full Notebook Summary ‚Äì QLoRA + DPO Fine-Tuning Pipeline\n",
    "\n",
    "- **üî§ Tokenizer & Dataset Preparation**\n",
    "  - Loaded the `TinyLlama` tokenizer and the `ultrachat_200k` dataset.\n",
    "  - Formatted prompts using TinyLlama‚Äôs chat template (`<|user|>`, `<|assistant|>`).\n",
    "\n",
    "- **ü§ñ Model Loading with Quantization (QLoRA)**\n",
    "  - Loaded the `TinyLlama-1.1B` model in **4-bit precision** using `BitsAndBytesConfig`.\n",
    "  - Enabled low-memory, efficient training via quantization.\n",
    "\n",
    "- **üîß LoRA Adapter Injection**\n",
    "  - Defined a `LoraConfig` to apply LoRA adapters to key transformer layers (e.g., `q_proj`, `v_proj`, `k_proj`, etc.).\n",
    "  - Prepared the quantized model for training and injected LoRA layers.\n",
    "\n",
    "- **üìö Supervised Fine-Tuning (SFT)**\n",
    "  - Fine-tuned the model on instruction-following data using `SFTTrainer`.\n",
    "  - Only LoRA adapter weights were updated during training.\n",
    "  - Saved SFT LoRA weights to `TinyLlama-1.1B-qlora`.\n",
    "\n",
    "- **üìà DPO Dataset Preparation**\n",
    "  - Loaded a human preference dataset: `distilabel-intel-orca-dpo-pairs`.\n",
    "  - Filtered for high-quality pairs (e.g., `chosen_score ‚â• 8` and no ties).\n",
    "  - Formatted prompts using the TinyLlama chat structure.\n",
    "\n",
    "- **‚öôÔ∏è DPO Training Configuration**\n",
    "  - Used `DPOConfig` to define training arguments (batch size, learning rate, cosine LR scheduler, etc.).\n",
    "  - Optimized for quantized and memory-efficient training.\n",
    "\n",
    "- **üèãÔ∏è‚Äç‚ôÇÔ∏è Direct Preference Optimization (DPO)**\n",
    "  - Further fine-tuned the SFT model using preference data via `DPOTrainer`.\n",
    "  - The model learned to prefer higher-quality answers over rejected ones.\n",
    "  - Saved DPO LoRA adapter weights to `TinyLlama-1.1B-dpo-qlora`.\n",
    "\n",
    "- **üîó Merging LoRA Adapters**\n",
    "  - Merged the SFT LoRA adapters into the base model using `merge_and_unload()`.\n",
    "  - Then merged the DPO LoRA adapters on top of the SFT model.\n",
    "  - Result: a fully fine-tuned base model with both SFT and DPO updates, without needing LoRA layers anymore.\n",
    "\n",
    "- **üí¨ Inference**\n",
    "  - Created a prompt using TinyLlama's expected chat format.\n",
    "  - Used Hugging Face's `pipeline()` with the final model to generate a response.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ The result is a memory-efficient, preference-aligned, instruction-tuned TinyLlama model ready for deployment or further experimentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%capture\n",
    "!pip install -q accelerate==0.31.0 peft==0.11.1 bitsandbytes==0.43.1 transformers==4.41.2 trl==0.9.4 sentencepiece==0.2.0 triton==3.1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5luSSUAu_6d"
   },
   "source": [
    "# Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPtcbw38_hVi"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717,
     "referenced_widgets": [
      "0c0285e0913b46638191933995384e81",
      "dcf360a0f47a49e4a45077254414c5a3",
      "15c6aefae79544f9976aaf48c76724f0",
      "14dfbe9d7ea64d948115cbfc419f088c",
      "27f6c79febad4975bad1c6826f56bb3a",
      "e41a3acb92354b39883d0a593cfd134f",
      "5580337a0b914e39997d375ab566d320",
      "0cd0b4ceab9d437f853687736c038724",
      "502d9bd6b8214cd5947b69581d16f43f",
      "a17a283f2efa46a39b5b08a3c9a3b354",
      "5632acd62d3f45a38317099f9d22ebb6",
      "9122ddf6e4c441aa888d237ab95f3db5",
      "7ef9099272e74f3ba85ccff9bec40fdd",
      "4bef0d8577984f5984779e4e80f541df",
      "e6fbcce852524d81af28fc0b308c416a",
      "997a77c0b71b45e785d07824116698ee",
      "cf32f48abc8a4b17a9ba384a50a04990",
      "b342ae0e84234300a0ce4c665ca34f0e",
      "2be641c16747464fa4030e8adfdc3d46",
      "cf001da1deae4f1e97454ea275c0f41a",
      "daa634044dd240efa142477a69643b81",
      "7697e11d19b54b0a895b89db5f1cab1c",
      "9aa1d03488e44364889b9b87db48369e",
      "d8c2e6bc0e4e4dff85217b4d6b294a4c",
      "e353f2f8d5f844658f90c252f35ab205",
      "1c6db3d796204b0587475bab3d3accc4",
      "0e0b34b158f745f0bf80c2b5d9f8047a",
      "a6394bf93d9847019bc69951a1343314",
      "3b4c717b24f04a1481b5ef9316abfda0",
      "72f3a7db32824b94987384879da3547b",
      "8c6d3817f69f433184e0554e66334202",
      "f322425b5f9842c39b083986866ddad9",
      "10962d2bcb2f442881e6be4cc822fb23",
      "ec75adaddca64c2d95ddcba24a3611f6",
      "571991d082184f39b8e75fcac8b40da9",
      "00303d6943e94fa3a18e180e119825e3",
      "564fd5d03ddc4db898de20119b596488",
      "6339b2becd1243eaa4bcc1dec80c70ce",
      "aefa2e2b6a3f4f75bc3ec84138a58272",
      "737fbc0ba9684ec68eeed5e7b4958a42",
      "01c18865664e471982c22916a8cad324",
      "7c2bfb3fc0a74868baae251eb02c1b5e",
      "e967b96a23994543ac1c57e86a32a72f",
      "3a7583cae9aa45b4b51a3630b07f176a",
      "a7b18257b8d445eab00a2ad90dde1935",
      "e01822a863864a878d7fc8308b3524b9",
      "c146636d840f4f81bb7650855621e779",
      "fbed7ecb20094e9a8cf0d35713561cd2",
      "0bd9340be6ba4edb9eb571ec25bf659c",
      "3f3ca64e868c41e18d07d47485d55b89",
      "315d7aa759cb4fc7a5234fe71fcb2f20",
      "b799a5db12964557b723e296b8682683",
      "cf2f19683db94c549f819ac419ba6cd6",
      "7f5806d80d484a83b7fb2fa21941b4b4",
      "a109942f24ed47ad9f7be28c56cdb1f3",
      "4c27c2ebc6c04157910d64034bfce031",
      "87f9cb64141c4afabedbc608ec00843e",
      "f558f81464b64db0bdc7f212ae39f2f7",
      "d29ea76455374ea1b16400c0108389cf",
      "a327cfa279b941309db9d501da4ab103",
      "7995c22adaba4f73a94da0dc49e06253",
      "21c143577ecc462b97251df3fdeee5c6",
      "aaa7114d60674c0b9439a9bb15d878ae",
      "728edb90e05e48419efde0fbc4b1854c",
      "0640421a36dc490f8ba89f550a913148",
      "0e72b56b079d4e20910bdc9fe1ea9192",
      "ebfdda7e72694e2e96dc5b894d624959",
      "98caa4ec23d44d08bb5c72756556eae2",
      "0df385f54f2a4b289b8fb453bcc05c89",
      "775f135d2ae3404fa3aa31bc3e137205",
      "40da08ae319e406c988ec665a40c7017",
      "7f9c6d434da14849b3d1965aa62083bf",
      "3cf04c5fa94f4400b8b71657abd1105a",
      "717649a83f6a440d89e49d675f2b035c",
      "3b3362cef58443e19c88cba029895229",
      "813b2c775ef34c20b9b2471b40d189b4",
      "1b19e64c2b3b444b93a3f50adb3ecbef",
      "8c2509bb3a3244f8bee5cd03b3a63b01",
      "5b44bf29b5f24f79aef39b189403bc4d",
      "ceb0cb33a74e464683351014cb2777b9",
      "75255917e2b8415a91e06eaf9261a432",
      "0f4374fe6fb946959bad87e95dc641b5",
      "67386e8a10e8437dba09bcd15b9ca95e",
      "f3db9002e1db4f1095c8d256d88b77bd",
      "6d288ede18de4c7daa8c24bbe3e734cb",
      "e514be007a6e413695acb9ab7541e1e7",
      "67bcaf2825584c40b56c22000b1ef813",
      "dd6840b67b064e998dbcc5a66a1924b1",
      "593b914a05834c71a737480e32e080af",
      "20bbacc57f0944efbc53beb5162e3949",
      "5295e8e550324fef87dbfd6c7e10d960",
      "14e82108cbc9491dbed6b426ca3238d6",
      "1a7bd5986d3949318b8be938be47cd75",
      "371426080f734157ba6c90fcb5b06d32",
      "64b9301aa9774da28ee49bed3e3a0c8e",
      "a97d7b4039f54cfd853c03ad1dc62e7f",
      "d98b4f9684cb46b5be80154f76a1695d",
      "1b0cfe5b09d549fa98b3a4abcbd0be42",
      "25de49bee947408e970a951b78fdf818",
      "2cd7a2846de944cb8f3f046042b1c259",
      "3644a994605e42b6885b41b6ad9d4039",
      "52278a1aa6344c798d8cca868ae72aa0",
      "65c42da7b6334852ab88e93d366c6d76",
      "3d3f29b0df5a479083856f3962dc15a8",
      "4461deb4578b414681111aa897e4cd6c",
      "2275c7f8c6424a479cb241213c5cee86",
      "8a6cb467bb3e4bd985948490bfe5a131",
      "12740f7ada4742e2b6d80b5bd6b74607",
      "f20d3572b6eb40f88436c28025b81bf7",
      "ee9a1521598640eba1a6ddec51fc2684",
      "c0270f68d92347cc8a0be3c40166cbe6",
      "754aefe850fd41b58695eaa2b11c2642",
      "1674cc41b1dd460aab86339c96ea25dc",
      "d0e398214f1041ad92f693b8d27b6aa2",
      "f9698c8009694677a3001f7d26df3282",
      "cd68b532faaf4339b38ee45f05067376",
      "0146e7107826418cbcf12224d1d3eff9",
      "abd89fffda5a4caf80ea321921699eed",
      "f583bba54fd44c9994b837a1d5a1f4cc",
      "1af514f472e241deb1d3110dee38ff1d",
      "ffc03d8ef2a247c1a0761c7180c7b0eb",
      "7068dee2697343a88b82236f35dbd325",
      "b4df107982fa482bb4951a3e1ea5ae3e",
      "ba287f57e76d404dbee0d5384d5eae9f",
      "893c0cf2bf71484d9f9a476ed7b7060b",
      "b5fade77a4c741ccae482a3085a79254",
      "be12786eefed4d72babd91fbef0703f1",
      "4432f74694924808b4d77fd80d925121",
      "18a2b862f3364f2a97adf727b9997830",
      "bdaedbfa2fe143b881de9ac928f260f1",
      "0541ddcf2e6d415aa11ed9be9518409e",
      "78270dd565a347dfb00cbb1ee54b474d",
      "473de04017ba465982c4e9e6a15d7ad9",
      "d2b3525bdd3944db9b00bd26addb2046",
      "2ddc6ec3271d4417a3d9b0bf43a66f60",
      "dfb5602808f64aeb901fe73f9d0ae3dd",
      "007a8d7830d345f19f044c377de3034f",
      "dc07ee7aa1e24c09aca0c8837fea0e2b",
      "4ac1d624ec9a4a6eab38801b1e8605ea",
      "3e1df78695db4902b88be100a5880d64",
      "88235119e9774ebfb91c16632420501d",
      "c9af312b93ef4a54a1bba402966891ed",
      "f3e1492ca4fc46c8b3e209fc53edc55b",
      "3de01e557b784bd387bd6985b05c58d2",
      "531aa8d491394608ad78374c96e1f182",
      "266bf17916d14b309031ef704ebf8b16",
      "034fe1a3c54c4aa5afab13b26946c390",
      "8fb7ab95392f41aba2884c67b4970b93",
      "5600fc1f67c64444b22f26b040e53bad",
      "603774211a8f44409f626aec41739f8b",
      "000129269bda41fc95af5d86e32037e1",
      "0d7ed05129774701a0e1fd5d9dee938e",
      "4fa7fbd7f10c43239619efe8c94891c2",
      "a77b8d97a3ca47fdabf6d3f154440332",
      "f78630d608e646a985d1af14bbe9108d",
      "89f8d133c512401caa43f4adc82923db",
      "38ed39fcb5854a5693cea58d2e7df3da",
      "5c22e529a1d345f58b2a35b932434ace",
      "ddd79e89dbd54530905cf3c11fedd4e6",
      "8db41e735e454cf187047c2453e6097d",
      "381293f50a7c41518b46e5dba81162b2",
      "658c9da8d4994da9bd6fc900959f79aa",
      "8d3012ca9c4a4b75bce993cf668fd062",
      "475082257c81404c8b1fd270f6dbf895",
      "ea68d2393dae4db6b7947d9589176c66",
      "f6768e31065f48598a625ed8831d69ac",
      "f782508c19df4dfba8a1e3bf14c50131",
      "9ea8370dddd1469daf46c6e9c9784c59",
      "ed09b731dfae40feb977fd982e92d237",
      "550634ce52b24efb9b29231ddd2bcdd8",
      "9d3cc2f714be4b8e9605f92682d7ac73",
      "a00545a641274994955018848dba8622",
      "c2928952588648fa824632bb43e75712",
      "6ae43605a1fb4dfaa38214c118201269",
      "57318597fd344d0a8bd0cef8855efe53",
      "5369f8381c0b43afa2bcb7193db21a39",
      "5994759ae74240b396b00df3d2e87caf",
      "6c2124867ce14599917c4f10a211911b",
      "4a575c47233e4722bc9252f8d650d014",
      "48869b2592504cc7a0d4dc5a8eb009ae",
      "3fa9beddb8304d3e8610923802ff6e25",
      "588bd78207204c74a4b9f45cd660f452",
      "af85f9be14e040ad9a38390d6629646d",
      "7b54eabb0cbe4966a0fdab4b3aab652d",
      "c93c5cb2419847c5a305c5376e3bcac1",
      "fac0d8b63fb44fd195c1b80d91037055",
      "504065bf78444cedaab7c54a4d6a888f",
      "79cb1e5865cb4cf1a3ec6592b2b70a87",
      "f659f53ffde94033b805b8166fc7b861",
      "61a6e7784a0d43d086ee4ac57fb138d4",
      "abd923ac9c324e76a8305bb624934ecc",
      "c5a657ab85c14fc9a8fe0d53e51edac9",
      "d285bec3cce74bfab2e54d764192cf94",
      "faf7e7fea65246a788edb30cc32f430e",
      "3301916017cc47ccacecdec36b37833d",
      "da88cae9dab740548ca0593208ff8e41",
      "05db7dee35e8409791c0b12711e92d4f",
      "a23266458ea94362bf5f3696429327ce"
     ]
    },
    "executionInfo": {
     "elapsed": 31435,
     "status": "ok",
     "timestamp": 1719390601273,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "SqeZchJiOXdd",
    "outputId": "516dbb28-1771-4e35-bc2a-3317a56960d8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:01<00:00, 1837.92 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "# Load a tokenizer to use its chat template\n",
    "'''This loads the tokenizer associated with the \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" model.\n",
    "This tokenizer contains a chat template, which defines how conversations (e.g., <|user|> and <|assistant|> tokens) should be formatted.'''\n",
    "template_tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "\n",
    "'''This function takes an individual example (a dictionary) from the dataset.\n",
    "It extracts the messages field, which contains a conversation history (a list of dicts with role and content, e.g. user/assistant).\n",
    "It then uses the tokenizer‚Äôs apply_chat_template() to convert this conversation into a formatted text prompt that the TinyLlama model expects.\n",
    "tokenize=False means it returns a string, not token IDs. The function returns a dictionary with one key: \"text\" (the formatted prompt).'''\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\"\n",
    "\n",
    "    # Format answers\n",
    "    chat = example[\"messages\"]\n",
    "    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Load and format the data using the template TinyLLama is using\n",
    "'''Loads a subset (\"test_sft\") of the \"HuggingFaceH4/ultrachat_200k\" dataset. This dataset contains synthetic chat conversations.\n",
    "Shuffles it with a fixed seed (for reproducibility).\n",
    "Selects the first 3,000 examples after shuffling.'''\n",
    "dataset = (\n",
    "    load_dataset(\"HuggingFaceH4/ultrachat_200k\",  split=\"test_sft\")\n",
    "      .shuffle(seed=42)\n",
    "      .select(range(3_000))\n",
    ")\n",
    "'''Applies the format_prompt function to each example in the dataset.\n",
    "As a result, each example will now include a \"text\" field that contains the prompt formatted with TinyLlama's chat template.'''\n",
    "dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1719390601273,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "dtl2xZptgyDf",
    "outputId": "304c49f2-16c8-47ad-f8fb-4d975012e6d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Given the text: Knock, knock. Who‚Äôs there? Hike.\n",
      "Can you continue the joke based on the given text material \"Knock, knock. Who‚Äôs there? Hike\"?</s>\n",
      "<|assistant|>\n",
      "Sure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold outside!</s>\n",
      "<|user|>\n",
      "Can you tell me another knock-knock joke based on the same text material \"Knock, knock. Who's there? Hike\"?</s>\n",
      "<|assistant|>\n",
      "Of course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here and let's go for a walk!</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of formatted prompt\n",
    "print(dataset[\"text\"][2576])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CyuLZGizDqUB"
   },
   "source": [
    "## Models - Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "1b4de2592d454a7ba7560ea6849dc6ba",
      "16f636aa216e44a1b9ebb66961b45361",
      "d55c8156644740c48012383eb903b7c5",
      "b40ba0141f194f36b95882644fb2a41e",
      "e8499c18c273492d8fb55fdd50f8f9d2",
      "db0c43c383d3479989e3a307a8c635fd",
      "2e5ccfc7cf4e45cbbdf74add6bf2fdf2",
      "b66b520fbdf84c78adc2f16910ed7a2a",
      "bb4131ce7126465ba7f0107f51d4bd5a",
      "865041ec26e3499cb66da96354bc9a7a",
      "32a2bd764cae40d59467ea71dccea11d",
      "b226cae1a8fc4b8384e0f5fc67d94d89",
      "220334bb55104a01b4e112a05a79db77",
      "a956b9fa522e4d0fafc8fb415ac109ec",
      "a6ae124d59814210913da1e50e82a90b",
      "c03dd90be661408c8d94238ce901081d",
      "23399b989c4449e0ae28c9932b104ba4",
      "16a053e1fc61408a97c650b58fd56913",
      "373a8dedb2b6498c8831f01912e87f10",
      "ec9bf18f5baa45d6a90c1fd2be0e941f",
      "88e2d8f90d814e61b2b8993f12f24d0c",
      "38e7e69833ec4298872afb91e3ba76a0",
      "da07fb8f4a204a4aa9125ddb89526992",
      "3071cd29353c4137a104b974efb903fb",
      "e650428153f441c19a9ce60c7f36b726",
      "6f6d884827dd403280cc2bd44febf8f7",
      "505f2e0ae82f4d5190e7f0a61c693c7d",
      "3ef9ff15982048bcbd9667af97be7a91",
      "f7e0a4c846264a3ebf8bec09a6520674",
      "ac64efb662ca4a2593609ee2797cb188",
      "2211c8ccb3eb4d0d8e5d7ce14e09c843",
      "dea87b44934648cb86d95f4f081005b1",
      "cf4f55ef9e2f450b891149a5cf3d190d",
      "4f88583ce59b467cb9791d09f1d57afd",
      "825cf142c3a842ccb3f7001138a11930",
      "12959d7e56374d8ca49ff18544b4db7e",
      "f301f0072ff649f3a75dea4ed687c294",
      "37273f57135141deb8cb53d858669834",
      "3c0da0f585f64a489a28fb6afa1e6f5f",
      "5b4f672bcb7546eca0e2dbce43900798",
      "e918450ee4b74d03bab029dd7230728e",
      "57040592ffb3433fbdfe90ea1a52d1ab",
      "0ab39d8c544a43bcade2ab94f2a61a0a",
      "fedef8865ad740d0a5cf4167b0067bf6",
      "7ab5f2a459764205ae4263b22f64a7aa",
      "f6092670ea49450e93fbf62eab75d996",
      "6c042632a5b94c5bb9b387ad17920908",
      "7ddca642a1bb4605ad508b1d56c3a61f",
      "1d44d180cee1440ca280e7004aa9bbda",
      "33fd53bddde646cf99019f98e04cb1d5",
      "744008ceeebd4809969b054d3a09d6c5",
      "1deb1756e91941af91bc2f404b5c52c9",
      "9e2a150555434cf39df5da21932a57bf",
      "397231517f4e46fbaef4408d36ecb1c8",
      "1c5d2c4d4d74406fbd4a3fb5b528eed9",
      "a641c51801964d749c11be9ecdaf8749",
      "4375456a00da42c4af307d3b0680ec02",
      "51778001c9214474870749b4af4e2d23",
      "63ae62bab5394d848265983975840f53",
      "e1337ad30ae74bf28c4f88ccecb24c06",
      "a80691a3b01a4c409c49833aa4e2c4ea",
      "f51f84cc3c9e41ea82720a9ed41cfbe4",
      "ec18963535394ba2acef196aff2603b9",
      "9666d1392eb5438b84b76dcfd3222992",
      "cd70eac32fea43a3bfb08cf52d74ae4b",
      "4f4660a0cec2426e8d7ff0b673de28be",
      "5f59b68201d94411b978417aa902e747",
      "e45f18c8c9b447eda9c64a47df8d4881",
      "179ee7d876784842b278c92ce0ac4f7d",
      "e664c66f54e9465c8bd404b2ae5aaf4e",
      "a55c2efc970b481a8fb1ad771c6fc1ee",
      "686140ba3e5b428088de8d94b6e3c707",
      "601b905edc4d410184ee34f83d993971",
      "b1738e5723ca4a4ca82f9038897ce547",
      "aa3dc6d6d4d44b2b8d25ab7fbb4bf224",
      "ad68d7d2d6ef4d90aef6d7536d61bd5b",
      "9fe1ce51c081490bbbfc9c3688bc3860"
     ]
    },
    "executionInfo": {
     "elapsed": 22033,
     "status": "ok",
     "timestamp": 1719390623304,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "M95Y207T7wSp",
    "outputId": "5ed465a8-9fa9-4c8e-8b50-bdc3630bbef1"
   },
   "outputs": [],
   "source": [
    "'''torch: PyTorch library for tensor computation and deep learning.\n",
    "AutoModelForCausalLM: Automatically loads a causal language model (used for text generation).\n",
    "AutoTokenizer: Loads the tokenizer that matches the model.\n",
    "BitsAndBytesConfig: Used for configuring quantization (loading the model in lower precision to save memory).'''\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "'''Specifies the name of the TinyLlama model checkpoint you're loading.\n",
    "This is likely a checkpoint partway through training (e.g., at 1.431 million steps on 3 trillion tokens).'''\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# 4-bit quantization configuration - Q in QLoRA\n",
    "'''load_in_4bit=True: Enables 4-bit model weights. bnb_4bit_quant_type=\"nf4\": Uses NF4 quantization (a newer quantization type with better performance for LLMs).\n",
    "bnb_4bit_compute_dtype=\"float16\": Uses float16 for computations. bnb_4bit_use_double_quant=True: Applies nested quantization (quantize the quantization constants), further compressing the model.\n",
    "This makes it possible to fine-tune or run inference on large models using a GPU with limited memory.'''\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n",
    ")\n",
    "\n",
    "# Load the model to train on the GPU\n",
    "'''Loads the model with the name model_name. device_map=\"auto\": Automatically places model parts on the appropriate GPU(s).\n",
    "quantization_config=bnb_config: Applies the 4-bit quantization settings defined earlier.\n",
    "This step loads the model in a memory-efficient, quantized format, ready for training or inference.'''\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "\n",
    "    # Leave this out for regular SFT\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "'''use_cache = False: Disables caching of past key-values during training. This is necessary when doing gradient checkpointing or training with sequences of variable lengths.\n",
    "pretraining_tp = 1: Sets the tensor parallelism factor to 1. Relevant if using tensor parallel training. In this context, it just ensures compatibility.'''\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "'''Loads the tokenizer associated with the model.\n",
    "trust_remote_code=False: Avoids executing any custom code from the model repo. This is a security-safe default.\n",
    "pad_token = \"<PAD>\": Sets the tokenizer‚Äôs pad token (TinyLlama may not come with a default one).\n",
    "padding_side = \"left\": Pads on the left side (important for models trained to generate from right-aligned text, especially with attention masks).'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1iGIch-sAMC"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86o1T5n4DziD"
   },
   "source": [
    "### LoRA Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0tYs1ZhYDyw9"
   },
   "outputs": [],
   "source": [
    "'''oRA is a parameter-efficient fine-tuning (PEFT) method. Instead of updating all the weights of a large model, it adds small trainable low-rank adapters to certain layers (e.g., attention layers). This:\n",
    "Greatly reduces memory and compute requirements. Enables fine-tuning even large models on consumer GPUs'''\n",
    "'''LoraConfig: Used to configure LoRA training. prepare_model_for_kbit_training: Prepares a quantized (e.g., 4-bit) model for training.\n",
    "get_peft_model: Wraps the model with LoRA adapters.'''\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Prepare LoRA Configuration\n",
    "'''| Parameter               | Meaning                                                                                |\n",
    "| ----------------------- | -------------------------------------------------------------------------------------- |\n",
    "| `lora_alpha=32`         | Scaling factor for LoRA weights. Controls how much they affect outputs.                |\n",
    "| `lora_dropout=0.1`      | Dropout applied to the LoRA layers during training.                                    |\n",
    "| `r=64`                  | LoRA rank ‚Äî controls how many parameters are added. Higher = more capacity.            |\n",
    "| `bias=\"none\"`           | Do not apply LoRA to bias terms.                                                       |\n",
    "| `task_type=\"CAUSAL_LM\"` | Specifies this is a causal language modeling task.                                     |\n",
    "| `target_modules=[...]`  | Which layers LoRA will modify. Typically includes linear layers in attention and MLPs. |\n",
    " '''\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Scaling\n",
    "    lora_dropout=0.1,  # Dropout for LoRA Layers\n",
    "    r=64,  # Rank\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # Layers to target\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# prepare model for training\n",
    "'''This function prepares a quantized model (e.g., 4-bit) for training with LoRA.\n",
    "It: Casts normalization layers to float32 for stability. Makes embeddings and outputs trainable if needed. Sets requires_grad=False for frozen base model weights.\n",
    "This step is crucial for compatibility with quantized models like those loaded with BitsAndBytes.'''\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "'''Wraps the base model with LoRA adapters using the config defined earlier.\n",
    "Only the small LoRA layers are now trainable ‚Äî the original model weights remain frozen.\n",
    "After this, you can train the model using a regular PyTorch or Hugging Face training loop.'''\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zhbh7kKuD24o"
   },
   "source": [
    "### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TwxZkx80G6bO"
   },
   "outputs": [],
   "source": [
    "'''this code is setting up training hyperparameters for a Hugging Face model using the TrainingArguments class from the transformers library'''\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Training arguments\n",
    "'''| Parameter                       | Description                                                                                                                           |\n",
    "| ------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `output_dir=\"./results\"`        | Directory to save model checkpoints and logs.                                                                                         |\n",
    "| `per_device_train_batch_size=2` | Train with 2 samples per GPU (or CPU if no GPU).                                                                                      |\n",
    "| `gradient_accumulation_steps=4` | Accumulate gradients over 4 steps before performing a backward/update step. Effectively simulates a larger batch size of `2 √ó 4 = 8`. |\n",
    "| `optim=\"paged_adamw_32bit\"`     | Optimizer used ‚Äî this is a **memory-efficient variant of AdamW**, used with quantized models (4-bit/8-bit).                           |\n",
    "| `learning_rate=2e-4`            | Initial learning rate for the optimizer.                                                                                              |\n",
    "| `lr_scheduler_type=\"cosine\"`    | Use a **cosine annealing** schedule for learning rate ‚Äî starts high and decays in a cosine curve.                                     |\n",
    "| `num_train_epochs=1`            | Train the model for 1 full pass through the dataset.                                                                                  |\n",
    "| `logging_steps=10`              | Log training metrics (like loss) every 10 steps.                                                                                      |\n",
    "| `fp16=True`                     | Use **mixed-precision (16-bit float)** training to reduce memory usage and speed up training on supported GPUs.                       |\n",
    "| `gradient_checkpointing=True`   | Save memory by recomputing activations during backpropagation (trades memory for compute). Useful when training large models.         |\n",
    " '''\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtwIo5a0D6f1"
   },
   "source": [
    "## Training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "1b741e85fcf1458f9875c12a9640dfee",
      "d034f23fd4df4e3296477d8dd76be5b1",
      "bee0b0ce2fb84c5eb67a04ced69752d1",
      "4d39d09e1e2648b1b5295f192e9ad356",
      "3f733eb54fe54d879c97dba7a5204ddd",
      "e9dab506c3b242d7b6228394ada6084b",
      "e7d4893b696c4941bf29d349eb2ceabb",
      "6d7ee17aa7024c8088c374781348f9f0",
      "7d8478e66e394f0fb077853e5319ee6a",
      "630e317f036f41f4a9852f7df81eef83",
      "eb35394940c74f60abe2daaeb243fa88"
     ]
    },
    "executionInfo": {
     "elapsed": 774977,
     "status": "ok",
     "timestamp": 1719391399990,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "B2D7RVihsE7Z",
    "outputId": "1a9f8125-6d39-410e-ff94-9a9ac493ff25"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/transformers/training_args.py:1965: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
      "  warnings.warn(\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:269: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:307: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3000/3000 [00:01<00:00, 2278.40 examples/s]\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:397: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 17:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.670900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.488000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.477800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.390400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.494900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.427600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.414300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.377200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>1.332300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.346200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.411400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.419200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.474600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.404400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.342400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.387200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.465700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.434100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.376300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.395400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.438100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.387300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.444300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.452000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''Imports SFTTrainer from the trl library ‚Äî an extension of Hugging Face‚Äôs Trainer designed specifically for fine-tuning large language models.\n",
    "It's ideal for chat models, QLoRA, LoRA, and other efficient training methods.'''\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "'''| Parameter                   | Description                                                                                                                                                             |\n",
    "| --------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `model=model`               | The model to fine-tune ‚Äî already set up with LoRA/QLoRA adapters.                                                                                                       |\n",
    "| `train_dataset=dataset`     | The training dataset ‚Äî should contain a `\"text\"` field with formatted prompts.                                                                                          |\n",
    "| `dataset_text_field=\"text\"` | Tells `SFTTrainer` to use the `\"text\"` field as the input for training.                                                                                                 |\n",
    "| `tokenizer=tokenizer`       | The tokenizer for tokenizing text prompts.                                                                                                                              |\n",
    "| `args=training_arguments`   | Training hyperparameters (from `TrainingArguments`).                                                                                                                    |\n",
    "| `max_seq_length=512`        | Maximum sequence length ‚Äî inputs longer than this will be truncated.                                                                                                    |\n",
    "| `peft_config=peft_config`   | PEFT config for QLoRA. This enables adapter-based fine-tuning. <br> üîπ **If omitted**, the trainer performs **full fine-tuning**, which is slower and uses more memory. |\n",
    " '''\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Leave this out for regular SFT\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save QLoRA weights\n",
    "'''Saves only the LoRA adapter weights (not the full model) to the directory \"TinyLlama-1.1B-qlora\".\n",
    "This is much smaller than saving the full model and can later be loaded using PEFT.'''\n",
    "trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsIBfv1PsId-"
   },
   "source": [
    "### Merge Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "M6cPdde4Z-ks"
   },
   "outputs": [],
   "source": [
    "'''Imports a PEFT-aware model class that can automatically detect and load:\n",
    "The base model Any attached LoRA adapters (used for QLoRA fine-tuning)\n",
    "This class is a wrapper over AutoModelForCausalLM, specialized for PEFT.'''\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "\n",
    "'''| Argument                 | Description                                                      |\n",
    "| ------------------------ | ---------------------------------------------------------------- |\n",
    "| `\"TinyLlama-1.1B-qlora\"` | Folder containing the adapter weights (from `save_pretrained`).  |\n",
    "| `low_cpu_mem_usage=True` | Optimizes memory usage while loading (helpful for large models). |\n",
    "| `device_map=\"auto\"`      | Automatically places model components on available GPUs or CPU.  |\n",
    " '''\n",
    "\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "'''This merges the LoRA adapter weights into the base model weights.\n",
    "After this, the model no longer depends on PEFT or LoRA:\n",
    "It becomes a standard Hugging Face model, ready for deployment or inference.\n",
    "All parameters are stored together ‚Äî no adapters.'''\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPRYGimIsM2-"
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6781,
     "status": "ok",
     "timestamp": 1719391410095,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "15dJC3ZrdVnK",
    "outputId": "3095ed46-5bb8-4288-b3a0-05d7daeaefc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Tell me something about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) that can generate human-like language. They are trained on large amounts of data, including text, audio, and video, and are capable of generating complex and nuanced language.\n",
      "\n",
      "LLMs are used in a variety of applications, including natural language processing (NLP), machine translation, and chatbots. They can be used to generate text, speech, or images, and can be trained to understand different languages and dialects.\n",
      "\n",
      "One of the most significant applications of LLMs is in the field of natural language generation (NLG). LLMs can be used to generate text in a variety of languages, including English, French, and German. They can also be used to generate speech, such as in chatbots or voice assistants.\n",
      "\n",
      "LLMs have also been used in the field of machine translation (MT). LLMs can be trained to translate between different languages, and can be used to generate high-quality translations in a variety of fields, including business, medicine, and education.\n",
      "\n",
      "In addition to their use in NLP and MT, LLMs have also been used in other areas, such as speech recognition and natural language understanding (NLU). LLMs can be used to recognize speech and understand what is being said, and can be used to generate responses or recommendations based on the context of the conversation.\n",
      "\n",
      "Overall, LLMs are a powerful tool that has the potential to revolutionize the way we communicate and interact with each other. They are being used in a wide range of applications, and their capabilities are only expected to grow in the future.\n"
     ]
    }
   ],
   "source": [
    "'''Imports Hugging Face‚Äôs high-level pipeline API, which simplifies running inference for tasks like:\n",
    "Text generation, Sentiment analysis, Translation, Summarization, etc.'''\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use our predefined prompt template\n",
    "'''This format matches the chat template that TinyLlama was trained on, ensuring better and more coherent responses.'''\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Run our instruction-tuned model\n",
    "'''This pipeline internally: Tokenizes the prompt, Runs it through the model, Decodes the output back into readable text'''\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JNfYZe9vCb8"
   },
   "source": [
    "# Preference Tuning (PPO/DPO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar2h9kZ9qmEG"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246,
     "referenced_widgets": [
      "d330d84ac98a4d14b51ffab13277e501",
      "2ed3f903f58e42b4a10c8937e7e8cdf5",
      "415333d658284e5f9566065f9bfc4808",
      "76038e4dce21442caa069273e8c22e42",
      "914814f0575948c688991533f85e59dc",
      "e78efc4e689745d69c6d91ac71d51f39",
      "f113f1d6bf9244c98c89f5610e371431",
      "67066b2143a74269bb15a97ace2f0ae2",
      "45b29b925a224dbebf61f67bf359f393",
      "f6a86cfcc1d347e5b182a9abf57b15b4",
      "9dedc5f35e9241a6b9aff057c1c6ef9b",
      "73e8b10b8ee349d1b91dff30f885e335",
      "79da0fb8fad247c1a5e6a5b8bde4d498",
      "c6ff9de93d92425481c29123ac68bf76",
      "c800d5aacd2f4844856c505f11e09e56",
      "0c5019fa5fc74a81a6b11afc49fe135c",
      "c9c3dad975c4436499c3ac3df06bad39",
      "78fb4c59298b4b10b3dd3d5707ae0d81",
      "a7ed257939254f11b1494bf7ae6d42f9",
      "7a856d774904424699aec1f2e9479016",
      "149367fc059e47f990dde648af05d17c",
      "3245f7bd4eb244b587eb15e3cd0b1d80",
      "063c3f5d0ed64ef4901efb5a4fd64149",
      "d571423cdf204acc8ce3e7a4da3e2526",
      "06220ed92658447fb48ade092c4bb36d",
      "6d05c08c79694bed8345d28bdfe19032",
      "54724f48c14b43f5ba4459459670334a",
      "f5d383934bbb4f758336f08b567f0824",
      "64d85e89ed7d46a981d2a00c47bdf8b2",
      "35a0f7cdae6f4a61acb0dffe5e698130",
      "850abb487dce4b1aa09f8094bc447a9b",
      "1aeb4f6ec0234ed3a6be1ac75244ae8e",
      "93156ea1f4a14de392c28e0b489b4290",
      "a29338f36ee34153a880c3f5fb985616",
      "50de9fd7ffd544559ebbd078ef12345f",
      "4036016f7aad43449ad70cd76f40c5eb",
      "9bfee60d3ab5416e983c42ae6ecdd0e0",
      "ff17fc6260e547268b518231cae50451",
      "80e75187ab95457795532aa6c7b00d76",
      "261b9119ab994265aac43d6b80ffc90d",
      "92ef80ab57e44151a0e9419639cb9a34",
      "bb0813f49acd457cadc27d2384e9274f",
      "bd41907a546e40059723a07c53f39339",
      "d30f87bdd81242e989c97a14ec2f98f3",
      "a98b46f3843b4724b6fad82fac16e219",
      "c1f7130a16d844ed91aa1c97c36f18a1",
      "4715e099e4454e9f9c54e0572d0508d4",
      "e9d5215839c44dbcb782855769ef3d0b",
      "c96ff8419847405889ef3b88a4684739",
      "fe77dd4438fb4de389bbf33ec23bf8b4",
      "3e34500a435a457c9cb321b4b258f76e",
      "30cfc3189acb4db7954e0b6efc9a5645",
      "530d30bd4d524415bec77c5d1725a4ac",
      "3804bdf7cf5f4d9c87ce42c47fadfe1a",
      "27c3bc5002ba49ef81d6c20b40d5719f"
     ]
    },
    "executionInfo": {
     "elapsed": 4958,
     "status": "ok",
     "timestamp": 1719391415052,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "UlbPVO_aac33",
    "outputId": "a2c446d6-e410-4d17-eb98-96b21264e0e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12859/12859 [00:00<00:00, 29714.81 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12859/12859 [00:00<00:00, 24422.82 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5922/5922 [00:00<00:00, 7520.06 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 5922\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''This code loads and formats a dataset for DPO (Direct Preference Optimization) training, using a TinyLlama-compatible prompt style.\n",
    "Load a preference dataset (with ranked outputs: chosen vs rejected).\n",
    "Format the data into prompt/response pairs that follow TinyLlama‚Äôs chat-style template (e.g., <|user|>, <|assistant|>, <|system|>).\n",
    "Filter out low-quality or irrelevant data.\n",
    "Prepare it for DPO fine-tuning, where the model is trained to prefer \"better\" responses.'''\n",
    "\n",
    "'''Takes a single example from the dataset. Formats it using TinyLlama's chat structure, with special tags:\n",
    "<|system|>: Optional system-level instruction (e.g., \"You are a helpful assistant.\")\n",
    "<|user|>: User's question or command.\n",
    "<|assistant|>: Marks where the model's answer should begin.\n",
    "Adds </s> (end-of-sequence token) at the end of each section.'''\n",
    "from datasets import load_dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\"\n",
    "\n",
    "    # Format answers\n",
    "    system = \"<|system|>\\n\" + example['system'] + \"</s>\\n\"\n",
    "    prompt = \"<|user|>\\n\" + example['input'] + \"</s>\\n<|assistant|>\\n\"\n",
    "    chosen = example['chosen'] + \"</s>\\n\"\n",
    "    rejected = example['rejected'] + \"</s>\\n\"\n",
    "\n",
    "    return {\n",
    "        \"prompt\": system + prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "# Apply formatting to the dataset and select relatively short answers\n",
    "'''Loads the \"train\" split of the argilla/distilabel-intel-orca-dpo-pairs dataset.\n",
    "This dataset contains preference pairs: each sample includes a prompt and two completions ‚Äî one labeled better (chosen) and one worse (rejected).'''\n",
    "dpo_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
    "\n",
    "'''Keeps only high-quality training examples:\n",
    "status != \"tie\": Only include samples where a clear winner was chosen.\n",
    "chosen_score >= 8: The chosen response must be rated 8 or higher (likely on a 1‚Äì10 scale).\n",
    "not r[\"in_gsm8k_train\"]: Excludes examples that overlap with GSM8K‚Äôs training data (to avoid data leakage).'''\n",
    "dpo_dataset = dpo_dataset.filter(\n",
    "    lambda r:\n",
    "        r[\"status\"] != \"tie\" and\n",
    "        r[\"chosen_score\"] >= 8 and\n",
    "        not r[\"in_gsm8k_train\"]\n",
    ")\n",
    "'''Applies the format_prompt() function to every row in the dataset.\n",
    "Removes all original columns and replaces them with:\n",
    "\"prompt\" (system + user prompt)\n",
    "\"chosen\" (preferred response)\n",
    "\"rejected\" (non-preferred response'''\n",
    "dpo_dataset = dpo_dataset.map(format_prompt, remove_columns=dpo_dataset.column_names)\n",
    "\n",
    "'''After these steps, you have a cleaned, formatted dataset ready for DPO training, where each sample has:\n",
    "A prompt (in TinyLlama format)\n",
    "A high-quality (chosen) response\n",
    "A lower-quality (rejected) response\n",
    "This is the standard setup for training models to prefer better responses using ranking-based methods like DPO.'''\n",
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AkCJ4CO5sQG6"
   },
   "source": [
    "## Models - Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5934,
     "status": "ok",
     "timestamp": 1719391420979,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "7YMmilm7c1-P",
    "outputId": "fbf5e75b-cf63-4ac6-b1b5-514cddceb842"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''Loads a QLoRA-fine-tuned model with 4-bit quantization. Merges the LoRA adapter weights into the base model\n",
    "Loads and configures the corresponding tokenizer'''\n",
    "'''AutoPeftModelForCausalLM: A PEFT-aware model loader that can load base models along with LoRA/QLoRA adapters.\n",
    "BitsAndBytesConfig: Used to configure 4-bit quantization for loading large models efficiently (QLoRA).\n",
    "AutoTokenizer: Loads the tokenizer associated with the model.'''\n",
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import BitsAndBytesConfig, AutoTokenizer\n",
    "\n",
    "# 4-bit quantization configuration - Q in QLoRA\n",
    "'''| Parameter                          | Description                                                            |\n",
    "| ---------------------------------- | ---------------------------------------------------------------------- |\n",
    "| `load_in_4bit=True`                | Load weights in 4-bit precision                                        |\n",
    "| `bnb_4bit_quant_type=\"nf4\"`        | Use **NF4** (Normalized Float 4) ‚Äî better performance vs. regular int4 |\n",
    "| `bnb_4bit_compute_dtype=\"float16\"` | Computations (e.g., matmuls) are done in 16-bit float                  |\n",
    "| `bnb_4bit_use_double_quant=True`   | Uses nested quantization to reduce memory even further                 |\n",
    " '''\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "'''Loads the fine-tuned TinyLlama model with LoRA adapters from the directory \"TinyLlama-1.1B-qlora\".\n",
    "Applies the bnb_config for 4-bit quantized loading. Uses device_map=\"auto\" to automatically place layers on available GPU(s)/CPU.\n",
    "low_cpu_mem_usage=True: Helps avoid memory overload on systems with limited RAM. ‚ö†Ô∏è At this point, the model is still composed of the base + LoRA adapter weights.'''\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "'''Combines the base model and the fine-tuned LoRA weights into one model. Unloads any adapter-specific layers.\n",
    "The result (merged_model) is now a standard model, with LoRA changes baked in. You no longer need PEFT or LoRA configs to use it for inference or export.'''\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "'''Loads the tokenizer that matches the original base model. trust_remote_code=False means you only trust standard Hugging Face tokenizers (for safety).\n",
    "Sets: pad_token = \"<PAD>\": Required for left-padding input sequences. padding_side = \"left\": Important for generation tasks (used with attention masks when padding sequences).'''\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iidCbaXMs1O4"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "m6IfkvLkylVD"
   },
   "outputs": [],
   "source": [
    "'''You‚Äôre importing from the PEFT (Parameter-Efficient Fine-Tuning) LoraConfig: Configuration for how LoRA adapters are applied.\n",
    "prepare_model_for_kbit_training: Prepares a quantized model (e.g. 4-bit or 8-bit) to be fine-tuned.\n",
    "get_peft_model: Injects LoRA layers into the model based on the config.'''\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Prepare LoRA Configuration\n",
    "'''LoRA works by freezing the base model and only training small rank-decomposed matrices inserted into key transformer layers.\n",
    "r=64: Low-rank dimension (fewer parameters).\n",
    "target_modules: These are specific linear projection layers in transformer blocks ‚Äî typically part of the attention and MLP components.\n",
    "This setup is optimized for models like LLaMA, GPT, etc.'''\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Scaling\n",
    "    lora_dropout=0.1,  # Dropout for LoRA Layers\n",
    "    r=64,  # Rank\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=  # Layers to target\n",
    "     ['k_proj', 'gate_proj', 'v_proj', 'up_proj', 'q_proj', 'o_proj', 'down_proj']\n",
    ")\n",
    "\n",
    "# prepare model for training\n",
    "'''Prepares a 4-bit (or 8-bit) quantized model for training by: Casting certain layers (like LayerNorm) to float32 for numerical stability.\n",
    "Ensuring that only LoRA-injected layers will be trainable. Freezing all other base model weights.\n",
    "üîí This is essential when using quantized models like QLoRA ‚Äî because you cannot train 4-bit weights directly.'''\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "'''Adds LoRA adapters into the model based on the peft_config.\n",
    "After this, model: Is ready for training using standard techniques (Trainer, SFTTrainer, etc.)\n",
    "Will only update the LoRA adapter weights during training (keeping the base model frozen)\n",
    "Has a small memory footprint, even on consumer GPUs'''\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lk-cEEd8nk27"
   },
   "outputs": [],
   "source": [
    "'''This code sets up training hyperparameters for Direct Preference Optimization (DPO) using the trl library (Transformers Reinforcement Learning by Hugging Face), \n",
    "specifically via the DPOConfig class. DPOConfig is a configuration class for training with Direct Preference Optimization (DPO), a method used to align LLMs with human preferences \n",
    "by teaching the model to prefer better responses (without needing a reward model like in RLHF).'''\n",
    "from trl import DPOConfig\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Training arguments\n",
    "'''| Argument                        | Description                                                                                        |\n",
    "| ------------------------------- | -------------------------------------------------------------------------------------------------- |\n",
    "| `output_dir=\"./results\"`        | Where the model checkpoints and logs will be saved.                                                |\n",
    "| `per_device_train_batch_size=2` | Number of examples per GPU per step.                                                               |\n",
    "| `gradient_accumulation_steps=4` | Accumulates gradients over 4 steps before backpropagation ‚Äî simulates a batch size of `2 √ó 4 = 8`. |\n",
    "| `optim=\"paged_adamw_32bit\"`     | Uses a memory-efficient version of the AdamW optimizer, suitable for quantized models.             |\n",
    "| `learning_rate=1e-5`            | The base learning rate for training.                                                               |\n",
    "| `lr_scheduler_type=\"cosine\"`    | Applies a **cosine decay** to the learning rate over time.                                         |\n",
    "| `max_steps=200`                 | Train for 200 update steps (not epochs).                                                           |\n",
    "| `logging_steps=10`              | Log metrics every 10 steps.                                                                        |\n",
    "| `fp16=True`                     | Use 16-bit floating-point precision (faster, less memory).                                         |\n",
    "| `gradient_checkpointing=True`   | Saves memory by recomputing some activations during backpropagation.                               |\n",
    "| `warmup_ratio=0.1`              | 10% of training steps are used to gradually ramp up the learning rate from zero.                   |\n",
    " '''\n",
    "training_arguments = DPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=200,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_ratio=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "c98fc033e551443b94dc8dae31d590bf",
      "b20056d36ac942cc9f8c3a9efc020f36",
      "3f8a810d4e2549c9a4ba2f3f2ee017e8",
      "47768d322ac94e1494d7f4a2f01440b7",
      "656d76b25562479ba3b24f22800a675b",
      "eab02ca086f44759a9b1b00d8f1a1245",
      "d960c3e6a7a04fb8b525dec294da6815",
      "983f9c1d7e47494383099916eed69c0d",
      "0236bebb8dce4b5dade5728304ffb964",
      "9e1a61b6c5f8482eadd024280da208f3",
      "4380e0fb571e41c9a58b09a06a20b853"
     ]
    },
    "executionInfo": {
     "elapsed": 805129,
     "status": "ok",
     "timestamp": 1719392226734,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "Pp3tUXhWm0pE",
    "outputId": "29378dc8-bb8a-435b-8330-e45aa26548c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_prompt_length, max_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/trl/trainer/dpo_trainer.py:358: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/trl/trainer/dpo_trainer.py:371: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/trl/trainer/dpo_trainer.py:411: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5922/5922 [00:18<00:00, 325.53 examples/s]\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 2:48:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.692200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.646000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.594900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.617100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.531600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.559600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.638800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.496400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.584900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.629700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.577800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.591300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.607000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.627400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.668700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.556300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''This code sets up and runs Direct Preference Optimization (DPO) fine-tuning on a QLoRA-prepared TinyLlama model using Hugging Face's trl library ‚Äî specifically, the DPOTrainer.'''\n",
    "from trl import DPOTrainer\n",
    "\n",
    "# Create DPO trainer\n",
    "'''| Argument                    | Description                                                                                                                                              |\n",
    "| --------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `model`                     | The TinyLlama model prepared with **QLoRA adapters**                                                                                                     |\n",
    "| `args=training_arguments`   | The `DPOConfig` settings defined earlier (batch size, learning rate, etc.)                                                                               |\n",
    "| `train_dataset=dpo_dataset` | A dataset of preference pairs (`prompt`, `chosen`, `rejected`)                                                                                           |\n",
    "| `tokenizer=tokenizer`       | Tokenizer that matches the model (set up earlier with TinyLlama)                                                                                         |\n",
    "| `peft_config=peft_config`   | The LoRA configuration (defines where and how to train adapters)                                                                                         |\n",
    "| `beta=0.1`                  | A hyperparameter controlling how strongly the model is optimized to prefer chosen over rejected responses (higher = more aggressive preference training) |\n",
    "| `max_prompt_length=512`     | Maximum length for the **prompt** part (input before the model response starts)                                                                          |\n",
    "| `max_length=512`            | Maximum total input length (prompt + response) during training                                                                                           |\n",
    " '''\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=dpo_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    beta=0.1,\n",
    "    max_prompt_length=512,\n",
    "    max_length=512,\n",
    ")\n",
    "\n",
    "# Fine-tune model with DPO\n",
    "'''Starts the fine-tuning process using DPO, where: For each example, the model is shown a prompt, a chosen response, and a rejected response.\n",
    "It learns to assign higher probability to the chosen response (based on likelihood), using a contrastive loss informed by the beta parameter.\n",
    "‚úÖ This is more sample-efficient and stable than reward-model-based RLHF methods.'''\n",
    "dpo_trainer.train()\n",
    "\n",
    "# Save adapter\n",
    "'''Saves only the LoRA adapter weights (not the full base model) to the directory \"TinyLlama-1.1B-dpo-qlora\".\n",
    "You can later: Reload it with AutoPeftModelForCausalLM.from_pretrained(...)\n",
    "Merge it into the base model for deployment (model.merge_and_unload())'''\n",
    "dpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QFE4OKFvyLMe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/golongson/miniconda3/envs/llm/lib/python3.11/site-packages/accelerate/utils/modeling.py:1384: UserWarning: Current model requires 2816 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''This code merges multiple LoRA adapters into a single base model ‚Äî a common final step when you've done sequential fine-tuning, such as:\n",
    "Supervised Fine-Tuning (SFT) using LoRA ‚Üí saved as \"TinyLlama-1.1B-qlora\"\n",
    "Direct Preference Optimization (DPO) LoRA fine-tuning ‚Üí saved as \"TinyLlama-1.1B-dpo-qlora\"'''\n",
    "from peft import PeftModel\n",
    "\n",
    "# Merge LoRA and base model\n",
    "'''Loads the SFT model with QLoRA adapters from \"TinyLlama-1.1B-qlora\".\n",
    "Uses AutoPeftModelForCausalLM which automatically loads both the base model and LoRA weights.\n",
    "Then, merge_and_unload() merges the SFT LoRA weights into the base model, returning a standard model (sft_model) without LoRA layers.\n",
    "‚úÖ Now sft_model is a standalone, fully fine-tuned base model after the SFT stage.'''\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\",\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sft_model = model.merge_and_unload()\n",
    "\n",
    "# Merge DPO LoRA and SFT model\n",
    "'''PeftModel.from_pretrained(...) takes:\n",
    "sft_model (already merged with SFT LoRA)\n",
    "TinyLlama-1.1B-dpo-qlora: the DPO-specific LoRA adapter weights\n",
    "This attaches the DPO LoRA adapters on top of the sft_model.\n",
    "merge_and_unload() then merges those DPO LoRA weights into the sft_model, giving you the final fully fine-tuned model that includes:\n",
    "Base model weights\n",
    "SFT LoRA modifications\n",
    "DPO LoRA modifications\n",
    "‚úÖ dpo_model is now the final merged model, fully trained through both SFT and DPO, and no longer needs LoRA adapters to function.'''\n",
    "dpo_model = PeftModel.from_pretrained(\n",
    "    sft_model,\n",
    "    \"TinyLlama-1.1B-dpo-qlora\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "dpo_model = dpo_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6777,
     "status": "ok",
     "timestamp": 1719392237608,
     "user": {
      "displayName": "Maarten Grootendorst",
      "userId": "11015108362723620659"
     },
     "user_tz": -120
    },
    "id": "zAkwJcHYmxr4",
    "outputId": "631aed7c-1e64-4e2c-db73-3e36ddee4e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Tell me something about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) that can generate human-like language. They are trained on large amounts of data, including text, audio, and video, and are capable of generating complex and nuanced language.\n",
      "\n",
      "LLMs are used in a variety of applications, including natural language processing (NLP), machine translation, and chatbots. They can be used to generate text, speech, or images, and can be trained to understand different languages and dialects.\n",
      "\n",
      "One of the most significant applications of LLMs is in the field of natural language generation (NLG). LLMs can be used to generate text in a variety of languages, including English, French, and German. They can also be used to generate speech, such as in chatbots or voice assistants.\n",
      "\n",
      "LLMs have also been used in the field of machine translation (MT). LLMs can be trained to translate between different languages, and can be used to generate high-quality translations in a variety of fields, including business, medicine, and education.\n",
      "\n",
      "In addition to their use in NLP and MT, LLMs have also been used in other areas, such as speech recognition and natural language understanding (NLU). LLMs can be used to recognize speech and understand what is being said, and can be used to generate responses or recommendations based on the context of the conversation.\n",
      "\n",
      "Overall, LLMs are a powerful tool that has the potential to revolutionize the way we communicate and interact with each other. They are being used in a wide range of applications, and their capabilities are only expected to grow in the future.\n"
     ]
    }
   ],
   "source": [
    "'''This code runs inference using a fine-tuned language model ‚Äî specifically, one that was instruction-tuned via SFT + DPO, and uses a prompt formatted in the style expected by \n",
    "TinyLlama (with chat-style tokens like <|user|> and <|assistant|>).'''\n",
    "from transformers import pipeline\n",
    "\n",
    "# Use our predefined prompt template\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Run our instruction-tuned model\n",
    "'''Sets up a text generation pipeline using:\n",
    "dpo_model: your final model (merged with both SFT and DPO LoRA weights)\n",
    "tokenizer: the tokenizer that matches TinyLlama\n",
    "‚úÖ At this point, you're ready to generate text using your aligned model.'''\n",
    "pipe = pipeline(task=\"text-generation\", model=dpo_model, tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPaxPKtmt1gCzzuqYr6g2+g",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
