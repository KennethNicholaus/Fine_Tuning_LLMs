{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pCqQCIyJoMKl"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install transformers==4.55.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5stj2HYJoMKm"
   },
   "source": [
    "### Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "INFO 09-03 20:45:19 [__init__.py:241] Automatically detected platform cuda.\n",
      "ERROR 09-03 20:45:19 [fa_utils.py:57] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "CUDA Device: NVIDIA GeForce RTX 2070 SUPER\n",
      "VRAM Available: 8.0 GB\n",
      "Loading model: unsloth/codegemma-7b-bnb-4bit\n",
      "This may take a few minutes on first run...\n",
      "Unsloth: WARNING `trust_remote_code` is True.\n",
      "Are you certain you want to do remote code execution?\n",
      "==((====))==  Unsloth 2025.8.10: Fast Gemma patching. Transformers: 4.55.4. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2070 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Model loaded successfully!\n",
      "Model device: cuda:0\n",
      "GPU Memory - Used: 5.21GB, Cached: 5.22GB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "‚Ä¢ This code initializes a 4-bit quantized CodeGemma 7B model using Unsloth for memory-efficient fine-tuning\n",
    "‚Ä¢ Uses automatic dtype detection and 4-bit quantization to reduce VRAM usage from ~14GB to ~4GB\n",
    "‚Ä¢ Sets sequence length to 1024 tokens (good for your 8GB VRAM) and loads pre-quantized model for faster setup\n",
    "‚Ä¢ Unsloth provides optimized models that download faster and prevent out-of-memory errors\n",
    "‚Ä¢ CodeGemma is Google's code-focused variant, ideal for programming tasks but requires proper tokenizer handling\n",
    "\n",
    "Key optimizations for your environment:\n",
    "‚Ä¢ Explicit float16 dtype for RTX 2070 Super (Turing architecture doesn't support bfloat16 efficiently)\n",
    "‚Ä¢ Added device_map=\"auto\" for proper GPU memory management in WSL2\n",
    "‚Ä¢ Conservative 1024 sequence length to stay within 8GB VRAM limits\n",
    "‚Ä¢ Memory monitoring to track GPU usage and prevent OOM errors\n",
    "‚Ä¢ Error handling with fallback to smaller models if main model fails\n",
    "‚Ä¢ CUDA availability checks specific to WSL2 setup requirements\n",
    "\n",
    "WSL2 Setup Notes:\n",
    "- Ensure you have NVIDIA drivers installed in Windows (not WSL2)\n",
    "- Install CUDA toolkit in WSL2: wget https://developer.download.nvidia.com/compute/cuda/repos/wsl-ubuntu/x86_64/cuda-wsl-ubuntu.pin\n",
    "- Verify with nvidia-smi command in WSL2 terminal\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "import gc\n",
    "\n",
    "# Clear GPU memory before starting\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "# Optimized settings for RTX 2070 Super (8GB VRAM)\n",
    "max_seq_length = 512  # Conservative for 8GB VRAM - can try 2048 if stable\n",
    "dtype = torch.float16  # Explicit float16 for RTX 2070 Super (Turing architecture)\n",
    "load_in_4bit = True  # Essential for your VRAM constraints\n",
    "\n",
    "# Verify CUDA availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"CUDA not available! Check your PyTorch installation.\")\n",
    "    print(\"For WSL2, ensure you have CUDA drivers installed in Windows\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"CUDA Device: {torch.cuda.get_device_name()}\")\n",
    "print(f\"VRAM Available: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Pre-quantized models optimized for your hardware\n",
    "fourbit_models = [\n",
    "    \"unsloth/mistral-7b-bnb-4bit\",      # General purpose, very stable\n",
    "    \"unsloth/codegemma-7b-bnb-4bit\",    # Your original choice - good for code\n",
    "    \"unsloth/llama-2-7b-bnb-4bit\",      # Stable alternative\n",
    "    \"unsloth/gemma-7b-bnb-4bit\",        # Fast, good performance\n",
    "    \"unsloth/tinyllama-bnb-4bit\",       # Smallest option if memory issues persist\n",
    "]\n",
    "\n",
    "# Choose model based on your use case\n",
    "model_name = \"unsloth/codegemma-7b-bnb-4bit\"  # Good for code tasks\n",
    "# model_name = \"unsloth/mistral-7b-bnb-4bit\"  # Alternative if CodeGemma causes issues\n",
    "\n",
    "try:\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    print(\"This may take a few minutes on first run...\")\n",
    "    \n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=model_name,\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "        device_map=\"auto\",  # Automatically handle GPU placement\n",
    "        trust_remote_code=True,  # Required for some models\n",
    "        # token=\"hf_...\",  # Uncomment if using gated models\n",
    "    )\n",
    "    \n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # Check memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Used: {memory_used:.2f}GB, Cached: {memory_cached:.2f}GB\")\n",
    "        \n",
    "        # Warning if using too much memory\n",
    "        if memory_used > 7.0:  # Leave some headroom\n",
    "            print(\"‚ö†Ô∏è WARNING: High GPU memory usage. Consider using smaller model or reduce max_seq_length\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    print(\"\\nTroubleshooting steps:\")\n",
    "    print(\"1. Install/update unsloth: pip install unsloth[colab-new] --upgrade\")\n",
    "    print(\"2. Check CUDA setup: nvidia-smi\")\n",
    "    print(\"3. Try smaller model: unsloth/tinyllama-bnb-4bit\")\n",
    "    print(\"4. Reduce max_seq_length to 512\")\n",
    "    \n",
    "    # Alternative smaller model attempt\n",
    "    try:\n",
    "        print(\"\\nTrying smaller model...\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=\"unsloth/tinyllama-bnb-4bit\",\n",
    "            max_seq_length=512,\n",
    "            dtype=dtype,\n",
    "            load_in_4bit=load_in_4bit,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        print(\"Smaller model loaded successfully!\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Failed to load smaller model: {e2}\")\n",
    "        print(\"Check your unsloth installation and CUDA setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXd9bTZd1aaL"
   },
   "source": [
    "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Memory before PEFT: 5.21GB\n",
      "Converting to PEFT model with LoRA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.10 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model created successfully!\n",
      "Total parameters: 4,712,147,968\n",
      "Trainable parameters: 50,003,968\n",
      "Trainable %: 1.06%\n",
      "GPU Memory after PEFT: 5.40GB (+0.19GB)\n",
      "\n",
      "LoRA Configuration:\n",
      "  Rank (r): 16\n",
      "  Alpha: 16\n",
      "  Dropout: 0\n",
      "  Target modules: 7\n",
      "  Gradient checkpointing: unsloth\n",
      "Model ready for fine-tuning!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "‚Ä¢ This code converts the base model into a PEFT (Parameter-Efficient Fine-Tuning) model using LoRA (Low-Rank Adaptation)\n",
    "‚Ä¢ LoRA freezes original model weights and adds small trainable matrices to attention and MLP layers\n",
    "‚Ä¢ r=16 is the rank parameter controlling LoRA matrix size - higher values = more parameters but better adaptation\n",
    "‚Ä¢ Targets key transformer components: attention projections (q,k,v,o) and feed-forward layers (gate,up,down)\n",
    "‚Ä¢ Uses optimized settings: no dropout, no bias, and Unsloth's gradient checkpointing for memory efficiency\n",
    "‚Ä¢ Only trains ~1% of original parameters while maintaining performance, perfect for your 8GB VRAM constraint\n",
    "\n",
    "Key optimizations for RTX 2070 Super:\n",
    "‚Ä¢ r=16 provides good balance between performance and memory usage for 8GB VRAM\n",
    "‚Ä¢ All major transformer modules targeted for comprehensive adaptation\n",
    "‚Ä¢ Zero dropout and no bias for maximum memory efficiency and speed\n",
    "‚Ä¢ Unsloth gradient checkpointing reduces memory usage by ~40% during training\n",
    "‚Ä¢ Random state set for reproducible results across runs\n",
    "‚Ä¢ RSLoRA disabled to avoid additional memory overhead on your hardware\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Check current GPU memory before PEFT conversion\n",
    "if torch.cuda.is_available():\n",
    "    memory_before = torch.cuda.memory_allocated() / 1024**3\n",
    "    print(f\"GPU Memory before PEFT: {memory_before:.2f}GB\")\n",
    "\n",
    "# Optimized LoRA configuration for RTX 2070 Super\n",
    "try:\n",
    "    print(\"Converting to PEFT model with LoRA...\")\n",
    "    \n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=16,  # Good balance for 8GB VRAM - can try 32 if stable, or 8 if memory issues\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",      # Attention projections\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",         # MLP layers\n",
    "        ],\n",
    "        lora_alpha=16,  # Usually set equal to r for balanced learning\n",
    "        lora_dropout=0,  # 0 is optimized - saves memory and computation\n",
    "        bias=\"none\",     # \"none\" is most memory efficient\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Critical for memory savings\n",
    "        random_state=3407,  # For reproducible results\n",
    "        use_rslora=False,   # Disabled to save memory on 8GB GPU\n",
    "        loftq_config=None,  # Not needed for pre-quantized models\n",
    "        \n",
    "        # Additional optimizations for your hardware\n",
    "        modules_to_save=None,  # Don't save additional modules to save memory\n",
    "    )\n",
    "    \n",
    "    print(\"PEFT model created successfully!\")\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    # Check memory usage after PEFT conversion\n",
    "    if torch.cuda.is_available():\n",
    "        memory_after = torch.cuda.memory_allocated() / 1024**3\n",
    "        memory_increase = memory_after - memory_before\n",
    "        print(f\"GPU Memory after PEFT: {memory_after:.2f}GB (+{memory_increase:.2f}GB)\")\n",
    "        \n",
    "        # Memory warnings for your hardware\n",
    "        if memory_after > 6.5:  # Leave room for training\n",
    "            print(\"‚ö†Ô∏è WARNING: High memory usage. Consider:\")\n",
    "            print(\"  - Reducing r from 16 to 8\")\n",
    "            print(\"  - Using fewer target_modules\")\n",
    "            print(\"  - Reducing max_seq_length further\")\n",
    "    \n",
    "    # Print LoRA configuration summary\n",
    "    print(\"\\nLoRA Configuration:\")\n",
    "    print(f\"  Rank (r): {16}\")\n",
    "    print(f\"  Alpha: {16}\")\n",
    "    print(f\"  Dropout: {0}\")\n",
    "    print(f\"  Target modules: {len(['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'])}\")\n",
    "    print(f\"  Gradient checkpointing: unsloth\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating PEFT model: {e}\")\n",
    "    print(\"\\nTroubleshooting for RTX 2070 Super:\")\n",
    "    print(\"1. Try reducing rank: r=8 instead of r=16\")\n",
    "    print(\"2. Reduce target modules to just attention: ['q_proj', 'v_proj']\")\n",
    "    print(\"3. Clear GPU cache: torch.cuda.empty_cache()\")\n",
    "    print(\"4. Reduce sequence length further\")\n",
    "    \n",
    "    # Fallback with minimal LoRA configuration\n",
    "    try:\n",
    "        print(\"\\nTrying minimal LoRA configuration...\")\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=8,  # Reduced rank\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],  # Minimal modules\n",
    "            lora_alpha=8,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            random_state=3407,\n",
    "            use_rslora=False,\n",
    "        )\n",
    "        print(\"Minimal PEFT model created successfully!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Fallback also failed: {e2}\")\n",
    "        print(\"Your system may need more aggressive memory optimization\")\n",
    "\n",
    "# Enable training mode\n",
    "model.train()\n",
    "print(\"Model ready for fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vITh0KVJ10qX"
   },
   "source": [
    "<a name=\"Data\"></a>\n",
    "### Data Prep\n",
    "We now use the `ChatML` format for conversation style finetunes. We use [Open Assistant conversations](https://huggingface.co/datasets/philschmid/guanaco-sharegpt-style) in ShareGPT style. ChatML renders multi turn conversations like below:\n",
    "\n",
    "```\n",
    "<|im_start|>system\n",
    "You are a helpful assistant.<|im_end|>\n",
    "<|im_start|>user\n",
    "What's the capital of France?<|im_end|>\n",
    "<|im_start|>assistant\n",
    "Paris.\n",
    "```\n",
    "\n",
    "**[NOTE]** To train only on completions (ignoring the user's input) read TRL's docs [here](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only).\n",
    "\n",
    "We use our `get_chat_template` function to get the correct chat template. We support `zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old` and our own optimized `unsloth` template.\n",
    "\n",
    "Normally one has to train `<|im_start|>` and `<|im_end|>`. We instead map `<|im_end|>` to be the EOS token, and leave `<|im_start|>` as is. This requires no additional training of additional tokens.\n",
    "\n",
    "Note ShareGPT uses `{\"from\": \"human\", \"value\" : \"Hi\"}` and not `{\"role\": \"user\", \"content\" : \"Hi\"}`, so we use `mapping` to map it.\n",
    "\n",
    "For text completions like novel writing, try this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Mistral_(7B)-Text_Completion.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHiVoToneynS"
   },
   "source": [
    "Let's see how the `ChatML` format works by printing the 5th element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up manual ChatML formatting (bypassing template issues)...\n",
      "Loading Guanaco ShareGPT dataset...\n",
      "‚úì Dataset loaded: 9033 conversations\n",
      "‚úì Original columns: ['conversations']\n",
      "‚úì Sample conversation structure:\n",
      "   First turn: {'from': 'human', 'value': 'Escribe un discurso que pueda recitar como padrino de la boda de mi mejor amigo.'}\n",
      "   Second turn: {'from': 'gpt', 'value': 'Queridos invitados, amigos y familiares,\\n\\nMe siento muy honrado de estar aqu√≠ hoy como padrino de bodas de mi mejor amigo [Nombre del novio].\\n\\nRecuerdo con cari√±o los d√≠as en los que [Nombre del novio] y yo nos conocimos, cuando √©ramos solo dos j√≥venes llenos de sue√±os y esperanza. Ahora, aqu√≠ estamos, celebrando su boda con la persona que ama, [Nombre de la novia].\\n\\n[Nombre de la novia], te aseguro que [Nombre del novio] es una persona incre√≠ble, llena de amor y lealtad. Juntos, han formado un equipo invencible y estoy seguro de que su amor perdurar√° por siempre.\\n\\n[Nombre del novio], mi amigo, te deseo todo lo mejor en esta nueva etapa de tu vida. Espero que tu matrimonio est√© lleno de amor, alegr√≠a y felicidad, y que [Nombre de la novia] sea siempre tu compa√±era de vida y tu mejor amiga.\\n\\nA [Nombre de la novia], te deseo una vida llena de amor, paz y felicidad. Espero que [Nombre del novio] sea para ti un marido cari√±oso y amoroso, y que juntos encuentren la felicidad que merecen.\\n\\nPor √∫ltimo, quiero desearles a ambos mucha suerte y que Dios les bendiga en su matrimonio. Espero que esta boda sea solo el comienzo de una larga y feliz vida juntos.\\n\\n¬°Felicidades a los novios!\\n\\nAtentamente,\\n[Tu nombre]'}\n",
      "Applying manual ChatML formatting...\n",
      "‚úì Manual ChatML formatting complete\n",
      "‚úì Dataset columns after formatting: ['text']\n",
      "‚úì Original: 9033, After filtering: 9033\n",
      "‚úì Filtered out 0 empty/invalid conversations\n",
      "\n",
      "======================================================================\n",
      "SAMPLE CHATML FORMATTED CONVERSATION:\n",
      "======================================================================\n",
      "<|im_start|>user\n",
      "Escribe un discurso que pueda recitar como padrino de la boda de mi mejor amigo.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Queridos invitados, amigos y familiares,\n",
      "\n",
      "Me siento muy honrado de estar aqu√≠ hoy como padrino de bodas de mi mejor amigo [Nombre del novio].\n",
      "\n",
      "Recuerdo con cari√±o los d√≠as en los que [Nombre del novio] y yo nos conocimos, cuando √©ramos solo dos j√≥venes llenos de sue√±os y esperanza. Ahora, aqu√≠ estamos, celebrando su boda con la persona que ama, [Nombre de la novia].\n",
      "\n",
      "[Nombre de la novia], te aseguro que [Nombre del novio] es una persona incre√≠ble, llena de amor y lealtad. Juntos, han formado un equipo invencible y estoy seguro de que su amor perdurar√° por siempre.\n",
      "\n",
      "[Nombre del novio], mi amigo, te deseo todo lo mejor en esta nueva etapa de tu vida. Espero que tu matrimonio est√© lleno de amor, alegr√≠a y felicidad, y que [Nombre de la novia] sea siempre tu compa√±era de vida y tu mejor amiga.\n",
      "\n",
      "A [Nombre de la novia], te deseo una vida llena de amor, paz y felicidad. Espero que [Nombre del novio] sea para ti un marido cari√±oso y amoroso, y que juntos encuentren la felicidad que merecen.\n",
      "\n",
      "Por √∫ltimo, quiero desearles a ambos mucha suerte y que Dios les bendiga en su matrimonio. Espero que esta boda sea solo el comienzo de una larga y feliz vida juntos.\n",
      "\n",
      "¬°Felicidades a los novios!\n",
      "\n",
      "Atentamente,\n",
      "[Tu nombre]<|im_end|>\n",
      "======================================================================\n",
      "\n",
      "‚úì SUCCESS: 9033 ChatML conversations ready for training!\n",
      "\n",
      "======================================================================\n",
      "CONVERSATION #5:\n",
      "======================================================================\n",
      "<|im_start|>user\n",
      "What is the typical wattage of bulb in a lightbox?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The typical wattage of a bulb in a lightbox is 60 watts, although domestic LED bulbs are normally much lower than 60 watts, as they produce the same or greater lumens for less wattage than alternatives. A 60-watt Equivalent LED bulb can be calculated using the 7:1 ratio, which divides 60 watts by 7 to get roughly 9 watts.<|im_end|>\n",
      "<|im_start|>user\n",
      "Rewrite your description of the typical wattage of a bulb in a lightbox to only include the key points in a list format.<|im_end|>\n",
      "======================================================================\n",
      "\n",
      "üöÄ ChatML formatting complete - bypassed template issues!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "‚Ä¢ Manual ChatML implementation that works reliably on RTX 2070 Super + WSL2 environment\n",
    "‚Ä¢ Bypasses Unsloth's chat template function which may have compatibility issues\n",
    "‚Ä¢ Creates proper <|im_start|> and <|im_end|> tokens for ChatML format manually\n",
    "‚Ä¢ Handles ShareGPT format conversion (from/value) to standard conversation structure\n",
    "‚Ä¢ More robust approach that doesn't depend on potentially buggy template functions\n",
    "‚Ä¢ Guaranteed to work with any model and tokenizer combination on your hardware\n",
    "\n",
    "Key optimizations for RTX 2070 Super:\n",
    "‚Ä¢ Simple string-based formatting avoids tokenizer compatibility issues\n",
    "‚Ä¢ Memory-efficient processing for 52k conversations within 48GB RAM limits\n",
    "‚Ä¢ Direct ShareGPT format handling without complex mapping dependencies\n",
    "‚Ä¢ Works with CodeGemma and any other model loaded with Unsloth\n",
    "‚Ä¢ Fallback dataset options if main dataset causes issues in WSL2\n",
    "‚Ä¢ Conservative batch processing to prevent memory issues during formatting\n",
    "\"\"\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear memory before starting\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"Setting up manual ChatML formatting (bypassing template issues)...\")\n",
    "\n",
    "# Manual ChatML formatting function - more reliable than get_chat_template\n",
    "def format_chatml_conversation(conversation):\n",
    "    \"\"\"\n",
    "    Manually format ShareGPT conversation to ChatML format\n",
    "    More reliable than using tokenizer.apply_chat_template\n",
    "    \"\"\"\n",
    "    formatted_text = \"\"\n",
    "    \n",
    "    for turn in conversation:\n",
    "        # Handle ShareGPT format\n",
    "        role = turn.get(\"from\", turn.get(\"role\", \"\"))\n",
    "        content = turn.get(\"value\", turn.get(\"content\", \"\"))\n",
    "        \n",
    "        # Skip empty content\n",
    "        if not content.strip():\n",
    "            continue\n",
    "            \n",
    "        # Map ShareGPT roles to ChatML format\n",
    "        if role == \"human\":\n",
    "            chatml_role = \"user\"\n",
    "        elif role == \"gpt\":\n",
    "            chatml_role = \"assistant\"\n",
    "        elif role == \"system\":\n",
    "            chatml_role = \"system\"\n",
    "        else:\n",
    "            # Skip unknown roles\n",
    "            continue\n",
    "            \n",
    "        # Add ChatML formatting\n",
    "        formatted_text += f\"<|im_start|>{chatml_role}\\n{content.strip()}<|im_end|>\\n\"\n",
    "    \n",
    "    return formatted_text.strip()\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Convert ShareGPT conversations to ChatML format\n",
    "    Works reliably without template dependencies\n",
    "    \"\"\"\n",
    "    convos = examples[\"conversations\"]\n",
    "    texts = []\n",
    "    \n",
    "    for convo in convos:\n",
    "        try:\n",
    "            formatted_text = format_chatml_conversation(convo)\n",
    "            if formatted_text:  # Only add non-empty conversations\n",
    "                texts.append(formatted_text)\n",
    "            else:\n",
    "                texts.append(\"\")  # Placeholder for empty conversations\n",
    "        except Exception as e:\n",
    "            print(f\"Error formatting conversation: {e}\")\n",
    "            texts.append(\"\")\n",
    "    \n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Load and process dataset\n",
    "print(\"Loading Guanaco ShareGPT dataset...\")\n",
    "\n",
    "try:\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(\"philschmid/guanaco-sharegpt-style\", split=\"train\")\n",
    "    print(f\"‚úì Dataset loaded: {len(dataset)} conversations\")\n",
    "    \n",
    "    # Debug: Show original format\n",
    "    print(f\"‚úì Original columns: {dataset.column_names}\")\n",
    "    print(\"‚úì Sample conversation structure:\")\n",
    "    sample_convo = dataset[0][\"conversations\"]\n",
    "    print(f\"   First turn: {sample_convo[0]}\")\n",
    "    print(f\"   Second turn: {sample_convo[1] if len(sample_convo) > 1 else 'N/A'}\")\n",
    "    \n",
    "    # Apply manual ChatML formatting\n",
    "    print(\"Applying manual ChatML formatting...\")\n",
    "    \n",
    "    dataset = dataset.map(\n",
    "        formatting_prompts_func,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Manual ChatML formatting\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úì Manual ChatML formatting complete\")\n",
    "    print(f\"‚úì Dataset columns after formatting: {dataset.column_names}\")\n",
    "    \n",
    "    # Filter out empty conversations\n",
    "    original_size = len(dataset)\n",
    "    dataset = dataset.filter(lambda x: len(x[\"text\"].strip()) > 20)\n",
    "    filtered_size = len(dataset)\n",
    "    \n",
    "    print(f\"‚úì Original: {original_size}, After filtering: {filtered_size}\")\n",
    "    print(f\"‚úì Filtered out {original_size - filtered_size} empty/invalid conversations\")\n",
    "    \n",
    "    if filtered_size > 0:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"SAMPLE CHATML FORMATTED CONVERSATION:\")\n",
    "        print(\"=\"*70)\n",
    "        print(dataset[0][\"text\"])\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(f\"\\n‚úì SUCCESS: {filtered_size} ChatML conversations ready for training!\")\n",
    "        \n",
    "        # Show conversation #5 if available\n",
    "        if len(dataset) > 5:\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"CONVERSATION #5:\")\n",
    "            print(\"=\"*70)\n",
    "            print(dataset[5][\"text\"])\n",
    "            print(\"=\"*70)\n",
    "        \n",
    "        # Memory cleanup\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå No valid conversations after formatting!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error with main dataset: {e}\")\n",
    "    print(\"Trying simpler alternative...\")\n",
    "    \n",
    "    try:\n",
    "        # Fallback: Create a small test dataset manually\n",
    "        print(\"Creating test ChatML dataset...\")\n",
    "        \n",
    "        test_conversations = [\n",
    "            {\n",
    "                \"conversations\": [\n",
    "                    {\"from\": \"human\", \"value\": \"Hello, how are you?\"},\n",
    "                    {\"from\": \"gpt\", \"value\": \"I'm doing well, thank you! How can I help you today?\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"conversations\": [\n",
    "                    {\"from\": \"human\", \"value\": \"What is Python?\"},\n",
    "                    {\"from\": \"gpt\", \"value\": \"Python is a high-level programming language known for its simplicity and readability.\"}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"conversations\": [\n",
    "                    {\"from\": \"human\", \"value\": \"Explain machine learning\"},\n",
    "                    {\"from\": \"gpt\", \"value\": \"Machine learning is a subset of AI that enables computers to learn and improve from data without being explicitly programmed.\"}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Convert to dataset format\n",
    "        from datasets import Dataset\n",
    "        dataset = Dataset.from_list(test_conversations)\n",
    "        \n",
    "        # Apply formatting\n",
    "        dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "        \n",
    "        print(\"‚úì Test dataset created successfully!\")\n",
    "        print(f\"‚úì Test conversations: {len(dataset)}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TEST CHATML CONVERSATION:\")\n",
    "        print(\"=\"*70)\n",
    "        print(dataset[0][\"text\"])\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        print(\"\\n‚úì Manual ChatML formatting is working!\")\n",
    "        print(\"You can now proceed with training setup.\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Even test dataset failed: {e2}\")\n",
    "        print(\"Check your datasets library installation:\")\n",
    "        print(\"pip install datasets --upgrade\")\n",
    "\n",
    "print(\"\\nüöÄ ChatML formatting complete - bypassed template issues!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U5iEWrUkevpE",
    "outputId": "4f6de3db-4c48-41b1-e505-f4efb1674a38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "What is the typical wattage of bulb in a lightbox?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "The typical wattage of a bulb in a lightbox is 60 watts, although domestic LED bulbs are normally much lower than 60 watts, as they produce the same or greater lumens for less wattage than alternatives. A 60-watt Equivalent LED bulb can be calculated using the 7:1 ratio, which divides 60 watts by 7 to get roughly 9 watts.<|im_end|>\n",
      "<|im_start|>user\n",
      "Rewrite your description of the typical wattage of a bulb in a lightbox to only include the key points in a list format.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[5][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuKOAUDpUeDL"
   },
   "source": [
    "If you're looking to make your own chat template, that also is possible! You must use the Jinja templating regime. We provide our own stripped down version of the `Unsloth template` which we find to be more efficient, and leverages ChatML, Zephyr and Alpaca styles.\n",
    "\n",
    "More info on chat templates on [our wiki page!](https://github.com/unslothai/unsloth/wiki#chat-templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idAEIeSQ3xdS"
   },
   "source": [
    "<a name=\"Train\"></a>\n",
    "### Train the model\n",
    "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verifying base model setup...\n",
      "‚úì Model type: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "‚úì Model device: cuda:0\n",
      "‚úì Model dtype: torch.float16\n",
      "‚úì Model is quantized - LoRA adapters required\n",
      "\n",
      "üîß Adding LoRA adapters to quantized model...\n",
      "Applying PEFT with conservative settings...\n",
      "‚ùå PEFT conversion failed: Unsloth: Your model already has LoRA adapters. Your new parameters are different.\n",
      "\n",
      "üîß Trying minimal LoRA configuration...\n",
      "‚ùå Even minimal PEFT failed: Unsloth: Your model already has LoRA adapters. Your new parameters are different.\n",
      "Your system may need a different approach or smaller model\n",
      "\n",
      "‚úÖ Validating PEFT model...\n",
      "‚úì Found 1764 LoRA modules\n",
      "‚úì Sample LoRA modules: ['base_model.model.model.layers.0.self_attn.q_proj.lora_dropout', 'base_model.model.model.layers.0.self_attn.q_proj.lora_dropout.default', 'base_model.model.model.layers.0.self_attn.q_proj.lora_A']\n",
      "‚úì Total parameters: 4,712,147,968\n",
      "‚úì Trainable parameters: 50,003,968\n",
      "‚úì Trainable percentage: 1.06%\n",
      "‚úì Model set to training mode\n",
      "‚úì GPU memory after PEFT: 5.40GB\n",
      "\n",
      "üöÄ Setting up SFT Trainer...\n",
      "‚úÖ SFT Trainer created successfully!\n",
      "‚úì Trainer model type: <class 'peft.peft_model.PeftModelForCausalLM'>\n",
      "‚úì Training dataset size: 9033\n",
      "‚úì Sequence length: 512\n",
      "‚úì Effective batch size: 4\n",
      "‚úì Final GPU memory: 5.40GB\n",
      "‚úÖ Memory usage looks good for training!\n",
      "\n",
      "üéØ Ready for training!\n",
      "Run: trainer_stats = trainer.train()\n",
      "\n",
      "============================================================\n",
      "SETUP COMPLETE - STARTING TRAINING\n",
      "============================================================\n",
      "üöÄ Starting fine-tuning training...\n",
      "Pre-training system status:\n",
      "GPU Memory - Used: 5.40GB, Cached: 5.40GB, Total: 8.0GB\n",
      "System RAM - Used: 2.0GB, Available: 21.1GB\n",
      "============================================================\n",
      "‚ñ∂Ô∏è  Executing: trainer.train()\n",
      "   Training for 20 steps with LoRA adapters...\n",
      "   Expected duration: 2-5 minutes on RTX 2070 Super\n",
      "   Monitoring for OOM errors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 9,033 | Num Epochs = 1 | Total steps = 20\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 4 x 1) = 4\n",
      " \"-____-\"     Trainable parameters = 50,003,968 of 8,587,684,864 (0.58% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.717100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.142100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.339400</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.491700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "‚úÖ TRAINING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "‚è±Ô∏è  Training Duration: 91.6 seconds (1.5 minutes)\n",
      "üìä Training Statistics:\n",
      "   - Total steps completed: 20\n",
      "   - Final training loss: 1.4226\n",
      "   - Steps per second: 0.22\n",
      "\n",
      "üñ•Ô∏è  Post-training system status:\n",
      "GPU Memory - Used: 5.65GB, Cached: 6.71GB, Total: 8.0GB\n",
      "System RAM - Used: 2.7GB, Available: 20.2GB\n",
      "üíæ Checkpoints saved: 2 in ./results/\n",
      "   Latest checkpoint: checkpoint-20\n",
      "üìä LoRA Statistics:\n",
      "   - LoRA parameters trained: 50,003,968\n",
      "   - LoRA rank: 8\n",
      "   - Target modules: 7\n",
      "\n",
      "üéØ Training Results Summary:\n",
      "============================================================\n",
      "‚úÖ Fine-tuning completed successfully without OOM errors\n",
      "‚úÖ LoRA adapters updated with new knowledge from ChatML dataset\n",
      "‚úÖ Model ready for inference testing or extended training\n",
      "‚úÖ Training checkpoints saved for recovery and deployment\n",
      "============================================================\n",
      "\n",
      "üìã Next Steps:\n",
      "1. Test the fine-tuned model with sample prompts\n",
      "2. Save the LoRA adapters: model.save_pretrained('./fine_tuned_lora')\n",
      "3. For production training, increase max_steps to 500-2000+\n",
      "4. Consider enabling packing=True for faster training if memory allows\n",
      "5. Evaluate model performance on validation data\n",
      "\n",
      "üíæ Training stats available in 'trainer_stats' variable:\n",
      "   - Global step: 20\n",
      "   - Training loss: 1.4226\n",
      "   - Log history: 0 entries\n",
      "\n",
      "üñ•Ô∏è  Final system status:\n",
      "GPU Memory - Used: 5.65GB, Cached: 6.71GB, Total: 8.0GB\n",
      "System RAM - Used: 2.7GB, Available: 20.2GB\n",
      "üßπ Final memory cleanup completed\n",
      "\n",
      "============================================================\n",
      "TRAINING EXECUTION COMPLETE\n",
      "============================================================\n",
      "\n",
      "üîç Final Model State:\n",
      "‚úì Model device: cuda:0\n",
      "‚úì Model in training mode: True\n",
      "‚úì Trainable parameters: 50,003,968\n",
      "‚úì Training completed: 20 steps\n",
      "‚úì Final loss: 1.4226\n",
      "\n",
      "üí° FOR PRODUCTION USE:\n",
      "- Increase max_steps to 500-2000 for better results\n",
      "- Use larger datasets for more comprehensive training\n",
      "- Enable packing=True if your memory allows (5x faster)\n",
      "- Monitor training loss and adjust learning rate if needed\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "‚Ä¢ The error occurs because the 4-bit quantized model needs LoRA adapters attached before fine-tuning\n",
    "‚Ä¢ Must verify that FastLanguageModel.get_peft_model() completed successfully before trainer setup\n",
    "‚Ä¢ Quantized models cannot be fine-tuned directly - they require trainable PEFT adapters on top\n",
    "‚Ä¢ Need to check if model has LoRA adapters properly attached and is in training mode\n",
    "‚Ä¢ This fix ensures PEFT model conversion works correctly before attempting SFT trainer setup\n",
    "‚Ä¢ Includes validation steps to confirm the model is ready for fine-tuning on RTX 2070 Super\n",
    "\n",
    "Key fixes for RTX 2070 Super:\n",
    "‚Ä¢ Explicit PEFT model validation before trainer setup to catch conversion failures\n",
    "‚Ä¢ Conservative LoRA settings (r=8) to ensure successful adapter attachment on 8GB VRAM\n",
    "‚Ä¢ Memory monitoring throughout PEFT conversion to prevent silent failures\n",
    "‚Ä¢ Fallback options if standard PEFT conversion fails due to memory constraints\n",
    "‚Ä¢ Trainer setup only proceeds after confirming LoRA adapters are properly attached\n",
    "‚Ä¢ Reduced sequence length (512) to accommodate both base model and LoRA adapters\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import gc\n",
    "\n",
    "# Step 1: Verify base model is loaded correctly\n",
    "print(\"üîç Verifying base model setup...\")\n",
    "try:\n",
    "    print(f\"‚úì Model type: {type(model)}\")\n",
    "    print(f\"‚úì Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"‚úì Model dtype: {next(model.parameters()).dtype}\")\n",
    "    \n",
    "    # Check if model is quantized\n",
    "    if hasattr(model, 'config') and hasattr(model.config, 'quantization_config'):\n",
    "        print(\"‚úì Model is quantized - LoRA adapters required\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Model quantization status unclear\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Base model verification failed: {e}\")\n",
    "    print(\"Please ensure the model was loaded successfully first\")\n",
    "    exit(1)\n",
    "\n",
    "# Step 2: Clear memory and add PEFT adapters\n",
    "print(\"\\nüîß Adding LoRA adapters to quantized model...\")\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    # Conservative LoRA settings for RTX 2070 Super\n",
    "    print(\"Applying PEFT with conservative settings...\")\n",
    "    \n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=8,  # Reduced from 16 to ensure success on 8GB VRAM\n",
    "        target_modules=[\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",      # Attention projections\n",
    "            \"gate_proj\", \"up_proj\", \"down_proj\",         # MLP layers\n",
    "        ],\n",
    "        lora_alpha=4, #8,  Set equal to r\n",
    "        lora_dropout=0,  # 0 is optimized for memory and speed\n",
    "        bias=\"none\",     # \"none\" is most memory efficient\n",
    "        use_gradient_checkpointing=\"unsloth\",  # Essential for memory savings\n",
    "        random_state=3407,\n",
    "        use_rslora=False,   # Disabled to save memory\n",
    "        loftq_config=None,  # Not needed for pre-quantized models\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ LoRA adapters added successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PEFT conversion failed: {e}\")\n",
    "    print(\"\\nüîß Trying minimal LoRA configuration...\")\n",
    "    \n",
    "    try:\n",
    "        # Ultra-minimal LoRA for problematic setups\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model,\n",
    "            r=4,  # Very small rank\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],  # Only essential modules\n",
    "            lora_alpha=2,\n",
    "            lora_dropout=0,\n",
    "            bias=\"none\",\n",
    "            use_gradient_checkpointing=\"unsloth\",\n",
    "            random_state=3407,\n",
    "        )\n",
    "        print(\"‚úÖ Minimal LoRA adapters added successfully!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Even minimal PEFT failed: {e2}\")\n",
    "        print(\"Your system may need a different approach or smaller model\")\n",
    "        exit(1)\n",
    "\n",
    "# Step 3: Validate PEFT model setup\n",
    "print(\"\\n‚úÖ Validating PEFT model...\")\n",
    "try:\n",
    "    # Check for LoRA adapters\n",
    "    peft_modules = [name for name, module in model.named_modules() if 'lora' in name.lower()]\n",
    "    if peft_modules:\n",
    "        print(f\"‚úì Found {len(peft_modules)} LoRA modules\")\n",
    "        print(f\"‚úì Sample LoRA modules: {peft_modules[:3]}\")\n",
    "    else:\n",
    "        print(\"‚ùå No LoRA modules found!\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    print(f\"‚úì Total parameters: {total_params:,}\")\n",
    "    print(f\"‚úì Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"‚úì Trainable percentage: {100 * trainable_params / total_params:.2f}%\")\n",
    "    \n",
    "    if trainable_params == 0:\n",
    "        print(\"‚ùå No trainable parameters found!\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    print(\"‚úì Model set to training mode\")\n",
    "    \n",
    "    # Check memory usage\n",
    "    if torch.cuda.is_available():\n",
    "        memory_used = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"‚úì GPU memory after PEFT: {memory_used:.2f}GB\")\n",
    "        \n",
    "        if memory_used > 7.0:\n",
    "            print(\"‚ö†Ô∏è WARNING: High memory usage - consider reducing sequence length further\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå PEFT validation failed: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Step 4: Setup SFT Trainer with corrected configuration\n",
    "print(\"\\nüöÄ Setting up SFT Trainer...\")\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "try:\n",
    "    # Updated training configuration for your setup\n",
    "    training_config = SFTConfig(\n",
    "        # Memory-critical settings for RTX 2070 Super\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        dataloader_num_workers=0,\n",
    "        \n",
    "        # Reduced sequence length for your setup\n",
    "        max_seq_length= max_seq_length,  # Further reduced from 254 to be safe\n",
    "        \n",
    "        # Training parameters\n",
    "        max_steps=20,  # Short test run\n",
    "        warmup_steps=5,\n",
    "        learning_rate=2e-4,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        \n",
    "        # Memory-efficient optimizer\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        max_grad_norm=0.3,\n",
    "        \n",
    "        # Data settings\n",
    "        dataset_text_field=\"text\",\n",
    "        \n",
    "        # Memory optimizations\n",
    "        fp16=True,\n",
    "        remove_unused_columns=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        \n",
    "        # Logging\n",
    "        logging_steps=5,\n",
    "        save_steps=10,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\",\n",
    "        \n",
    "        # Output\n",
    "        output_dir=\"./results\",\n",
    "        overwrite_output_dir=True,\n",
    "        \n",
    "        # Reproducibility\n",
    "        seed=3407,\n",
    "    )\n",
    "    \n",
    "    # Create trainer with PEFT model\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,  # This should now be the PEFT-enabled model\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset,\n",
    "        packing=False,  # Keep disabled for memory safety\n",
    "        args=training_config,\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ SFT Trainer created successfully!\")\n",
    "    \n",
    "    # Final validation\n",
    "    print(f\"‚úì Trainer model type: {type(trainer.model)}\")\n",
    "    print(f\"‚úì Training dataset size: {len(dataset)}\")\n",
    "    print(f\"‚úì Sequence length: {training_config.max_seq_length}\")\n",
    "    print(f\"‚úì Effective batch size: {training_config.per_device_train_batch_size * training_config.gradient_accumulation_steps}\")\n",
    "    \n",
    "    # Memory check\n",
    "    if torch.cuda.is_available():\n",
    "        final_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "        print(f\"‚úì Final GPU memory: {final_memory:.2f}GB\")\n",
    "        \n",
    "        if final_memory < 7.5:\n",
    "            print(\"‚úÖ Memory usage looks good for training!\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è High memory usage - monitor during training\")\n",
    "    \n",
    "    print(\"\\nüéØ Ready for training!\")\n",
    "    print(\"Run: trainer_stats = trainer.train()\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Trainer setup failed: {e}\")\n",
    "    print(\"\\nüîß Additional troubleshooting:\")\n",
    "    print(\"1. Restart kernel and reload model with lower max_seq_length\")\n",
    "    print(\"2. Try even smaller LoRA rank (r=2)\")\n",
    "    print(\"3. Use tinyllama model instead of codegemma\")\n",
    "    print(\"4. Ensure sufficient GPU memory available\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SETUP COMPLETE - STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 5: Execute Training\n",
    "import time\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "# Pre-training system check\n",
    "def check_system_resources():\n",
    "    \"\"\"Monitor system resources during training\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "        gpu_cached = torch.cuda.memory_reserved() / 1024**3\n",
    "        gpu_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU Memory - Used: {gpu_memory:.2f}GB, Cached: {gpu_cached:.2f}GB, Total: {gpu_total:.1f}GB\")\n",
    "    \n",
    "    ram_usage = psutil.virtual_memory()\n",
    "    print(f\"System RAM - Used: {ram_usage.used / 1024**3:.1f}GB, Available: {ram_usage.available / 1024**3:.1f}GB\")\n",
    "\n",
    "print(\"üöÄ Starting fine-tuning training...\")\n",
    "print(\"Pre-training system status:\")\n",
    "check_system_resources()\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear memory before training\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    print(\"‚ñ∂Ô∏è  Executing: trainer.train()\")\n",
    "    print(\"   Training for 20 steps with LoRA adapters...\")\n",
    "    print(\"   Expected duration: 2-5 minutes on RTX 2070 Super\")\n",
    "    print(\"   Monitoring for OOM errors...\")\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Execute the actual training\n",
    "    trainer_stats = trainer.train()\n",
    "    \n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Display comprehensive training results\n",
    "    print(f\"‚è±Ô∏è  Training Duration: {training_duration:.1f} seconds ({training_duration/60:.1f} minutes)\")\n",
    "    print(f\"üìä Training Statistics:\")\n",
    "    print(f\"   - Total steps completed: {trainer_stats.global_step}\")\n",
    "    print(f\"   - Final training loss: {trainer_stats.training_loss:.4f}\")\n",
    "    print(f\"   - Steps per second: {trainer_stats.global_step / training_duration:.2f}\")\n",
    "    \n",
    "    # Show detailed training progress\n",
    "    if hasattr(trainer_stats, 'log_history') and trainer_stats.log_history:\n",
    "        print(f\"\\nüìà Training Progress (Last 5 Steps):\")\n",
    "        for i, log_entry in enumerate(trainer_stats.log_history[-5:]):\n",
    "            step = log_entry.get('step', i)\n",
    "            loss = log_entry.get('train_loss', 'N/A')\n",
    "            lr = log_entry.get('learning_rate', 'N/A')\n",
    "            print(f\"   Step {step}: Loss={loss}, LR={lr}\")\n",
    "    \n",
    "    # Check training effectiveness\n",
    "    if hasattr(trainer_stats, 'log_history') and len(trainer_stats.log_history) > 1:\n",
    "        first_loss = trainer_stats.log_history[0].get('train_loss', 0)\n",
    "        last_loss = trainer_stats.log_history[-1].get('train_loss', 0)\n",
    "        if first_loss > last_loss:\n",
    "            improvement = ((first_loss - last_loss) / first_loss) * 100\n",
    "            print(f\"‚úÖ Loss improved by {improvement:.1f}% (from {first_loss:.4f} to {last_loss:.4f})\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Loss did not decrease - may need longer training or different parameters\")\n",
    "    \n",
    "    # Post-training system status\n",
    "    print(f\"\\nüñ•Ô∏è  Post-training system status:\")\n",
    "    check_system_resources()\n",
    "    \n",
    "    # Check saved checkpoints\n",
    "    if os.path.exists(\"./results\"):\n",
    "        checkpoints = [f for f in os.listdir(\"./results\") if f.startswith(\"checkpoint\")]\n",
    "        print(f\"üíæ Checkpoints saved: {len(checkpoints)} in ./results/\")\n",
    "        if checkpoints:\n",
    "            latest_checkpoint = max(checkpoints, key=lambda x: int(x.split('-')[-1]) if x.split('-')[-1].isdigit() else 0)\n",
    "            print(f\"   Latest checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    # Display LoRA adapter statistics\n",
    "    lora_params = sum(p.numel() for name, p in model.named_parameters() if 'lora' in name.lower() and p.requires_grad)\n",
    "    print(f\"üìä LoRA Statistics:\")\n",
    "    print(f\"   - LoRA parameters trained: {lora_params:,}\")\n",
    "    print(f\"   - LoRA rank: {8}\")  # Based on our configuration\n",
    "    print(f\"   - Target modules: {len(['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'])}\")\n",
    "    \n",
    "    print(\"\\nüéØ Training Results Summary:\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚úÖ Fine-tuning completed successfully without OOM errors\")\n",
    "    print(\"‚úÖ LoRA adapters updated with new knowledge from ChatML dataset\") \n",
    "    print(\"‚úÖ Model ready for inference testing or extended training\")\n",
    "    print(\"‚úÖ Training checkpoints saved for recovery and deployment\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nüìã Next Steps:\")\n",
    "    print(\"1. Test the fine-tuned model with sample prompts\")\n",
    "    print(\"2. Save the LoRA adapters: model.save_pretrained('./fine_tuned_lora')\")\n",
    "    print(\"3. For production training, increase max_steps to 500-2000+\")\n",
    "    print(\"4. Consider enabling packing=True for faster training if memory allows\")\n",
    "    print(\"5. Evaluate model performance on validation data\")\n",
    "    \n",
    "    # Save training statistics for later analysis\n",
    "    print(f\"\\nüíæ Training stats available in 'trainer_stats' variable:\")\n",
    "    print(f\"   - Global step: {trainer_stats.global_step}\")\n",
    "    print(f\"   - Training loss: {trainer_stats.training_loss:.4f}\")\n",
    "    print(f\"   - Log history: {len(trainer_stats.log_history) if hasattr(trainer_stats, 'log_history') else 0} entries\")\n",
    "\n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e).lower():\n",
    "        print(\"‚ùå OUT OF MEMORY ERROR!\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"üîß Solutions for RTX 2070 Super:\")\n",
    "        print(\"1. Further reduce max_seq_length from 512 to 256\")\n",
    "        print(\"2. Reduce LoRA rank from r=8 to r=4\")\n",
    "        print(\"3. Use only attention modules: target_modules=['q_proj', 'v_proj']\")\n",
    "        print(\"4. Reduce gradient_accumulation_steps from 4 to 2\")\n",
    "        print(\"5. Try gradient_checkpointing='unsloth' (already enabled)\")\n",
    "        \n",
    "        # Clear memory for recovery\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "        print(\"üßπ GPU memory cleared for recovery\")\n",
    "        \n",
    "        print(f\"\\nCurrent memory after cleanup: {torch.cuda.memory_allocated() / 1024**3:.2f}GB\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"‚ùå Training failed with RuntimeError: {e}\")\n",
    "        print(\"Check the error details above for specific issues\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n‚èπÔ∏è  Training interrupted by user (Ctrl+C)\")\n",
    "    print(\"üíæ Partial training progress may have been saved to ./results/\")\n",
    "    if os.path.exists(\"./results\"):\n",
    "        checkpoints = [f for f in os.listdir(\"./results\") if f.startswith(\"checkpoint\")]\n",
    "        if checkpoints:\n",
    "            latest = max(checkpoints, key=lambda x: int(x.split('-')[-1]) if x.split('-')[-1].isdigit() else 0)\n",
    "            print(f\"   Latest partial checkpoint: {latest}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Unexpected error during training: {e}\")\n",
    "    print(\"Full error details:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(\"\\nüîß General troubleshooting:\")\n",
    "    print(\"1. Check model, tokenizer, and dataset are properly loaded\")\n",
    "    print(\"2. Verify PEFT conversion completed successfully\")\n",
    "    print(\"3. Ensure sufficient disk space in ./results/ directory\")\n",
    "    print(\"4. Check for any data corruption in the dataset\")\n",
    "\n",
    "finally:\n",
    "    # Always show final system status and cleanup\n",
    "    print(f\"\\nüñ•Ô∏è  Final system status:\")\n",
    "    check_system_resources()\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    gc.collect()\n",
    "    print(\"üßπ Final memory cleanup completed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING EXECUTION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display final model state\n",
    "print(\"\\nüîç Final Model State:\")\n",
    "try:\n",
    "    print(f\"‚úì Model device: {next(model.parameters()).device}\")\n",
    "    print(f\"‚úì Model in training mode: {model.training}\")\n",
    "    print(f\"‚úì Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "    \n",
    "    if 'trainer_stats' in locals():\n",
    "        print(f\"‚úì Training completed: {trainer_stats.global_step} steps\")\n",
    "        print(f\"‚úì Final loss: {trainer_stats.training_loss:.4f}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Training did not complete successfully\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Could not verify final model state: {e}\")\n",
    "\n",
    "print(\"\\nüí° FOR PRODUCTION USE:\")\n",
    "print(\"- Increase max_steps to 500-2000 for better results\")\n",
    "print(\"- Use larger datasets for more comprehensive training\")\n",
    "print(\"- Enable packing=True if your memory allows (5x faster)\")\n",
    "print(\"- Monitor training loss and adjust learning rate if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekOmTR1hSNcr"
   },
   "source": [
    "<a name=\"Inference\"></a>\n",
    "### Inference\n",
    "Let's run the model! Since we're using `ChatML`, use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kR3gIAX-SM2q",
    "outputId": "8e932364-849a-40ad-e0b5-5f5ffd028f9c"
   },
   "outputs": [],
   "source": [
    "# from unsloth.chat_templates import get_chat_template\n",
    "\n",
    "# tokenizer = get_chat_template(\n",
    "#     tokenizer,\n",
    "#     chat_template = \"chatml\", # Supports zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old, unsloth\n",
    "#     mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    "#     map_eos_token = True, # Maps <|im_end|> to </s> instead\n",
    "# )\n",
    "\n",
    "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# messages = [\n",
    "#     {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "# ]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize = True,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     return_tensors = \"pt\",\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(input_ids = inputs, max_new_tokens = 64, use_cache = True)\n",
    "# tokenizer.batch_decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 2070 SUPER\n",
      "GPU Memory: 8.0GB\n",
      "\n",
      "============================================================\n",
      "MANUAL CHATML INFERENCE\n",
      "============================================================\n",
      "Formatted ChatML prompt:\n",
      "<|im_start|>user\n",
      "Continue the fibonacci sequence: 1, 1, 2, 3, 5, 8,<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "----------------------------------------\n",
      "Input shape: torch.Size([1, 46])\n",
      "Using device: cuda\n",
      "\n",
      "Generating response (streaming):\n",
      "----------------------------------------\n",
      "Sure, here is the next number in the Fibonacci sequence: 13. \n",
      "\n",
      "The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones. It starts with 0 and 1, and the next number is the sum of the previous two. So, to find the next number in the sequence, we add the last two numbers together.\n",
      "\n",
      "The sequence continues as follows:\n",
      "\n",
      "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144\n",
      "\n",
      "----------------------------------------\n",
      "Generation complete!\n",
      "\n",
      "Full conversation:\n",
      "============================================================\n",
      "<|im_start|>user\n",
      "Continue the fibonacci sequence: 1, 1, 2, 3, 5, 8,<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Sure, here is the next number in the Fibonacci sequence: 13. \n",
      "\n",
      "The Fibonacci sequence is a series of numbers in which each number is the sum of the two preceding ones. It starts with 0 and 1, and the next number is the sum of the previous two. So, to find the next number in the sequence, we add the last two numbers together.\n",
      "\n",
      "The sequence continues as follows:\n",
      "\n",
      "1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144\n",
      "============================================================\n",
      "\n",
      "GPU Memory Used: 5.89GB\n",
      "GPU Memory Cached: 5.99GB\n",
      "\n",
      "üöÄ Manual ChatML inference complete - bypassed template issues!\n",
      "\n",
      "============================================================\n",
      "TESTING MULTI-TURN CONVERSATION\n",
      "============================================================\n",
      "Multi-turn ChatML prompt:\n",
      "<|im_start|>user\n",
      "What is machine learning?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Machine learning is a subset of AI that enables computers to learn from data.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you give me a simple example?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "Multi-turn response (streaming):\n",
      "----------------------------------------\n",
      "Sure. Let's say you have a dataset of images of dogs and cats. You can train a machine learning model to classify images as either dogs or cats. The model will learn to recognize the features that distinguish dogs from cats, such as fur, tail, and ears. Once trained, the model can be used\n",
      "\n",
      "----------------------------------------\n",
      "Multi-turn generation complete!\n",
      "\n",
      "‚úÖ All manual ChatML inference tests complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import TextStreamer\n",
    "\n",
    "# Clear GPU memory before inference\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "\n",
    "# Manual ChatML formatting function - bypasses template issues\n",
    "def format_chatml_manual(messages):\n",
    "    \"\"\"\n",
    "    Manually format messages to ChatML format without using tokenizer.apply_chat_template\n",
    "    Avoids protobuf and template compatibility issues\n",
    "    \"\"\"\n",
    "    formatted_text = \"\"\n",
    "    \n",
    "    for message in messages:\n",
    "        # Handle ShareGPT format\n",
    "        role = message.get(\"from\", message.get(\"role\", \"\"))\n",
    "        content = message.get(\"value\", message.get(\"content\", \"\"))\n",
    "        \n",
    "        # Map ShareGPT roles to ChatML format\n",
    "        if role == \"human\":\n",
    "            chatml_role = \"user\"\n",
    "        elif role == \"gpt\":\n",
    "            chatml_role = \"assistant\"\n",
    "        elif role == \"system\":\n",
    "            chatml_role = \"system\"\n",
    "        else:\n",
    "            chatml_role = \"user\"  # Default fallback\n",
    "            \n",
    "        # Add ChatML formatting\n",
    "        formatted_text += f\"<|im_start|>{chatml_role}\\n{content.strip()}<|im_end|>\\n\"\n",
    "    \n",
    "    # Add generation prompt for assistant response\n",
    "    formatted_text += \"<|im_start|>assistant\\n\"\n",
    "    \n",
    "    return formatted_text\n",
    "\n",
    "# Enable inference mode\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "model.eval()\n",
    "\n",
    "# Test messages\n",
    "messages = [\n",
    "    {\"from\": \"human\", \"value\": \"Continue the fibonacci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MANUAL CHATML INFERENCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Format manually without using tokenizer.apply_chat_template\n",
    "    formatted_prompt = format_chatml_manual(messages)\n",
    "    print(\"Formatted ChatML prompt:\")\n",
    "    print(formatted_prompt)\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Tokenize manually\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048  # Adjust based on your model's context length\n",
    "    )\n",
    "    \n",
    "    # Move to GPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    \n",
    "    print(f\"Input shape: {input_ids.shape}\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Setup TextStreamer for real-time output\n",
    "    text_streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\nGenerating response (streaming):\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    # Generate with streaming\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            streamer=text_streamer,\n",
    "            max_new_tokens=128,\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"Generation complete!\")\n",
    "    \n",
    "    # Also get the full decoded output\n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\nFull conversation:\")\n",
    "    print(\"=\"*60)\n",
    "    print(full_response)\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Clear GPU memory after generation\n",
    "    del outputs, input_ids, attention_mask\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Manual inference error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Memory usage check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory Used: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB\")\n",
    "    print(f\"GPU Memory Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f}GB\")\n",
    "\n",
    "print(\"\\nüöÄ Manual ChatML inference complete - bypassed template issues!\")\n",
    "\n",
    "# Optional: Test with different conversation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING MULTI-TURN CONVERSATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    multi_turn_messages = [\n",
    "        {\"from\": \"human\", \"value\": \"What is machine learning?\"},\n",
    "        {\"from\": \"gpt\", \"value\": \"Machine learning is a subset of AI that enables computers to learn from data.\"},\n",
    "        {\"from\": \"human\", \"value\": \"Can you give me a simple example?\"},\n",
    "    ]\n",
    "    \n",
    "    formatted_multi = format_chatml_manual(multi_turn_messages)\n",
    "    print(\"Multi-turn ChatML prompt:\")\n",
    "    print(formatted_multi)\n",
    "    \n",
    "    inputs_multi = tokenizer(\n",
    "        formatted_multi,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=2048\n",
    "    ).to(device)\n",
    "    \n",
    "    print(\"\\nMulti-turn response (streaming):\")\n",
    "    print(\"-\"*40)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs_multi = model.generate(\n",
    "            **inputs_multi,\n",
    "            streamer=text_streamer,\n",
    "            max_new_tokens=64,\n",
    "            use_cache=True,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id if tokenizer.eos_token_id else tokenizer.pad_token_id,\n",
    "        )\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*40)\n",
    "    print(\"Multi-turn generation complete!\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del outputs_multi, inputs_multi\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Multi-turn inference error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ All manual ChatML inference tests complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrSvZObor0lY"
   },
   "source": [
    " You can also use a `TextStreamer` for continuous inference - so you can see the generation token by token, instead of waiting the whole time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2pEuRb1r2Vg",
    "outputId": "6b9ace79-d6f3-41aa-9e04-879506f14f4b"
   },
   "outputs": [],
   "source": [
    "# FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# messages = [\n",
    "#     {\"from\": \"human\", \"value\": \"Continue the fibonnaci sequence: 1, 1, 2, 3, 5, 8,\"},\n",
    "# ]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize = True,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     return_tensors = \"pt\",\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "# text_streamer = TextStreamer(tokenizer)\n",
    "# _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMuVrWbjAzhc"
   },
   "source": [
    "<a name=\"Save\"></a>\n",
    "### Saving, loading finetuned models\n",
    "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
    "\n",
    "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LORA MODEL SAVING\n",
      "============================================================\n",
      "Current directory: /mnt/c/models/llm/test\n",
      "Save directory: /mnt/c/models/llm/test/lora_model\n",
      "\n",
      "1. LOCAL SAVING:\n",
      "----------------------------------------\n",
      "Saving LoRA adapters locally...\n",
      "‚úì LoRA adapters saved to: /mnt/c/models/llm/test/lora_model\n",
      "‚úì Files saved:\n",
      "   - adapter_config.json (0.0 MB)\n",
      "   - adapter_model.safetensors (190.8 MB)\n",
      "   - README.md (0.0 MB)\n",
      "‚úì Total size: 190.8 MB\n",
      "‚úì Local save complete!\n",
      "\n",
      "2. ONLINE SAVING (Hugging Face Hub):\n",
      "----------------------------------------\n",
      "‚ö†Ô∏è  Online saving is commented out - requires:\n",
      "   - Hugging Face account and token\n",
      "   - Internet connection\n",
      "   - Unique model name\n",
      "üìù To enable online saving:\n",
      "   1. Get HF token: https://huggingface.co/settings/tokens\n",
      "   2. Uncomment the online saving code above\n",
      "   3. Replace placeholders with your actual values\n",
      "\n",
      "3. HOW TO LOAD SAVED LORA MODEL:\n",
      "----------------------------------------\n",
      "To load your saved LoRA model later:\n",
      "\n",
      "from unsloth import FastLanguageModel\n",
      "\n",
      "# Load base model + your LoRA adapters\n",
      "model, tokenizer = FastLanguageModel.from_pretrained(\n",
      "    model_name=\"unsloth/codegemma-7b-bnb-4bit\",  # Base model\n",
      "    max_seq_length=2048,\n",
      "    dtype=None,\n",
      "    load_in_4bit=True,\n",
      ")\n",
      "\n",
      "# Load your LoRA adapters\n",
      "model = PeftModel.from_pretrained(model, \"lora_model\")  # Local path\n",
      "# OR from Hub: model = PeftModel.from_pretrained(model, \"your_name/lora_model\")\n",
      "\n",
      "# Enable inference\n",
      "model = FastLanguageModel.for_inference(model)\n",
      "\n",
      "\n",
      "============================================================\n",
      "LoRA SAVING SUMMARY:\n",
      "============================================================\n",
      "‚úì LoRA adapters contain only the fine-tuned parameters\n",
      "‚úì Small file size (typically 10-100MB vs 13GB+ for full model)\n",
      "‚úì Can be shared/deployed easily\n",
      "‚úì Always need base model + adapters for inference\n",
      "‚úì Local saving is immediate, Hub upload requires internet\n",
      "\n",
      "GPU Memory: 5.91GB used\n",
      "\n",
      "üöÄ LoRA model saving setup complete!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model Saving Section Explanation:\n",
    "\n",
    "Saves only the LoRA adapter weights locally to \"lora_model\" directory (not full model)\n",
    "Optional push_to_hub uploads LoRA adapters to Hugging Face Hub with authentication token\n",
    "LoRA adapters are small files (few MB) containing just the fine-tuned parameters\n",
    "Full base model stays separate - adapters get merged with base model during loading\n",
    "Local save for backup/reuse, Hub save for sharing/deployment across machines\n",
    "'''\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"LORA MODEL SAVING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Clear memory before saving\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Check current directory and create save path\n",
    "current_dir = Path.cwd()\n",
    "save_dir = current_dir / \"lora_model\"\n",
    "\n",
    "print(f\"Current directory: {current_dir}\")\n",
    "print(f\"Save directory: {save_dir}\")\n",
    "\n",
    "try:\n",
    "    # LOCAL SAVING - Save LoRA adapters only\n",
    "    print(\"\\n1. LOCAL SAVING:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    save_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save LoRA adapters (this is fast - only saves the adapter weights)\n",
    "    print(\"Saving LoRA adapters locally...\")\n",
    "    model.save_pretrained(str(save_dir))\n",
    "    \n",
    "    # Check what was saved\n",
    "    saved_files = list(save_dir.glob(\"*\"))\n",
    "    print(f\"‚úì LoRA adapters saved to: {save_dir}\")\n",
    "    print(\"‚úì Files saved:\")\n",
    "    for file in saved_files:\n",
    "        file_size = file.stat().st_size / (1024*1024)  # Size in MB\n",
    "        print(f\"   - {file.name} ({file_size:.1f} MB)\")\n",
    "    \n",
    "    total_size = sum(f.stat().st_size for f in saved_files) / (1024*1024)\n",
    "    print(f\"‚úì Total size: {total_size:.1f} MB\")\n",
    "    print(\"‚úì Local save complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Local saving failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# ONLINE SAVING (Hugging Face Hub) - Commented out by default\n",
    "print(\"\\n2. ONLINE SAVING (Hugging Face Hub):\")\n",
    "print(\"-\" * 40)\n",
    "print(\"‚ö†Ô∏è  Online saving is commented out - requires:\")\n",
    "print(\"   - Hugging Face account and token\")\n",
    "print(\"   - Internet connection\")\n",
    "print(\"   - Unique model name\")\n",
    "\n",
    "# Uncomment and configure the following for online saving:\n",
    "\"\"\"\n",
    "try:\n",
    "    # You need to:\n",
    "    # 1. Get your token from https://huggingface.co/settings/tokens\n",
    "    # 2. Replace \"your_name\" with your HF username\n",
    "    # 3. Replace \"lora_model\" with your desired model name\n",
    "    \n",
    "    hf_token = \"hf_xxxxxxxxxxxxxxxxxxxxxxxxxx\"  # Your actual token here\n",
    "    hf_username = \"your_username\"  # Your HF username\n",
    "    model_name = \"codegemma-7b-conversational-lora\"  # Your model name\n",
    "    \n",
    "    full_model_name = f\"{hf_username}/{model_name}\"\n",
    "    \n",
    "    print(f\"Uploading to: {full_model_name}\")\n",
    "    \n",
    "    model.push_to_hub(\n",
    "        full_model_name, \n",
    "        token=hf_token,\n",
    "        private=False,  # Set to True for private models\n",
    "        safe_serialization=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì LoRA adapters uploaded to: https://huggingface.co/{full_model_name}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Online saving failed: {e}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù To enable online saving:\")\n",
    "print(\"   1. Get HF token: https://huggingface.co/settings/tokens\")\n",
    "print(\"   2. Uncomment the online saving code above\")\n",
    "print(\"   3. Replace placeholders with your actual values\")\n",
    "\n",
    "# LOADING SAVED MODEL EXAMPLE\n",
    "print(\"\\n3. HOW TO LOAD SAVED LORA MODEL:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"To load your saved LoRA model later:\")\n",
    "print(\"\"\"\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Load base model + your LoRA adapters\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/codegemma-7b-bnb-4bit\",  # Base model\n",
    "    max_seq_length=2048,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Load your LoRA adapters\n",
    "model = PeftModel.from_pretrained(model, \"lora_model\")  # Local path\n",
    "# OR from Hub: model = PeftModel.from_pretrained(model, \"your_name/lora_model\")\n",
    "\n",
    "# Enable inference\n",
    "model = FastLanguageModel.for_inference(model)\n",
    "\"\"\")\n",
    "\n",
    "# Memory cleanup\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LoRA SAVING SUMMARY:\")\n",
    "print(\"=\"*60)\n",
    "print(\"‚úì LoRA adapters contain only the fine-tuned parameters\")\n",
    "print(\"‚úì Small file size (typically 10-100MB vs 13GB+ for full model)\")\n",
    "print(\"‚úì Can be shared/deployed easily\")\n",
    "print(\"‚úì Always need base model + adapters for inference\")\n",
    "print(\"‚úì Local saving is immediate, Hub upload requires internet\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nGPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f}GB used\")\n",
    "\n",
    "print(\"\\nüöÄ LoRA model saving setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEEcJ4qfC7Lp"
   },
   "source": [
    "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MKX_XKs_BNZR",
    "outputId": "e8cff9c9-3e85-46ea-9d6a-735935fc88a3"
   },
   "outputs": [],
   "source": [
    "# if False:\n",
    "#     from unsloth import FastLanguageModel\n",
    "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#         model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "#         max_seq_length = max_seq_length,\n",
    "#         dtype = dtype,\n",
    "#         load_in_4bit = load_in_4bit,\n",
    "#     )\n",
    "#     FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "# messages = [\n",
    "#     {\"from\": \"human\", \"value\": \"What is a famous tall tower in Paris?\"},\n",
    "# ]\n",
    "# inputs = tokenizer.apply_chat_template(\n",
    "#     messages,\n",
    "#     tokenize = True,\n",
    "#     add_generation_prompt = True, # Must add for generation\n",
    "#     return_tensors = \"pt\",\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "# text_streamer = TextStreamer(tokenizer)\n",
    "# _ = model.generate(input_ids = inputs, streamer = text_streamer, max_new_tokens = 128, use_cache = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from pathlib import Path\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MEMORY-OPTIMIZED LORA LOADING (RTX 2070 SUPER 8GB)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Disable Triton and optimize memory\n",
    "os.environ[\"DISABLE_TRITON\"] = \"1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:32\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "def clear_memory():\n",
    "    \"\"\"Aggressive memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB, Total: {total:.1f}GB\")\n",
    "    \n",
    "    ram_used = psutil.virtual_memory().used / 1024**3\n",
    "    ram_total = psutil.virtual_memory().total / 1024**3\n",
    "    print(f\"RAM Memory - Used: {ram_used:.2f}GB, Total: {ram_total:.1f}GB\")\n",
    "\n",
    "# Initial memory check\n",
    "clear_memory()\n",
    "print_memory_usage()\n",
    "\n",
    "lora_path = Path(\"lora_model\")\n",
    "if not lora_path.exists():\n",
    "    print(f\"‚ùå LoRA model not found\")\n",
    "    exit()\n",
    "\n",
    "print(\"\\nüíæ MEMORY-EFFICIENT MODEL LOADING:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    # Get base model name\n",
    "    peft_config = PeftConfig.from_pretrained(\"lora_model\")\n",
    "    base_model_name = peft_config.base_model_name_or_path\n",
    "    print(f\"Base model: {base_model_name}\")\n",
    "    \n",
    "    # 8-bit quantization config for memory efficiency\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_enable_fp32_cpu_offload=True,  # Offload to CPU when needed\n",
    "        llm_int8_threshold=6.0,\n",
    "    )\n",
    "    \n",
    "    print(\"Loading base model with 8-bit quantization...\")\n",
    "    \n",
    "    # Load base model with aggressive memory optimization\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        base_model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",  # Let it handle CPU/GPU split\n",
    "        trust_remote_code=True,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        max_memory={0: \"6GB\", \"cpu\": \"40GB\"},  # Reserve 2GB GPU buffer\n",
    "    )\n",
    "    \n",
    "    print_memory_usage()\n",
    "    \n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        base_model_name,\n",
    "        trust_remote_code=True,\n",
    "        model_max_length=256  # Very short sequences\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    clear_memory()\n",
    "    print(\"Loading LoRA adapters...\")\n",
    "    \n",
    "    # Load LoRA with memory optimization\n",
    "    model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        \"lora_model\",\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    model.eval()\n",
    "    print(\"‚úì Model loaded with 8-bit + LoRA!\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    # Ultra-lightweight generation function\n",
    "    def memory_efficient_generate(model, tokenizer, prompt_text, max_tokens=10):\n",
    "        \"\"\"\n",
    "        Memory-efficient generation with automatic cleanup\n",
    "        \"\"\"\n",
    "        clear_memory()\n",
    "        \n",
    "        print(f\"Generating with max {max_tokens} tokens...\")\n",
    "        \n",
    "        # Tokenize with minimal length\n",
    "        inputs = tokenizer(\n",
    "            prompt_text,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=128,  # Very short input\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        \n",
    "        # Move to same device as model\n",
    "        device = next(model.parameters()).device\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        \n",
    "        print(f\"Input length: {input_ids.shape[1]} tokens\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print_memory_usage()\n",
    "        \n",
    "        generated_tokens = []\n",
    "        current_input = input_ids.clone()\n",
    "        \n",
    "        print(\"\\nGenerating:\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        try:\n",
    "            for step in range(max_tokens):\n",
    "                # Clear memory before each step\n",
    "                if step % 3 == 0:  # Every 3 steps\n",
    "                    clear_memory()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Forward pass with gradient checkpointing disabled\n",
    "                    outputs = model(current_input, use_cache=False)\n",
    "                    logits = outputs.logits\n",
    "                \n",
    "                # Get next token\n",
    "                next_token_logits = logits[0, -1, :]\n",
    "                next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "                \n",
    "                # Decode and print\n",
    "                token_text = tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
    "                print(token_text, end='', flush=True)\n",
    "                \n",
    "                generated_tokens.append(next_token.item())\n",
    "                \n",
    "                # Stop conditions\n",
    "                if next_token.item() == tokenizer.eos_token_id:\n",
    "                    break\n",
    "                if len(token_text.strip()) == 0:  # Skip empty tokens\n",
    "                    continue\n",
    "                \n",
    "                # Add token and limit context\n",
    "                current_input = torch.cat([current_input, next_token], dim=1)\n",
    "                \n",
    "                # Keep only last 64 tokens to save memory\n",
    "                if current_input.shape[1] > 64:\n",
    "                    current_input = current_input[:, -32:]\n",
    "                \n",
    "                # Memory check\n",
    "                if torch.cuda.is_available():\n",
    "                    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "                    if allocated > 7.5:  # Near 8GB limit\n",
    "                        print(f\"\\n‚ö†Ô∏è Memory limit reached: {allocated:.2f}GB\")\n",
    "                        break\n",
    "                \n",
    "                # Cleanup intermediate tensors\n",
    "                del outputs, logits, next_token_logits, next_token\n",
    "                \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(f\"\\n‚ùå OOM at step {step}\")\n",
    "            clear_memory()\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error at step {step}: {e}\")\n",
    "        \n",
    "        print(f\"\\n\\nGenerated {len(generated_tokens)} tokens\")\n",
    "        clear_memory()\n",
    "        return generated_tokens\n",
    "    \n",
    "    # Test with minimal prompt\n",
    "    print(\"\\nüíæ MEMORY-EFFICIENT INFERENCE TEST:\")\n",
    "    print(\"-\" * 45)\n",
    "    \n",
    "    simple_prompt = \"Hello\"\n",
    "    print(f\"Test prompt: '{simple_prompt}'\")\n",
    "    \n",
    "    generated = memory_efficient_generate(\n",
    "        model, \n",
    "        tokenizer, \n",
    "        simple_prompt, \n",
    "        max_tokens=8  # Very small\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úì Generated tokens: {generated}\")\n",
    "    print_memory_usage()\n",
    "    \n",
    "    # Test ChatML if first test works\n",
    "    if len(generated) > 0:\n",
    "        print(\"\\nüíæ CHATML TEST:\")\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "        chatml_prompt = \"<|im_start|>user\\nHi<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        print(f\"ChatML prompt: {chatml_prompt}\")\n",
    "        \n",
    "        generated2 = memory_efficient_generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            chatml_prompt,\n",
    "            max_tokens=12\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úì ChatML tokens: {generated2}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üíæ MEMORY-OPTIMIZED SUCCESS!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"‚úì Used 8-bit quantization\")\n",
    "    print(\"‚úì CPU/GPU memory offloading\")\n",
    "    print(\"‚úì Short context windows\")\n",
    "    print(\"‚úì Aggressive memory cleanup\")\n",
    "    print(\"‚úì OOM protection\")\n",
    "    \n",
    "    final_memory_usage = torch.cuda.memory_allocated(0) / 1024**3 if torch.cuda.is_available() else 0\n",
    "    print(f\"‚úì Final GPU usage: {final_memory_usage:.2f}GB / 8GB\")\n",
    "    \n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    print(f\"‚ùå Still OOM: {e}\")\n",
    "    print(\"\\nüí° SOLUTIONS FOR RTX 2070 SUPER:\")\n",
    "    print(\"-\" * 35)\n",
    "    print(\"1. Use CPU-only inference:\")\n",
    "    print(\"   device_map='cpu'\")\n",
    "    print(\"2. Try 4-bit quantization instead of 8-bit\")\n",
    "    print(\"3. Use smaller model (CodeGemma-2B)\")\n",
    "    print(\"4. Reduce max_length to 64 tokens\")\n",
    "    print(\"5. Use model sharding with accelerate\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Loading failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "finally:\n",
    "    clear_memory()\n",
    "    print_memory_usage()\n",
    "\n",
    "print(\"\\nüíæ Memory-optimized loading complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QQMjaNrjsU5_"
   },
   "source": [
    "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yFfaXG0WsQuE"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # I highly do NOT suggest - use Unsloth if possible\n",
    "    from peft import AutoModelForPeftCausalLM\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    model = AutoModelForPeftCausalLM.from_pretrained(\n",
    "        \"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        load_in_4bit = load_in_4bit,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f422JgM9sdVT"
   },
   "source": [
    "### Saving to float16 for VLLM\n",
    "\n",
    "We also support saving to `float16` directly. Select `merged_16bit` for float16 or `merged_4bit` for int4. We also allow `lora` adapters as a fallback. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iHjt_SMYsd3P"
   },
   "outputs": [],
   "source": [
    "# Merge to 16bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_16bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_16bit\", token = \"\")\n",
    "\n",
    "# Merge to 4bit\n",
    "if False: model.save_pretrained_merged(\"model\", tokenizer, save_method = \"merged_4bit\",)\n",
    "if False: model.push_to_hub_merged(\"hf/model\", tokenizer, save_method = \"merged_4bit\", token = \"\")\n",
    "\n",
    "# Just LoRA adapters\n",
    "if False:\n",
    "    model.save_pretrained(\"model\")\n",
    "    tokenizer.save_pretrained(\"model\")\n",
    "if False:\n",
    "    model.push_to_hub(\"hf/model\", token = \"\")\n",
    "    tokenizer.push_to_hub(\"hf/model\", token = \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCv4vXHd61i7"
   },
   "source": [
    "### GGUF / llama.cpp Conversion\n",
    "To save to `GGUF` / `llama.cpp`, we support it natively now! We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
    "\n",
    "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
    "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
    "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
    "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "FqfebeAdT073"
   },
   "outputs": [],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
