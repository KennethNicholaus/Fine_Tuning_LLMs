{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GWiqzepPUZc"
   },
   "source": [
    "# Fine_Tune_granite-3.3-2b-instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz3InP0nU6Nx"
   },
   "source": [
    "This Notebooke provides a guide to fine-tuning Granite 3.3 2B Instruct model using [Unsloth](https://github.com/unslothai/unsloth?tab=readme-ov-file), an optimized open-source framework designed for efficient LLM fine-tuning and reinforcement learning.\n",
    "\n",
    "Fine-tuning refers to the process of further training a pre-trained model on a task-specific dataset to improve its performance in specialized contexts. Here, we focus on Low-Rank Adaptation (LoRA), a method within the broader category of Parameter-Efficient Fine-Tuning (PEFT), where only a subset of model parameters are modified. PEFT methods preserve the majority of the pre-trained knowledge, hence minimizing the risks of catastrophic forgetting.\n",
    "\n",
    "Fine-tuning is particularly valuable when prompting or retrieval-based techniques fall short. While prompt engineering enables zero-shot or few-shot learning, it often results in inconsistent outputs, especially for complex tasks or domain-specific requirements. Similarly, Retrieval-Augmented Generation (RAG) enhances factual grounding by incorporating external context but does not alter the model's underlying reasoning or stylistic behavior. Fine-tuning addresses these limitations by embedding the desired patterns, tone, and logic directly into the model, resulting in more robust and reliable outputs.\n",
    "\n",
    "There are several distinct types of fine-tuning, each suited to different use cases. Instruction tuning aligns the model with general task-following behavior, conversation tuning optimizes it for dialogue and multi-turn interactions, and domain-specific tuning adapts the model to specialized fields.\n",
    "\n",
    "This recipe explores domain specific training and fine-tunes the model to perform better on math reasoning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1X6SSe5F5p5R"
   },
   "source": [
    "**THIS NOTEBOOK WORKS IN LINUX/WINDOWS ENVIRONMENT AND REQUIRES A CUDA-ENABLED GPU (NVIDIA GPU).**\n",
    "\n",
    "Please refer to the Unsloth system requirements [here](https://docs.unsloth.ai/get-started/beginner-start-here/unsloth-requirements).\n",
    "\n",
    "This notebook has been optimized to run efficiently on a single NVIDIA RTX 2070 Super GPU with 8GB VRAM. The code configurations, batch sizes, and memory management have been specifically tuned for this hardware to ensure smooth fine-tuning without memory overflow issues.\n",
    "\n",
    "If you want to fine-tune using larger datasets or models, you may need a machine with a more powerful GPU. Your local computer can't run this notebook without a CUDA-enabled GPU.\n",
    "\n",
    "**Hardware Requirements:**\n",
    "- **GPU**: NVIDIA RTX 2070 Super (8GB VRAM) or equivalent\n",
    "- **CUDA**: Compatible CUDA drivers installed\n",
    "- **Memory**: Sufficient system RAM to support GPU operations\n",
    "\n",
    "**Troubleshooting for Local RTX 2070 Super Setup:**\n",
    "- **Verify CUDA installation**: Run `nvidia-smi` in terminal to confirm GPU is detected and CUDA drivers are properly installed\n",
    "- **Monitor GPU memory**: Use `nvidia-smi` during training to ensure VRAM usage stays within the 8GB limit\n",
    "- **Adjust batch size if needed**: If you encounter out-of-memory errors, reduce the batch size in the training configuration\n",
    "- **Check GPU utilization**: Ensure the GPU is being utilized efficiently during training processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrRNXzt8ufge"
   },
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wIH_Yt1aTGjs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ibm-granite-community/utils\n",
      "  Cloning https://github.com/ibm-granite-community/utils to /tmp/pip-req-build-p2yqryd6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ibm-granite-community/utils /tmp/pip-req-build-p2yqryd6\n",
      "  Resolved https://github.com/ibm-granite-community/utils to commit 60ecc5d292b5c33271586d5eb2bb53cf996f15ba\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.2.1)\n",
      "Requirement already satisfied: protobuf in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (6.32.0)\n",
      "Requirement already satisfied: datasets<4.0.0,>=3.4.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (3.6.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.34.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.34.4)\n",
      "Requirement already satisfied: hf_transfer in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.1.9)\n",
      "Requirement already satisfied: python-dotenv in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ibm-granite-community-utils==0.1.dev87) (1.1.1)\n",
      "Requirement already satisfied: langchain_core in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ibm-granite-community-utils==0.1.dev87) (0.3.75)\n",
      "Requirement already satisfied: typing_extensions in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from ibm-granite-community-utils==0.1.dev87) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (3.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (2.3.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (2.3.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2025.3.0)\n",
      "Requirement already satisfied: packaging in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from datasets<4.0.0,>=3.4.1) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from huggingface_hub>=0.34.0) (1.1.9)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets<4.0.0,>=3.4.1) (3.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from requests>=2.32.2->datasets<4.0.0,>=3.4.1) (2025.8.3)\n",
      "Requirement already satisfied: langsmith>=0.3.45 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from langchain_core->ibm-granite-community-utils==0.1.dev87) (0.4.22)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from langchain_core->ibm-granite-community-utils==0.1.dev87) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from langchain_core->ibm-granite-community-utils==0.1.dev87) (1.33)\n",
      "Requirement already satisfied: pydantic>=2.7.4 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from langchain_core->ibm-granite-community-utils==0.1.dev87) (2.11.7)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain_core->ibm-granite-community-utils==0.1.dev87) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain_core->ibm-granite-community-utils==0.1.dev87) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain_core->ibm-granite-community-utils==0.1.dev87) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain_core->ibm-granite-community-utils==0.1.dev87) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from langsmith>=0.3.45->langchain_core->ibm-granite-community-utils==0.1.dev87) (0.24.0)\n",
      "Requirement already satisfied: anyio in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core->ibm-granite-community-utils==0.1.dev87) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core->ibm-granite-community-utils==0.1.dev87) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core->ibm-granite-community-utils==0.1.dev87) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain_core->ibm-granite-community-utils==0.1.dev87) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain_core->ibm-granite-community-utils==0.1.dev87) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pydantic>=2.7.4->langchain_core->ibm-granite-community-utils==0.1.dev87) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain_core->ibm-granite-community-utils==0.1.dev87) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from pandas->datasets<4.0.0,>=3.4.1) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets<4.0.0,>=3.4.1) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: bitsandbytes in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.47.0)\n",
      "Requirement already satisfied: accelerate in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (1.10.1)\n",
      "Requirement already satisfied: xformers==0.0.29.post3 in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.0.29.post3)\n",
      "Requirement already satisfied: peft in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.17.1)\n",
      "Requirement already satisfied: trl in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (0.22.1)\n",
      "Requirement already satisfied: tqdm in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: triton in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (3.2.0)\n",
      "Requirement already satisfied: cut_cross_entropy in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (25.1.1)\n",
      "Requirement already satisfied: unsloth_zoo in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (2025.8.9)\n",
      "Requirement already satisfied: unsloth in /home/golongson/miniconda3/envs/dl/lib/python3.11/site-packages (2025.8.10)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "  sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "\n",
    "%pip install --no-deps bitsandbytes \\\n",
    "  accelerate \\\n",
    "  xformers==0.0.29.post3 \\\n",
    "  peft \\\n",
    "  trl \\\n",
    "  tqdm \\\n",
    "  triton \\\n",
    "  cut_cross_entropy \\\n",
    "  unsloth_zoo \\\n",
    "  unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cedAoOdtvJai"
   },
   "source": [
    "## Fine Tuning Granite Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k6HuHjFGczMU"
   },
   "source": [
    "The Granite 3.3 2B model, while generally proficient in natural language understanding and generation, may struggle when tasked with specialized reasoning challenges such as high-accuracy mathematical problem solving. These limitations are expected, given the relatively smaller parameter size of the model. To address this, fine-tuning the Granite 3.3 2B model on domain-specific mathematical dataset is a promising approach to improve its accuracy and reliability in quantitative tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KShI9NiiHfEW"
   },
   "source": [
    "### Loading the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gkcj-BHy3zH"
   },
   "source": [
    "In this section, we load the Granite 3.3 2B Instruct base model, preparing it for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QdgxJuA5u_Iq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.8.10: Fast Granite patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2070 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d01615d4b0f4c4297d21e24da5099e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2009c6ef0e6e48fa8fac705eafc1aff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3972aa2ce3814020a2e1bfed9637c617",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328a2ec87df2439f86c33d0b49a4b295",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4481b7b852794b609c76118fd0c6692d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d15e38bbbf4fd0bf5d003b60f4c35b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58c0822289a42c0a2f52668734ca1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a30a9e7273045249779dbf39ebbe993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a079f1604c4bb786b96090b4acf434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5a692e3d9e4e939bbd61af75c23454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/207 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb92164d564c4a49946a25b0c8d0a2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/801 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibm-granite/granite-3.3-2b-instruct does not have a padding token! Will use pad_token = <|end_of_text|>.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"ibm-granite/granite-3.3-2b-instruct\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaTq78_rbZKh"
   },
   "source": [
    "### Prepare the Math Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbFRUNkszVDB"
   },
   "source": [
    "Here, the code formats a math dataset into a chat-style prompt-response structure using the tokenizer's chat template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "xfruESI6Q4q2"
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def formatting_prompts_func(examples):\n",
    "    messages = []\n",
    "    for i in range(len(examples[\"problem\"])):\n",
    "        messages.append([\n",
    "            {\"role\": \"user\", \"content\": examples[\"problem\"][i]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"solution\"][i]}\n",
    "        ])\n",
    "    texts = [tokenizer.apply_chat_template(message, tokenize = False, add_generation_prompt = False) + EOS_TOKEN for message in messages]\n",
    "    return { \"text\" : texts, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "goA-dO8yQ8SX"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15698815cbc54c63a40362341192cd9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557d81d4c07b44efa40c6c7662664210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/2.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88be27e28b9c4b898df18d46d931e851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/1.86M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af792f60cd4743208ce6411f3397ed90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67e4e6535394954ace421f6d353a4f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab922164a87a4e42a3685bb34883d6f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"xDAN2099/lighteval-MATH\", split=\"train[:500]\", trust_remote_code=True)\n",
    "dataset = dataset.map(\n",
    "    formatting_prompts_func,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvZ7pMYP19ZI"
   },
   "source": [
    "This is how the final fine-tuning dataset samples looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OEWh-2p3RBAG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|start_of_role|>system<|end_of_role|>Knowledge Cutoff Date: April 2024.\n",
      "Today's Date: September 02, 2025.\n",
      "You are Granite, developed by IBM. You are a helpful AI assistant.<|end_of_text|>\n",
      "<|start_of_role|>user<|end_of_role|>Let \\[f(x) = \\left\\{\n",
      "\\begin{array}{cl} ax+3, &\\text{ if }x>2, \\\\\n",
      "x-5 &\\text{ if } -2 \\le x \\le 2, \\\\\n",
      "2x-b &\\text{ if } x <-2.\n",
      "\\end{array}\n",
      "\\right.\\]Find $a+b$ if the piecewise function is continuous (which means that its graph can be drawn without lifting your pencil from the paper).<|end_of_text|>\n",
      "<|start_of_role|>assistant<|end_of_role|>For the piecewise function to be continuous, the cases must \"meet\" at $2$ and $-2$. For example, $ax+3$ and $x-5$ must be equal when $x=2$. This implies $a(2)+3=2-5$, which we solve to get $2a=-6 \\Rightarrow a=-3$. Similarly, $x-5$ and $2x-b$ must be equal when $x=-2$. Substituting, we get $-2-5=2(-2)-b$, which implies $b=3$. So $a+b=-3+3=\\boxed{0}$.<|end_of_text|>\n",
      "<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6ew0rqTyprA"
   },
   "source": [
    "### LoRA fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B15d-yXI2b3g"
   },
   "source": [
    "We now add LoRA adapters for parameter efficient finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4OWnHgDbyf8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model` require gradients\n"
     ]
    }
   ],
   "source": [
    "target_modules =  [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                   \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    base_model,\n",
    "    r = 16, # Rank of lora matrices\n",
    "    target_modules = target_modules,  # Modules of the llm the lora weights are used\n",
    "    lora_alpha = 16, # scales the weights of the adapters\n",
    "    lora_dropout = 0, # Unsloth recommends 0 is better for fast patching\n",
    "    bias = \"none\",    # \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", #\"unsloth\" for very long context, decreases vram\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,\n",
    "    loftq_config = None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pVUQTDd3urM"
   },
   "source": [
    "Using Hugging Face TRL's SFTTrainer, we configure the training environment for the base model. Feel free to experiment with the training arguments to observe their impact on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Rv-FAiRHyt1n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: We found double BOS tokens - we shall remove one automatically.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09faad542bb5470fa89662bf62dd9a4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=20):   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = 2048,\n",
    "    dataset_num_proc = 2,\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        num_train_epochs = 2,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 25,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48SoOXw05VMm"
   },
   "source": [
    "We now initiate the fine-tuning of the model. With current training environment, the 1.1% of the parameters are trainable and it takes ~7 mins with the specified training arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ekUtiAz2y17y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 500 | Num Epochs = 2 | Total steps = 126\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 28,180,480 of 2,561,720,320 (1.10% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 05:09, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.019100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.548900</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.431700</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.358100</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPlx1Mmk8vO1"
   },
   "source": [
    "Here are some training statistics for your reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2_EREU6R5gYZ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.5447963480911557, metrics={'train_runtime': 355.8373, 'train_samples_per_second': 2.81, 'train_steps_per_second': 0.354, 'total_flos': 4910164085219328.0, 'train_loss': 0.5447963480911557, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMUe5Lr3y_q6"
   },
   "source": [
    "## Inference the fine-tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHZt85Fx7-QM"
   },
   "source": [
    "The fine-tuned model is ready for inferencing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  \\[\\frac{x^4+2y^2}{6} = \\frac{2^4+2\\cdot5^2}{6} = \\frac{16+2\\cdot25}{6}\n",
      "= \\frac{16+50}{6} = \\frac{66}{6} = \\boxed{11}.\\]\n"
     ]
    }
   ],
   "source": [
    "from ibm_granite_community.notebook_utils import wrap_text\n",
    "import torch\n",
    "\n",
    "# Ensure model is in inference mode\n",
    "model.eval()\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "query = \"If $x = 2$ and $y = 5$, then what is the value of $\\frac{x^4+2y^2}{6}$ ?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": query}\n",
    "]\n",
    "\n",
    "# Create input encoding\n",
    "encoding = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\",\n",
    "    return_dict=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Generate with minimal parameters to avoid cache issues\n",
    "with torch.no_grad():\n",
    "    try:\n",
    "        output_ids = model.generate(\n",
    "            input_ids=encoding[\"input_ids\"],\n",
    "            attention_mask=encoding[\"attention_mask\"],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,  # Reduced for stability\n",
    "            use_cache=False,\n",
    "            do_sample=False,\n",
    "            num_beams=1,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error with model.generate: {e}\")\n",
    "        # Fallback: try with base model\n",
    "        base_model = model.get_base_model()\n",
    "        output_ids = base_model.generate(\n",
    "            input_ids=encoding[\"input_ids\"],\n",
    "            attention_mask=encoding[\"attention_mask\"],\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            max_new_tokens=512,\n",
    "            use_cache=False,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "# Decode response\n",
    "response = tokenizer.decode(\n",
    "    output_ids[0][encoding[\"input_ids\"].shape[-1]:],\n",
    "    skip_special_tokens=True\n",
    ")\n",
    "print(wrap_text(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJTGMwhcaj6X"
   },
   "source": [
    "The expected response must be along the lines of:\n",
    "\n",
    "\n",
    "> We have  \\[\\frac{x^4 + 2y^2}{6} = \\frac{2^4 + 2(5^2)}{6} = \\frac{16+2(25)}{6} = \\frac{16+50}{6} = \\frac{66}{6} = \\boxed{11}.\\]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_9numO39Vti"
   },
   "source": [
    "You can also use a [TextStreamer](https://huggingface.co/docs/transformers.js/en/api/generation/streamers#module_generation/streamers.TextStreamer) for real-time inference, allowing you to view the model’s output token by token as it’s generated, rather than waiting for the full response. This is demonstrated in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9b6Z-zbYWp21"
   },
   "source": [
    "## Saving Fine Tuned model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7psby_mWt1V"
   },
   "source": [
    "The fine-tuned models can either be saved locally or online on HuggingFace. The models can then be loaded using FastLanguage model and set for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "upcOlWe7A1vc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Finetuned_Granite/tokenizer_config.json',\n",
       " 'Finetuned_Granite/special_tokens_map.json',\n",
       " 'Finetuned_Granite/chat_template.jinja',\n",
       " 'Finetuned_Granite/vocab.json',\n",
       " 'Finetuned_Granite/merges.txt',\n",
       " 'Finetuned_Granite/added_tokens.json',\n",
       " 'Finetuned_Granite/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"Finetuned_Granite\")  # Local saving\n",
    "tokenizer.save_pretrained(\"Finetuned_Granite\") # Local saving\n",
    "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
    "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ukhInbmja6nj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.10: Fast Granite patching. Transformers: 4.56.0.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2070 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472a7a5f520645dfb43462e4977a260f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ibm-granite/granite-3.3-2b-instruct does not have a padding token! Will use pad_token = <|end_of_text|>.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraniteForCausalLM(\n",
       "  (model): GraniteModel(\n",
       "    (embed_tokens): Embedding(49159, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-39): 40 x GraniteDecoderLayer(\n",
       "        (self_attn): GraniteAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GraniteMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): GraniteRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): GraniteRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): GraniteRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): GraniteRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=49159, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"Finetuned_Granite\", # Locally saved model\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = False,\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VIbfewS7cqkR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, we simplify the expressions inside the cube roots:\n",
      "$\\sqrt[3]{1+8} = \\sqrt[3]{9}$\n",
      "$\\sqrt[3]{1+\\sqrt[3]{8}} = \\sqrt[3]{1+2} = \\sqrt[3]{3}$\n",
      "Now, we can simplify the expression:\n",
      "$\\sqrt[3]{9} \\cdot \\sqrt[3]{3} = \\sqrt[3]{9 \\cdot 3} = \\sqrt[3]{27} = \\boxed{3}$The answer is: 3<|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "query = \"Simplify $\\sqrt[3]{1+8} \\cdot \\sqrt[3]{1+\\sqrt[3]{8}}$.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": query},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = True,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "# Create attention mask\n",
    "attention_mask = (inputs != tokenizer.pad_token_id).long()\n",
    "\n",
    "# Generate output with attention mask\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "_ = model.generate(\n",
    "    input_ids=inputs,\n",
    "    attention_mask=attention_mask,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUUnuCqScyFb"
   },
   "source": [
    "Expected response:\n",
    "\n",
    "\n",
    ">The first cube root becomes $\\sqrt[3]{9}$. $\\sqrt[3]{8}=2$, so the second cube root becomes $\\sqrt[3]{3}$. Multiplying these gives $\\sqrt[3]{27} = \\boxed{3}$.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
