{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0oSLJkXty-t_"
   },
   "source": [
    "### Summary of LoRA Fine-Tuning Workflow\n",
    "\n",
    "- Loaded a pretrained causal language model (`bloomz-560m`) and its tokenizer.\n",
    "- Prepared prompts dataset (`fka/awesome-chatgpt-prompts`) and tokenized the input prompts.\n",
    "- Configured LoRA (Low-Rank Adaptation) to fine-tune only a small subset of parameters efficiently.\n",
    "- Wrapped the base model with LoRA adapters using `peft` library.\n",
    "- Set up training using Hugging Face `Trainer` with efficient training arguments.\n",
    "- Fine-tuned the LoRA model on a subset of prompts.\n",
    "- Saved the fine-tuned LoRA adapters separately from the base model.\n",
    "- Loaded the saved LoRA adapters onto the base model for inference.\n",
    "- Generated motivational coaching style text using the LoRA-adapted model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "620MdVMk7iUS"
   },
   "source": [
    "# Load the PEFT and Datasets Libraries.\n",
    "\n",
    "The PEFT library contains the Hugging Face implementation of differente fine-tuning techniques, like LoRA Tuning.\n",
    "\n",
    "Using the Datasets library we have acces to a huge amount of Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_UyyuMGnCPjA"
   },
   "outputs": [],
   "source": [
    "# !pip install -q peft==0.8.2\n",
    "# !pip install -q datasets==2.16.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOnJlBY-81Wl"
   },
   "source": [
    "From the transformers library we import the necesary classes to import the model and the tokenizer.\n",
    "\n",
    "Then we can load the Tokenizer and the model.\n",
    "\n",
    "Bloom is one of the smallest and smarter model available to be trained with PEFT Library using Prompt Tuning. You can use either of the models in the Bloom Family, I encorage you to use at least two of them and see the differences.\n",
    "\n",
    "I'm using the smallest one just to spend less time trainig, and avoid memory problems in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "9c4ae639b7a94d72828fd8992a0ce24e",
      "4cf144fa96b4495ca27718417fbbd710",
      "b8ce23b406074da0a1059b2b8577e2ac",
      "0802cd99d88c452ebd51ae48ab215629",
      "768fe0039abd4272b852277ea80e23c0",
      "1bb5f4d404fb4872bb7e95ec54829e3f",
      "4cc96d0c624e4e49bc6d9dcc05dec34c",
      "0c81884bfe6c492ea23a3762999cdeb3",
      "aa584d38d70144dc9d729037d2c24b01",
      "8aea4c0000244f1a8dd37aac6a3f5d08",
      "f317e93cc7874a9f83b1a7f06cdd37a2",
      "de8c0b4005254099bb9089cd833da55f",
      "ec7ac8e078d1487e9f90c7b76a13493d",
      "e6f792a7efbe442ab605bbeba49b4438",
      "d0c62a258032483b9a7c055ff3975242",
      "7a9dcf30b7f0448085f513375a5b9995",
      "792ff0fe8e8541a6a9702def01bf99fa",
      "420edacdb9d5463cb82afac96d0bfdb4",
      "3a417829217144b687ca28c7200fea85",
      "55bf7b5ad77942be92a013728c2f9de6",
      "0582589f97734d55880f5cd1d6c21867",
      "21b175b750384f0880e772d83d18893b",
      "325192b6a1b74e2eab55c869304b61c9",
      "2f31b8ccc3584d26a5a76c82ab86c5f7",
      "a747845407684c5daf7ca6ae819300ad",
      "0f655829fc9b4cdfac3a258d4e145af6",
      "4cb1a75091c04ebe804c14d3f3fb0e48",
      "7d422388cdee46bcb2f49fb18c051fe2",
      "7a4c5b9c036d486a95802d81506acc6b",
      "5b0cddf46b804587a498ec7bd6212e59",
      "fdea1ab3425c456e8ac0885514f8b0c8",
      "2fbfe103488540598121a81609280d7d",
      "f609c995c16648dabbac316103f131e6",
      "64dae694360e4f1790ea495830cadb6e",
      "706086bc251446a4910437730a9aa09b",
      "157f327015934b2ba379cd6fa24e1606",
      "dd7c6cf3ac7e4d75b5888355b473bfee",
      "a8b55a52a2ac4f1ab01e17a8e819e27c",
      "ea5c047a408544d48ba274590bb4c06e",
      "b7c73179415041f1b28e7c5c7b60793a",
      "3fc8ab3462804b74b3771124fe92fd5c",
      "dd3eb5396b444b6ca5c8e927162a8d8c",
      "6d2a711ef2d640109d2c45579f3773d2",
      "f0abe6bb0faf42c0b713a3f680198e2d",
      "28979a1944f644948356f0f898a4fd87",
      "a1dbe02d633d479684f5315266abbe64",
      "fb253623d6fb48858a9ba123924cc45a",
      "a29de1fead2b4f9788f721b5dadc6496",
      "f80e1b0d000545eb8483226ec60f9d15",
      "28f5c265f7554247b0ff267c072e84c3",
      "068a7ad0f7034d8690d28c5df22e6627",
      "3a283a580ac6428992eaa2244e5b5179",
      "892f837c4ab34c9fbe71c4876ed1b1b2",
      "97334c93293d49409aac40859a74b38e",
      "fc6bd1fe60ec422b8a8bfcc5768362a0"
     ]
    },
    "id": "vziwd2UuCYGl",
    "outputId": "ad66b4f8-cdec-4222-be18-0ad4a650640c"
   },
   "outputs": [],
   "source": [
    "'''This imports two classes from the transformers library by Hugging Face:\n",
    "AutoModelForCausalLM: A generic class to load causal language models (used for text generation).\n",
    "AutoTokenizer: Automatically loads the appropriate tokenizer for the chosen model. Tokenizers are responsible for converting text into tokens that the model can understand.'''\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "'''Sets the model_name variable to \"bigscience/bloomz-560m\", which is a pre-trained language model from the BigScience project.\n",
    "It has 560 million parameters. The second line is commented out, but if used, it would switch to a larger version of the model (1.1 billion parameters).'''\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "#model_name=\"bigscience/bloom-1b1\"\n",
    "'''Downloads and loads the tokenizer associated with the given model name from Hugging Faceâ€™s model hub.'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "'''Downloads and loads the pre-trained causal language model specified by model_name.'''\n",
    "foundation_model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qtc1gbK39Hp7"
   },
   "source": [
    "## Inference with the pre-trained model.\n",
    "I'm going to do a test with the pre-trained model without fine-tuning, to see if something changes after the fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jak6FzpvFTHk"
   },
   "outputs": [],
   "source": [
    "#this function returns the outputs from the model received, and inputs.\n",
    "'''model: The language model (e.g. foundation_model from earlier).\n",
    "inputs: A dictionary (from the tokenizer) containing at least input_ids and attention_mask.\n",
    "max_new_tokens: Optional argument to set how many new tokens to generate. Default is 100.'''\n",
    "def get_outputs(model, inputs, max_new_tokens=100):\n",
    "    '''Calls the .generate() method from the Hugging Face transformers library, which generates text from the model.\n",
    "input_ids: The tokenized input text (e.g. prompt).\n",
    "attention_mask: Tells the model which tokens to pay attention to (1) and which to ignore (0).\n",
    "max_new_tokens: The maximum number of new tokens to generate in addition to the input.\n",
    "repetition_penalty=1.5: Helps reduce repetition in the generated text. A value >1 discourages the model from repeating the same text.\n",
    "early_stopping=True: The generation will stop early if an end-of-sequence token is generated or the model sees fit.\n",
    "eos_token_id: Tells the model what token represents \"end of sequence\" so it knows when to stop generating.'''\n",
    "        outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.5, #Avoid repetition.\n",
    "        early_stopping=True, #The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkFqjS459jAa"
   },
   "source": [
    "The dataset used for the fine-tuning contains prompts to be used with Large Language Models.\n",
    "\n",
    "I'm going to request the pre-trained model that acts like a motivational coach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BAYg7czFYeK",
    "outputId": "70ea487e-b8f0-46b0-a5b8-02f549e37406"
   },
   "outputs": [],
   "source": [
    "#Inference original model\n",
    "'''Tokenizes the input string using the previously loaded tokenizer.\n",
    "return_tensors=\"pt\" means the tokenizer returns PyTorch tensors, which are required for model input.'''\n",
    "input_sentences = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\")\n",
    "'''Calls your previously defined get_outputs() function.\n",
    "Uses the foundation_model to generate a response of up to 50 new tokens.\n",
    "The output is a tensor containing the full token sequence (original prompt + generated continuation).'''\n",
    "foundational_outputs_sentence = get_outputs(foundation_model, input_sentences, max_new_tokens=50)\n",
    "'''Decodes the output tokens back into human-readable text.\n",
    "batch_decode() is used instead of decode() because the model returns a batch (even if it's just one item).\n",
    "skip_special_tokens=True removes any special tokens like <pad> or <eos> that might be present.'''\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQUGY47p9ysI"
   },
   "source": [
    "Not sure if the answer is correct or not, but for sure is not a prompt. We need to train our model if we want that acts like a prompt engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FL5L_DcR9ggA"
   },
   "source": [
    "# Preparing the Dataset.\n",
    "The Dataset used is:\n",
    "\n",
    "https://huggingface.co/datasets/fka/awesome-chatgpt-prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "91e11b399afb4c398290b2ca6a44ce59",
      "819d92536d2a49068bb4f48c193ee3d1",
      "0e6314f1ba664817a6a78bac084a2bf7",
      "793dded818fb4bac8b463fd6bbac9881",
      "255adb88a55e434ead25f9a0d527297e",
      "07be0c76abe94ef685485270b000ae45",
      "b21822448df44256aff528c03eb06879",
      "36bfe3f3d9e440609503fda07bf0a213",
      "7a85d78cc7764dba94385449b491a59a",
      "cb2dcb2ef0d74f9f81c94380717fe411",
      "417f4d606d08433f9d3d0271658959e7",
      "75aac4dcf4904c2cab9a878ec27f6825",
      "98fc085bba554d9e865a99f16e399d81",
      "fdfe211f45184be2bea81cba216c8177",
      "faf6e3300fe640b7bb36ec431e30e6aa",
      "a1ce6df81c3b4dc38c227a1de582c337",
      "22b21158aaa74fdcb860bf4bf1471415",
      "a7e46b9a06df4ee19903718b8decb688",
      "54b43269bcff4d17922c22960475c2bd",
      "a24633b5ab12496ea432ed8e9a7a511d",
      "e6b1843c25e24bf88b45502557427e5d",
      "54bf288b937d4d70b486aff2e7a15841",
      "44353433c3d243d098fb419925de198b",
      "d37a0e1979944e6f8e0a1891de9be642",
      "3e59de2a068c4717a94bc74652c5d445",
      "d55ffebab63b4e6f87c3d4dc90ac7e86",
      "b3763405979c47578459ad025b8de984",
      "999dc8fdb5c84b58bb144e654e17e484",
      "13dff0c2236546ac9e96d432adbc0065",
      "4dbf4ca675e640cf8f9ddd9f4a076c6c",
      "a453220532eb4d0eae45777ff8ef85d0",
      "f6a9c112d1224a7db59084deb13f79ca",
      "7ed4435622cf426f85945131a91079d9",
      "d841736bc2f64fc99614d7741cda69f3",
      "29a3ac626fec498b940b2f0790f5f105",
      "a91909e2bc2f41b7bba63da48700dbb6",
      "8f099eb7f29b43af891058be731f3d43",
      "e3f83fec084846c09ec67cbd464210ac",
      "f54840d5c8774b9182886e0627fd2c9f",
      "bc5563140cd742f09702b58a89f87c36",
      "ba6d6d75b7834de29b3ad76907a93066",
      "6a2c96d604a148e6af9844449d954329",
      "9ce8d876f0aa4683a334443484accd28",
      "523876dcefec495fb8649ced3aadcc65"
     ]
    },
    "id": "DyIMQ7IHFbIx",
    "outputId": "41859cca-4e9a-45c4-ad3f-38811f585d32"
   },
   "outputs": [],
   "source": [
    "'''This imports the load_dataset function from the Hugging Face datasets library.\n",
    "datasets is used to easily load, manipulate, and preprocess large datasets for NLP.'''\n",
    "from datasets import load_dataset\n",
    "'''This is a Hugging Face-hosted dataset named \"fka/awesome-chatgpt-prompts\" â€” a community-curated collection of interesting ChatGPT prompt examples'''\n",
    "dataset = \"fka/awesome-chatgpt-prompts\"\n",
    "\n",
    "#Create the Dataset to create prompts.\n",
    "'''Downloads and loads the dataset.\n",
    "\n",
    "The resulting data object is a dictionary-like structure with splits, typically including:\n",
    "data[\"train\"] â€” the main portion of the dataset.\n",
    "ðŸ§  Each example in this dataset typically contains fields like:\n",
    "act: Describes the kind of task or role (e.g., \"motivational coach\").\n",
    "prompt: The actual prompt to give to the language model.'''\n",
    "data = load_dataset(dataset)\n",
    "'''Applies the tokenizer to the \"prompt\" field across the dataset.\n",
    "batched=True ensures the tokenizer processes batches of samples (faster and more efficient).\n",
    "Adds new columns to each dataset example: input_ids, attention_mask, etc.'''\n",
    "data = data.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "'''Selects the first 50 examples from the train split.\n",
    "Useful for:Reducing computation during testing or prototyping\n",
    "Quick sanity checks'''\n",
    "train_sample = data[\"train\"].select(range(50))\n",
    "'''Removes the act column from each example.\n",
    "That column describes the persona or role (e.g., \"motivational coach\") but may not be needed if you're only working with raw prompts.'''\n",
    "train_sample = train_sample.remove_columns('act')\n",
    "'''Displays the processed train_sample dataset in a notebook interface (e.g., Jupyter or Google Colab).\n",
    "Shows the first few examples, which should now include:prompt, input_ids, attention_mask'''\n",
    "display(train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gmlZY3fk_9fm",
    "outputId": "63354560-4cef-48c2-bdec-11594d5fe162"
   },
   "outputs": [],
   "source": [
    "print(train_sample[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVPAJsrUAHiJ"
   },
   "source": [
    "# Fine-Tuning.\n",
    "First is necesary create a LoRA config.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwAK6kxCDCfM"
   },
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCalslQFGL7K"
   },
   "outputs": [],
   "source": [
    "# TARGET_MODULES\n",
    "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220\n",
    "'''Imports the PEFT (Parameter-Efficient Fine-Tuning) library, which integrates with Hugging Face Transformers.\n",
    "LoraConfig: Used to define the LoRA configuration. get_peft_model: Wraps a pretrained model with LoRA. PeftModel: The resulting model class after LoRA adaptation.'''\n",
    "import peft\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "'''| Parameter                            | Description                                                                                                                            |\n",
    "| ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `r=4`                                | **Rank** of the low-rank decomposition. Higher = more trainable parameters.                                                            |\n",
    "| `lora_alpha=1`                       | **Scaling factor**: Scales the LoRA weights. Often 1, but increasing can improve training if `r` is small.                             |\n",
    "| `target_modules=[\"query_key_value\"]` | Specifies which submodules in the model to inject LoRA layers into. `\"query_key_value\"` is often used in transformer attention layers. |\n",
    "| `lora_dropout=0.05`                  | Dropout applied **within LoRA layers** during training. Helps prevent overfitting.                                                     |\n",
    "| `bias=\"lora_only\"`                   | Only trains bias parameters that are **directly related to LoRA layers** (not all biases).                                             |\n",
    "| `task_type=\"CAUSAL_LM\"`              | Specifies the task: **Causal Language Modeling** (like GPT-style next-token prediction).                                               |\n",
    " '''\n",
    "lora_config = LoraConfig(\n",
    "    r=4, #As bigger the R bigger the parameters to train.\n",
    "    lora_alpha=1, # a scaling factor that adjusts the magnitude of the weight matrix. Usually set to 1\n",
    "    target_modules=[\"query_key_value\"], #You can obtain a list of target modules in the URL above.\n",
    "    lora_dropout=0.05, #Helps to avoid Overfitting.\n",
    "    bias=\"lora_only\", # this specifies if the bias parameter should be trained.\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUddynl0B1Ck"
   },
   "source": [
    "The most important parameter is **r**, it defines how many parameters will be trained. As bigger the valuer more parameters are trained, but it means that the model will be able to learn more complicated relations between input and output.\n",
    "\n",
    "Yo can find a list of the **target_modules** available on the [Hugging Face Documentation]( https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py#L220)\n",
    "\n",
    "**lora_dropout** is like the commom dropout is used to avoid overfitting.\n",
    "\n",
    "**bias** I was hesitating if use *none* or *lora_only*. For text classification the most common value is none, and for chat or question answering, *all* or *lora_only*.\n",
    "\n",
    "**task_type**. Indicates the task the model is beign trained for. In this case, text generation.\n",
    "\n",
    "### Create the PEFT model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BG1zBmhsGQ-h",
    "outputId": "092ac0fe-a6b4-4556-9e99-aca298e9907e"
   },
   "outputs": [],
   "source": [
    "'''This wraps your original foundation_model (e.g., BLOOMZ-560M) with LoRA adapters using the lora_config you defined earlier.\n",
    "The result is a new model (peft_model) where: Most of the original modelâ€™s parameters are frozen (not trainable).\n",
    "Only small LoRA modules inside query_key_value layers (and optionally some bias terms) are trainable.'''\n",
    "peft_model = get_peft_model(foundation_model, lora_config)\n",
    "print(peft_model.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2onXUsaw-Ga0"
   },
   "source": [
    "The number of trainable parameters is really small compared with the total number of parameters in the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HArPQ_lvGUkY"
   },
   "outputs": [],
   "source": [
    "#Create a directory to contain the Model\n",
    "import os\n",
    "working_dir = './'\n",
    "\n",
    "output_directory = os.path.join(working_dir, \"peft_lab_outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rWalmqWm4STo"
   },
   "source": [
    "In the TrainingArgs we inform the number of epochs we want to train, the output directory and the learning_rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ND0aJ-t6ARqD"
   },
   "outputs": [],
   "source": [
    "#Creating the TrainingArgs\n",
    "'''Brings in the Trainer API from the Hugging Face Transformers library, which simplifies model training.\n",
    "TrainingArguments is used to configure the training process (output paths, learning rate, batch size, etc.).'''\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "'''| Parameter                     | Description                                                                                                                                   |\n",
    "| ----------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `output_dir=output_directory` | Directory to save model checkpoints and logs. Make sure `output_directory` is defined beforehand (e.g., as a string like `\"./lora_outputs\"`). |\n",
    "| `auto_find_batch_size=True`   | Automatically finds the **largest batch size** that fits in memory. Very helpful if you're unsure what the hardware can handle.               |\n",
    "| `learning_rate=3e-2`          | This is **0.03**, which is relatively high â€” but thatâ€™s okay for LoRA, since you're only training a small number of parameters.               |\n",
    "| `num_train_epochs=2`          | The number of times to iterate over the training dataset.                                                                                     |\n",
    "| `use_cpu=True`                | Forces training on CPU instead of GPU. This is fine for prototyping or small models, but **slow** for larger ones.                            |\n",
    " '''\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_directory,\n",
    "    auto_find_batch_size=True, # Find a correct bvatch size that fits the size of Data.\n",
    "    learning_rate= 3e-2, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=2,\n",
    "    #use_cpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgxsV-iy_J_o"
   },
   "source": [
    "Now we can train the model.\n",
    "To train the model we need:\n",
    "\n",
    "\n",
    "*   The PEFT Model.\n",
    "*   The training_args\n",
    "* The Dataset\n",
    "* The result of DataCollator, the Dataset ready to be procesed in blocks.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "z5NYHqBnGZyF",
    "outputId": "aa381635-2591-4186-b18d-753e737723be"
   },
   "outputs": [],
   "source": [
    "#This cell may take up to 15 minutes to execute.\n",
    "'''| Parameter                    | Purpose                                                                                                                                                                                                                                                                                          |\n",
    "| ---------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |\n",
    "| `model=peft_model`           | Uses the **LoRA-wrapped model** (from `get_peft_model`) for training.                                                                                                                                                                                                                            |\n",
    "| `args=training_args`         | Passes in your previously defined `TrainingArguments`.                                                                                                                                                                                                                                           |\n",
    "| `train_dataset=train_sample` | The training dataset â€” in your case, the **first 50 tokenized prompts** from the ChatGPT prompt dataset.                                                                                                                                                                                         |\n",
    "| `data_collator=...`          | This dynamically pads the inputs in a batch so they are of equal length, which is required by PyTorch. <br> You're using `DataCollatorForLanguageModeling` with `mlm=False`, meaning you're training with **causal language modeling** (like GPT), **not masked language modeling** (like BERT). |\n",
    " '''\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_sample,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEKiFdpDGgOx"
   },
   "outputs": [],
   "source": [
    "#Save the model.\n",
    "'''Creates a path to save the model by combining output_directory (where you store outputs/checkpoints) with a subfolder name like \"lora_model\".'''\n",
    "peft_model_path = os.path.join(output_directory, f\"lora_model\")\n",
    "'''Saves the LoRA-adapted model weights (not the full base model) to the specified directory.\n",
    "This includes:LoRA adapter weights, Config files (adapter_config.json, adapter_model.bin), Metadata for later loading'''\n",
    "trainer.model.save_pretrained(peft_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_TAjrSWSe14q"
   },
   "outputs": [],
   "source": [
    "#Load the Model.\n",
    "'''| Parameter            | Description                                                                                                                                                                                     |\n",
    "| -------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `foundation_model`   | This is your original base model (e.g., `bloomz-560m`). It's being re-wrapped with the LoRA adapter weights.                                                                                    |\n",
    "| `peft_model_path`    | The path to the folder where your LoRA adapter was saved (contains `adapter_config.json`, `adapter_model.bin`, etc.).                                                                           |\n",
    "| `is_trainable=False` | Indicates that the model is **loaded in inference mode**, so youâ€™re not planning to fine-tune further. LoRA weights will be frozen. <br> âœ… Useful when you're doing generation/evaluation only. |\n",
    " '''\n",
    "loaded_model = PeftModel.from_pretrained(foundation_model,\n",
    "                                        peft_model_path,\n",
    "                                        is_trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dK--YFPR6OxH"
   },
   "source": [
    "## Inference the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I_27uvJudf03",
    "outputId": "5e7ee680-a30c-4d07-e6b1-b8f187198850"
   },
   "outputs": [],
   "source": [
    "'''Converts your prompt string into token IDs and attention masks in PyTorch tensor format. This will be the input for the model.'''\n",
    "input_sentences = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\")\n",
    "'''Calls your earlier-defined get_outputs function. Uses the LoRA-adapted model (loaded_model) to generate up to 50 new tokens.\n",
    "Generates continuation text based on the prompt.'''\n",
    "foundational_outputs_sentence = get_outputs(loaded_model, input_sentences, max_new_tokens=50)\n",
    "'''Decodes the generated tokens back into text. skip_special_tokens=True removes tokens like <eos>, <pad>, etc.\n",
    "Prints the final output as a list of strings (usually one string for a single input).'''\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tr4Sm32y89Ji"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNBihjGx70DtC3db82s+h3x",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
