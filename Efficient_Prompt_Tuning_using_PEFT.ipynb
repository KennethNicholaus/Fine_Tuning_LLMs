{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates how to perform **prompt tuning** on the BLOOMZ-560M language model using PEFT (Parameter-Efficient Fine-Tuning). It loads two datasets‚Äîone with prompts and one with quotes‚Äîthen adds and trains a small set of virtual tokens to adapt the model efficiently for each task. Training is done using Hugging Face‚Äôs `Trainer`, and the tuned adapters are saved separately for easy loading and text generation later, enabling lightweight customization without updating the full model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEFT Prompt Tuning with BLOOMZ-560M: Summary\n",
    "\n",
    "- **Set up model and tokenizer**\n",
    "  - Load pretrained BLOOMZ model and tokenizer from Hugging Face.\n",
    "  - Choose model variant (e.g., `\"bigscience/bloomz-560m\"`).\n",
    "\n",
    "- **Define constants**\n",
    "  - Number of virtual tokens (`NUM_VIRTUAL_TOKENS = 4`).\n",
    "  - Number of training epochs (`NUM_EPOCHS = 5`).\n",
    "\n",
    "- **Helper function to generate outputs**\n",
    "  - Wrap model `.generate()` with custom parameters (e.g., repetition penalty).\n",
    "\n",
    "- **Load and tokenize datasets**\n",
    "  - Load prompt dataset (`fka/awesome-chatgpt-prompts`), tokenize and select subset.\n",
    "  - Load sentence dataset (`Abirate/english_quotes`), tokenize and select subset.\n",
    "\n",
    "- **Configure PEFT prompt tuning**\n",
    "  - Create `PromptTuningConfig` with random initialization and virtual tokens.\n",
    "  - Wrap foundation model with PEFT for prompt tuning (for both prompt and sentence datasets).\n",
    "\n",
    "- **Prepare training arguments**\n",
    "  - Define `TrainingArguments` tailored for CPU, auto batch sizing, higher LR, and output dirs.\n",
    "\n",
    "- **Set up output directories**\n",
    "  - Create separate folders to save prompt-tuned and sentence-tuned PEFT adapters.\n",
    "\n",
    "- **Create Hugging Face `Trainer` instances**\n",
    "  - Use `DataCollatorForLanguageModeling` (causal LM mode).\n",
    "  - Build trainers for prompt and sentence PEFT models.\n",
    "\n",
    "- **Train models**\n",
    "  - Run `.train()` on both trainers.\n",
    "\n",
    "- **Save tuned adapters**\n",
    "  - Save PEFT adapters separately to their output directories.\n",
    "\n",
    "- **Load tuned adapters for inference**\n",
    "  - Load base BLOOMZ model.\n",
    "  - Load PEFT adapters with `PeftModel.from_pretrained()` on base model.\n",
    "  - Generate and decode text outputs for evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "This workflow enables efficient fine-tuning of large language models by training a small set of virtual tokens (prompt tuning), reducing compute and storage costs while customizing generation behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d16bf5ec-888b-4c76-a655-193fd4cc8a36",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JechhJhhVaTA",
    "outputId": "c00b0a5a-d9ef-43d9-c51f-09a23a26a569"
   },
   "outputs": [],
   "source": [
    "!pip install -q peft==0.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6CRxq5Z2WJ7C",
    "outputId": "78ee1044-003c-4ea0-9911-63182ce43d2d"
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets==2.14.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGbh426RVaTB"
   },
   "source": [
    "From the transformers library, we import the necessary classes to instantiate the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31738463-c9b0-431d-869e-1735e1e2f5c7",
     "showTitle": false,
     "title": ""
    },
    "id": "KWOEt-yOVaTB"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qYsnwjSVaTC"
   },
   "source": [
    "### Loading the model and the tokenizers.\n",
    "\n",
    "Bloom is one of the smallest and smartest models available for training with the PEFT Library using Prompt Tuning. You can choose any model from the Bloom Family, and I encourage you to try at least two of them to observe the differences.\n",
    "\n",
    "I'm opting for the smallest one to minimize training time and avoid memory issues in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MnqIhv2UVaTC"
   },
   "outputs": [],
   "source": [
    "'''This line is assigning a pretrained language model name from Hugging Face's Model Hub to the variable model_name.\n",
    "\"bigscience/bloomz-560m\" refers to the BLOOMZ-560M model, a version of the BLOOMZ series developed by BigScience. The 560m refers to the size of the model, which has 560 million parameters.\n",
    "The second line is commented out (#). It shows an alternative model: \"bigscience/bloom-1b1\" (BLOOM with 1.1 billion parameters), but it's not currently active.\n",
    "These models are part of the BLOOM or BLOOMZ family, which are multilingual and open-source large language models.'''\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "#model_name=\"bigscience/bloom-1b1\"\n",
    "'''This likely refers to soft prompt tuning or prefix tuning, a parameter-efficient way to fine-tune large models.\n",
    "NUM_VIRTUAL_TOKENS = 4 means that the fine-tuning method will prepend 4 virtual tokens to the input.\n",
    "These tokens are learned embeddings, not actual words, and are optimized during training to guide the model's behavior on downstream tasks.'''\n",
    "NUM_VIRTUAL_TOKENS = 4\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "6bc0e15d5aa3443fa44521a32ca4e35a",
      "1f64826fb6224909892c82971ae70fed",
      "314f824147874bf7aaecb8f4d26cc0e1",
      "34fe192875c34406a61073020a33be5d",
      "0ddeada4f5ae4dc7af7c5fcc2ffe9e04",
      "b45514507c2d4459b3053ee4bac6b750",
      "315f5088aec0465cb5d28146b1ea0ba7",
      "903da47c2dba4317867df9a061284224",
      "a7c3b0d5f3dc462f9f7aad680b65812c",
      "8b610c67429e4e03bf257557d0fa1122",
      "090ab3b5b7fa43129095b97913eff590",
      "3e9b75cef0734759af4cd1e37a66c066",
      "02a14ade81df43d8af50ef04f1f81286",
      "10310d3f0e2345ed8d1742f4194e68c7",
      "8866b41af5ae4e37be0bc5f98df32f3f",
      "c8256a3bb93b440498067e5323119d5a",
      "96e08a5d2a3549b4a0282046453af60c",
      "1f0f299c285149c1a8f86fb8edf6e590",
      "96b504e618094adeb1b098acbcea9999",
      "c2f010b915e8471a820ffda9969f0f38",
      "52c1750e4bf14d1a9db3c5ccf1a6c441",
      "16c8dd64352847dfae0fca56bf2f518a",
      "108461b576d548359a8b2b3476d57e37",
      "91ef1a0c576e45ae90281370ca3e7bae",
      "464fa564480144308feb0bf316cdd820",
      "07695b4b6253462cbef6baa093903dd3",
      "332b92bada0243c18c9a342800734be9",
      "ec4317990b7c4d7fb8db0c6cb7d363b5",
      "f80511cbf04848e799c01b732dbe4278",
      "eb4e9076b3d848d78bc1a135a79dc8b4",
      "ab98af57131147dc98eb8de06821098f",
      "7843b818cff048268e7ebec0f8d88ec6",
      "1e93f8fc93954ea89ebb50152f931aac",
      "ff4b4c19de5b40b28a99db6593363bd1",
      "c7ff6ebbb7b6435ab4f78413310bb7ea",
      "96a53d44e208431492cb4b7a69ec3eaa",
      "6bb7bbf385c74f64add0104197ed69e5",
      "314a5ef30ee447f1a974c3452d075dcc",
      "477545b6bfae47899447718e10fccbf7",
      "2545be18d5bb46ab910003183e77be97",
      "c734eb4d993049d5b7d6b78d5ce4e67b",
      "52b5194f2aad41f4b77d41dd7f6c0464",
      "35e27e2bfada49628699d86392b9cd1e",
      "bc1d411fdd2d4777b2a7c12890b762e4",
      "81a546ba66d54b328cec5aeb9f1738fc",
      "073512159297449cb30849d2b109de05",
      "68cfcf45daf8496cbb73c83ef1d64983",
      "c1e71aad0a12470bad60f9b1ac62b50b",
      "aacfaa54f4164ca9938638a47522548e",
      "9095e111a6f54fb0ade8850c29523df5",
      "5055ebf8630a416f87eb460b7ac5d32a",
      "a6ae17ae16534bb6988a3c5a21597f49",
      "05acc46f5f8848719096e136307d72a1",
      "d034e577373643499e9743ed4dec9ed7",
      "e75efca9d33945f9acc7262ef818cf6d"
     ]
    },
    "id": "fSMu3qRsVaTC",
    "outputId": "520da7df-a7fd-4be1-b69b-d5a315418c26"
   },
   "outputs": [],
   "source": [
    "'''Loads the tokenizer associated with the model.\n",
    "AutoTokenizer is a generic class that automatically picks the right tokenizer class based on the model name.\n",
    "from_pretrained(model_name) tells it to load the tokenizer files for \"bigscience/bloomz-560m\" from the Hugging Face hub.\n",
    "The tokenizer is responsible for: Splitting text into tokens (tokenization). Mapping those tokens to numerical IDs. Decoding IDs back into human-readable text.'''\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "'''Loads the pretrained causal language model (BLOOMZ-560M in this case).\n",
    "\n",
    "AutoModelForCausalLM is a class that loads the correct model type for causal language modeling ‚Äî which is the type used for tasks like: Text generation, Code completion, Dialogue modeling\n",
    "from_pretrained(model_name) again fetches the model weights from Hugging Face.\n",
    "trust_remote_code=True allows the loading of custom model code from the repository.\n",
    "Some models include their own modeling_*.py files. This is necessary if the model repo defines custom behavior beyond the base transformer models.'''\n",
    "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8W2fWhOnVaTC"
   },
   "source": [
    "## Inference with the pre trained bloom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47j2D3WWVaTC"
   },
   "outputs": [],
   "source": [
    "#this function returns the outputs from the model received, and inputs.\n",
    "'''This function get_outputs() is designed to generate text outputs from a language model (like BLOOMZ) using the Hugging Face Transformers API.\n",
    "input_ids=inputs[\"input_ids\"]: These are the token IDs for your input prompt.\n",
    "attention_mask=inputs[\"attention_mask\"]: Specifies which tokens should be attended to (1 = real token, 0 = padding).\n",
    "max_new_tokens=...: Caps how many tokens the model will generate beyond the prompt.\n",
    "repetition_penalty=1.5: Discourages the model from repeating itself. Values > 1 penalize repetition; 1.5 is a moderately strong penalty.\n",
    "early_stopping=True: Allows generation to stop before hitting max_new_tokens if an end-of-sequence (eos_token_id) is generated.\n",
    "eos_token_id=tokenizer.eos_token_id: This tells the model what token signals the end of a response. It uses the correct one from the tokenizer.'''\n",
    "def get_outputs(model, inputs, max_new_tokens=100):\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        repetition_penalty=1.5, #Avoid repetition.\n",
    "        early_stopping=True, #The model can stop before reach the max_length\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return outputs\n",
    "'''Returns the generated token IDs from the model. These will need to be decoded back to text using the tokenizer, like this:'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca4d203a-5152-4947-ab34-cfd0b40a102a",
     "showTitle": false,
     "title": ""
    },
    "id": "kRLSfuo2VaTC"
   },
   "source": [
    "As we want to have two different trained models, I will create two distinct prompts.\n",
    "\n",
    "The first model will be trained with a dataset containing prompts, and the second one with a dataset of motivational sentences.\n",
    "\n",
    "The first model will receive the prompt \"I want you to act as an English translator,\" and the second model will receive \"There are two things that matter:\"\n",
    "\n",
    "But first, I'm going to collect some results from the model without fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d4c80a9-4edd-4fcd-aef0-996f4da5cc02",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QvStaT7cVaTC",
    "outputId": "1b36791d-232a-4106-aa33-6c5ad9eab4de"
   },
   "outputs": [],
   "source": [
    "'''Purpose: Tokenizes your prompt using the model's tokenizer. \"I want you to act as a motivational coach.\" is the seed prompt.\n",
    "return_tensors=\"pt\" returns the input as PyTorch tensors, which is what the model expects.'''\n",
    "input_prompt = tokenizer(\"I want you to act as a motivational coach. \", return_tensors=\"pt\")\n",
    "'''Generates text from the model starting with the prompt. The model will generate up to 50 new tokens, unless it hits an eos_token_id and stops early.\n",
    "It uses all the settings you defined in get_outputs() (repetition penalty, early stopping, etc.).'''\n",
    "foundational_outputs_prompt = get_outputs(foundational_model, input_prompt, max_new_tokens=50)\n",
    "'''Decodes the list of token IDs into human-readable strings. skip_special_tokens=True removes things like <pad>, <eos>, etc., from the output.\n",
    "batch_decode is used even though it's likely a batch of size 1 ‚Äî it's still appropriate because the model outputs a tensor of shape [batch_size, sequence_length].'''\n",
    "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Xhm3jZMVaTD",
    "outputId": "7ea55d99-bdb5-42c6-dd18-33397b7a1800"
   },
   "outputs": [],
   "source": [
    "input_sentences = tokenizer(\"There are two things that matter:\", return_tensors=\"pt\")\n",
    "foundational_outputs_sentence = get_outputs(foundational_model, input_sentences, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.batch_decode(foundational_outputs_sentence, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f438d43b-6b9f-445e-9df4-60ea09640764",
     "showTitle": false,
     "title": ""
    },
    "id": "OGbJTbRnVaTD"
   },
   "source": [
    "Both answers are more or less correct. Any of the Bloom models is pre-trained and can generate sentences accurately and sensibly. Let's see if, after training, the responses are either equal or more accurately generated.\n",
    "\n",
    "## Preparing the Datasets\n",
    "The Datasets useds are:\n",
    "* https://huggingface.co/datasets/fka/awesome-chatgpt-prompts\n",
    "* https://huggingface.co/datasets/Abirate/english_quotes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RD8H_LLaVaTD"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ed62b41-e3fa-4a41-a0a9-59f35a6904f9",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278,
     "referenced_widgets": [
      "64d07933fedd4c66be1b9d5f12ee473a",
      "4134750ceaac4f93ba17cf1b8e75f00f",
      "632f0a4d11924f929f9530d25b7473d5",
      "05c874292e5647ce9386615dfafb2701",
      "2f1015073a8e4016a50cfa84e1bc236a",
      "8c662dc6d7824b8ba63c0c0b1e1d8050",
      "5f5122eebc664379992996f5bdbfc4c7",
      "83df811ee3a245da835ec395c2465509",
      "59826c220c094d5ca35e464c8b15a64e",
      "0a4f6826087a4a4fb01a3cb204603c01",
      "4778adf21e494dd5bc99959c03f726e3",
      "bc4b11633c5344e4ab04083b45404ec2",
      "b5a94e4d25b14f64b07ce4b70b978b7b",
      "cedf7764df5442acbc23666db291b80c",
      "3356ce2c4cfe4ea88c9ddc2f48fb9a97",
      "573f51472d25493e82c491266d1215be",
      "4a784498c27846ecbcd076fb6460bcd4",
      "17e7817ca8474fedbff67aa2837108b7",
      "db8302796d554686a2c23f5f6203ab67",
      "ff4c711a7855407b93ad90095def36e6",
      "2304846e159c4b1185f1978b758295b8",
      "654d59fec3ef4ac9b5043cea1b8ac409",
      "e328a2daa276448fa68e3886d0d80b2d",
      "dd29aa8d22d74125848e5fd72791e5a1",
      "c5384b5808af435fadd9f35bb4aa1dcc",
      "db592adc0fbc43b69852cb4e3134202b",
      "e9d802a601e7416a910ce7b07ffbc3f7",
      "c898db5ff41d4690b269c162334a5a40",
      "a04c5c513f3644e7b65b99949d21d93a",
      "a5b6f7d74b7a44edb0ef8ec38e76f53e",
      "8da916b61eb94f8db494d0299e061f6a",
      "33a62b38b6d3484fa20fd7a2e3345185",
      "306d9451f939423aa98df4d1b83bd111",
      "fb0d2c94d4d14d34b1f8d134933e7589",
      "d79d17ac86eb44a58b7f9616b3933194",
      "7d1d41d883044a4aa8a09c745a4238be",
      "e402e7e6df8648878e733ef83c778737",
      "6593a151c1d74b0ebdbedc6ccbcf6e5a",
      "494862c46bcd4fb7961a8e90f056dfaa",
      "2b90959753964957b4a26da36f78f4de",
      "4ee54f5bc4cf417b96af98fb4bf24412",
      "533dc3e524484602bdbf9f639f7f5cbb",
      "2b534aead9714e09b1744043b0975b33",
      "b0a3daaeffd14decb4bc1620fe1126ff",
      "01b146e0e4464a2eab73322d1ed0045d",
      "a0671d80da2e4bbfb26e61dc648efa92",
      "2dde589177c3411494f0af8d99e6ded4",
      "e9db42fcbae440f8a2133de44eff32e2",
      "07f76d38a8b04b14a39efc91609d2123",
      "1480ac2ed1c9465b95a4b3a4caecf889",
      "2b53c9c5b39449e7accb794f2d8f75b8",
      "75f98e9cb8744d71a7dcb0394888ee8b",
      "39fa9ef0abf24ef4836b63305ded8466",
      "1a76dfc6b902432d977ac19b67ef9a35",
      "34ef02cd22e54a8eab572959ef4b5a1e",
      "11035e43f0b442d9ab68acd89f9118d3",
      "1634859977c34322b5a88d62e48eae01",
      "f1acb17fbf1e48fca5c9169faa591cf4",
      "dc07d32bdf4046178afdd1371a996afc",
      "6436fdbfc38946a89ff06df13f42a8ab",
      "55e0a778e3c64bec99bdfc564fbbce90",
      "bf6e0c611752431294f53de6c900bd89",
      "ec23ab78757d45c7afa064b8e6289cb1",
      "0e583e176a85488caf514efeda3947f9",
      "9bf8ae19b7e2458598efcc651708f38e",
      "689f2c694fcc40318e99022e0971f679"
     ]
    },
    "id": "xmAp_o4PVaTD",
    "outputId": "c19dff6e-ac7b-463c-d083-a23c268813e0"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "'''Loads the Hugging Face dataset called fka/awesome-chatgpt-prompts. This dataset contains a collection of structured prompts, often with an act and prompt field \n",
    "(e.g., \"Act as a doctor\", \"Act as a motivational coach\", etc.).'''\n",
    "\n",
    "dataset_prompt = \"fka/awesome-chatgpt-prompts\"\n",
    "\n",
    "#Create the Dataset to create prompts.\n",
    "data_prompt = load_dataset(dataset_prompt)\n",
    "'''Applies your tokenizer to the \"prompt\" field in a batched manner. This replaces each sample with the tokenizer's output (which includes 'input_ids', 'attention_mask', etc.).\n",
    "‚ö†Ô∏è Note: This operation will overwrite the existing structure of each sample with the tokenizer's return values unless you explicitly preserve the original columns.'''\n",
    "data_prompt = data_prompt.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "'''Selects the first 50 examples from the \"train\" split. This is useful for quick testing, development, or small-scale fine-tuning.'''\n",
    "train_sample_prompt = data_prompt[\"train\"].select(range(50))\n",
    "\n",
    "#train_sample_prompt = train_sample_prompt.remove_columns('act')\n",
    "\n",
    "display(train_sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZcOaE5CU658",
    "outputId": "5f53fed3-7e96-4928-e8ab-220083581503"
   },
   "outputs": [],
   "source": [
    "print(train_sample_prompt[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278,
     "referenced_widgets": [
      "f9b05a6bbf29483e99d8586569db4c6a",
      "c94cbe32efe54cbcb72d086c95efa2c5",
      "0ec30e9b9305488d90ae5c9c7d2e13bb",
      "b0279fbbee534244b662c4b281f059d3",
      "c7fd870954984092b642a8b6611a9836",
      "55cce89e90d849f8ab9e43c21b29fbfb",
      "14e9b44e7be843579d77d6eedf98aef2",
      "24d8ac945b3646a7bec0cbc1e0045a68",
      "e03abe276e67485088efdce14d5dcf06",
      "36883829debf45cab9ec7f509ac48f88",
      "66c0f654cb5a427198c7b93afc41a11a",
      "9c5ecf3cbc9d4fedbf908c28c94a6a0b",
      "15dd294610d9450a9379e88085e9ed34",
      "aafd5f70b53f403b9657eb708122c0d4",
      "f6976461a00549a3ba7a809b9147c6f6",
      "e154f8bb2bf64c2b9a77b0c2fbd45dc9",
      "d024b7dfeaeb44b88350533d20e262d7",
      "67e18cb27b3a4504ac46a1fcc378ee16",
      "e232f5b1b04f4a148b47e23e56b31c51",
      "365629c2808a408b8c25c5918e7e2aa6",
      "674223beeba54331bb3e4dbc31fd9602",
      "156b13bd7d8e48179bd41b9bbee72403",
      "eeab5a8ef5d94f018eed7b141422fa1f",
      "694dc0b5672b4c62a3c9655a2684d635",
      "bd42c1e5d81d4329bf347c36ba0b4aec",
      "cc44ee4c957f40ab88b9f063b687f7a1",
      "befc483048464831b9ce22bf1c93cc4c",
      "539ca30421ea40bf921285c2c26e08fa",
      "a14674865d6346beb59f4567cdc41c7d",
      "2a253edff18d4eed93d3d771333c1ac1",
      "a78c3b8580c0456cb896c18bacd8203d",
      "f1118217f495429c88b5b364c88ca8d8",
      "4eaa260b44d745e3a771d1652a0bdb50",
      "ddb556736c964019a7a60657baac9407",
      "710d8706e4dc4b3397a05f35ff4a0daf",
      "2e06491c627d414d858320b19c0bebe9",
      "4db199fd830a4249b6b39e0fa87aebe1",
      "21c0d6c38bee4385a39c11fc1c7d5282",
      "033ed003edc8412996ab5b3cb6bfeb2b",
      "634e5026246541d8b97e09238e3855db",
      "ea93589fc89043db8e6413785f89c5b9",
      "3ded5405bf1d452cb98f1616db7a8f96",
      "d60c1e09d70549fa89e75357f7372462",
      "7eb50b70d3cc4c41ab1ea362e3e086d7",
      "1fa1f64361764ea1a0469e93727a43e2",
      "5b80dc92e54f46559ec5d04fa4bdc4c3",
      "5c01e87d867e4b11b8635ef8bf5289f3",
      "e6c5e43d87624150a4b08e210f10c8d9",
      "1f523267e057432ba942eb6f97959e64",
      "a397f219ecb2462d8b57a5d55f1d694f",
      "32b8a5917a0c49faa720137c31ce3e19",
      "2d06ebe1badd4003855c89a9da602299",
      "ba4d71d93d2a4548afc10195e12d007d",
      "2da5e19ff10d458f9e12fffbb459db05",
      "bf90ce60c54b45b782f1c8871664c5bd",
      "049e9c70abd3437baa0c4def0dd7d6f4",
      "c10fd0d488954f3abc5e6d3c8616e5bf",
      "7d7e3458f23f420b82102ad40b7375ce",
      "8cfad56cb88a4445860e10073f34d520",
      "d1b9eb78c89e4a608fd38cf390b9002a",
      "6527496a9d9d41f38c10320e73657ea6",
      "6ae95e9c233a4dafb919dd104ef6c8e1",
      "ef737b7f6c33471e8c4cf8e572822a14",
      "b866b91b548e465f948f929bb6af0d60",
      "db9a09d18bfd4029afec582528620790",
      "32a0b1117d7c48d393036ce112370cb8"
     ]
    },
    "id": "WeM66LmEVaTD",
    "outputId": "f8bc65f6-5a95-4432-eedd-f3dfee2fa0f5"
   },
   "outputs": [],
   "source": [
    "'''Loads the public dataset Abirate/english_quotes. This dataset includes fields like: \"quote\" (the actual text), \"author\" (who said it),\n",
    "\"tags\" (topic or theme categories like \"inspiration\", \"life\", etc.)'''\n",
    "dataset_sentences = load_dataset(\"Abirate/english_quotes\")\n",
    "'''Applies the tokenizer to the \"quote\" field. batched=True allows efficient batch tokenization. This replaces each sample with the output of tokenizer(...) (e.g., input_ids, attention_mask)'''\n",
    "data_sentences = dataset_sentences.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "'''Selects the first 25 quotes for a smaller working dataset ‚Äî useful for testing or small-scale experiments.'''\n",
    "train_sample_sentences = data_sentences[\"train\"].select(range(25))\n",
    "'''Removes \"author\" and \"tags\" columns ‚Äî leaving just tokenized quote data.'''\n",
    "train_sample_sentences = train_sample_sentences.remove_columns(['author', 'tags'])\n",
    "display(train_sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b97381d4-5fe2-49d0-be5d-2fe3421edc5c",
     "showTitle": false,
     "title": ""
    },
    "id": "0-5mv1ZpVaTD"
   },
   "source": [
    "## fine-tuning.  \n",
    "\n",
    "### PEFT configurations\n",
    "\n",
    "\n",
    "API docs:\n",
    "https://huggingface.co/docs/peft/main/en/package_reference/tuners#peft.PromptTuningConfig\n",
    "\n",
    "We can use the same configuration for both models to be trained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6df8e1f1-be9e-42db-b4a4-6af7cd351004",
     "showTitle": false,
     "title": ""
    },
    "id": "sOg1Yh-oVaTD"
   },
   "outputs": [],
   "source": [
    "'''This library allows tuning only a small part of the model (like prompts, adapters, or LoRA layers), reducing memory and compute needs.'''\n",
    "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "\n",
    "'''| Parameter                                    | Purpose                                                                                                                                        |\n",
    "| -------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `task_type=TaskType.CAUSAL_LM`               | Specifies that this is a **Causal Language Modeling** task (like GPT, BLOOM, etc.), where the model predicts the next word/token.              |\n",
    "| `prompt_tuning_init=PromptTuningInit.RANDOM` | Initializes the virtual prompt tokens **randomly**. (Alternatives include using tokens from a real prompt or a known initialization.)          |\n",
    "| `num_virtual_tokens=NUM_VIRTUAL_TOKENS`      | The number of **learnable prompt tokens** (e.g., 4, as defined earlier). These will be prepended to every input during training and inference. |\n",
    "| `tokenizer_name_or_path=model_name`          | Used to align the virtual token embeddings with the tokenizer's vocabulary.                                                                    |\n",
    "'''\n",
    "generation_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
    "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
    "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "an9KBtB1VaTD"
   },
   "source": [
    "### Creating two Prompt Tuning Models.\n",
    "We will create two identical prompt tuning models using the same pre-trained model and the same config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c_D8oDQZVaTD",
    "outputId": "a4bf353d-db50-4d62-d167-c22835bcd3be"
   },
   "outputs": [],
   "source": [
    "'''This wraps your original foundational_model (e.g., BLOOMZ-560M) with a prompt tuning adapter. It doesn‚Äôt modify the pretrained model weights.\n",
    "Instead, it adds a set of virtual prompt embeddings that are trained while the rest of the model stays frozen.'''\n",
    "peft_model_prompt = get_peft_model(foundational_model, generation_config)\n",
    "print(peft_model_prompt.print_trainable_parameters())\n",
    "'''The exact numbers will vary depending on: NUM_VIRTUAL_TOKENS (you set this to 4).\n",
    "The hidden size of the model (for BLOOMZ-560M, it's 768).\n",
    "üìå How it's calculated: For prompt tuning, the trainable parameters = num_virtual_tokens √ó hidden_size √ó 2\n",
    "The √ó2 comes from learning both key and value prompt embeddings.\n",
    "So: 4 √ó 768 √ó 2 = 6144 trainable parameters (for BLOOMZ-560M).\n",
    "This is a tiny fraction of the total model size, which makes training faster, cheaper, and much more resource-efficient.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IktYfj68VaTE",
    "outputId": "e8b93e58-6c89-4f5f-d0cb-6d1ecaa3f9e8"
   },
   "outputs": [],
   "source": [
    "'''You're now creating a second PEFT model called peft_model_sentences, for use with your English quotes dataset. You're wrapping the same foundational_model (bloomz-560m) again, using the \n",
    "same generation_config. This gives you another instance of a prompt-tuned model with a fresh set of virtual tokens,  initialized randomly. This means peft_model_sentences can be \n",
    "trained separately on a different task (e.g., quotes), while peft_model_prompt could be trained on your prompt-style dataset\n",
    "‚Äî both sharing the same base model but learning different soft prompts.'''\n",
    "peft_model_sentences = get_peft_model(foundational_model, generation_config)\n",
    "print(peft_model_sentences.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff5bc33-8cfb-4144-8962-9c54362a7faa",
     "showTitle": false,
     "title": ""
    },
    "id": "i6WhJSUwVaTE"
   },
   "source": [
    "**That's amazing: did you see the reduction in trainable parameters? We are going to train a 0.001% of the paramaters available.**\n",
    "\n",
    "Now we are going to create the training arguments, and we will use the same configuration in both trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJoznfzjVaTE"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "'''| Argument                    | Meaning                                                                                                                     |\n",
    "| --------------------------- | --------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `output_dir=path`           | Where to store model checkpoints, logs, and outputs.                                                                        |\n",
    "| `use_cpu=True`              | Tells Hugging Face `Trainer` to use CPU instead of GPU. Very important for CPU-only systems.                                |\n",
    "| `auto_find_batch_size=True` | Dynamically adjusts batch size during training to avoid OOM errors. Great for large models.                                 |\n",
    "| `learning_rate=0.0035`      | A higher learning rate than full-model fine-tuning ‚Äî works well for prompt tuning, where only a few parameters are trained. |\n",
    "| `num_train_epochs=6`        | Number of passes over the dataset.                                                                                          |\n",
    "'''\n",
    "def create_training_arguments(path, learning_rate=0.0035, epochs=6):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
    "        use_cpu=True, # This is necessary for CPU clusters.\n",
    "        auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically\n",
    "        learning_rate= learning_rate, # Higher learning rate than full fine-tuning\n",
    "        num_train_epochs=epochs\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54b78a8f-81f0-44c0-b0bc-dcb14891715f",
     "showTitle": false,
     "title": ""
    },
    "id": "cb1j50DSVaTE"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "working_dir = \"./\"\n",
    "\n",
    "#Is best to store the models in separate folders.\n",
    "#Create the name of the directories where to store the models.\n",
    "output_directory_prompt =  os.path.join(working_dir, \"peft_outputs_prompt\")\n",
    "output_directory_sentences = os.path.join(working_dir, \"peft_outputs_sentences\")\n",
    "\n",
    "#Just creating the directoris if not exist.\n",
    "if not os.path.exists(working_dir):\n",
    "    os.mkdir(working_dir)\n",
    "if not os.path.exists(output_directory_prompt):\n",
    "    os.mkdir(output_directory_prompt)\n",
    "if not os.path.exists(output_directory_sentences):\n",
    "    os.mkdir(output_directory_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OC5IhO9mVaTE"
   },
   "source": [
    "We need to indicate the directory containing the model when creating the TrainingArguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4v4RSSeVaTE"
   },
   "outputs": [],
   "source": [
    "'''You‚Äôre calling your create_training_arguments() function twice: Once for your prompt-tuning model, saving checkpoints & logs to peft_outputs_prompt\n",
    "Once for your sentence/quotes-tuning model, saving to peft_outputs_sentences. Both use a learning rate of 0.003 and the number of epochs you defined earlier (NUM_EPOCHS = 5 or 6).'''\n",
    "training_args_prompt = create_training_arguments(output_directory_prompt, 0.003, NUM_EPOCHS)\n",
    "training_args_sentences = create_training_arguments(output_directory_sentences, 0.003, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c593deb6-5626-4fd9-89c2-2329e2f9b6e0",
     "showTitle": false,
     "title": ""
    },
    "id": "GdMfjk5RVaTE"
   },
   "source": [
    "## Train\n",
    "\n",
    "We will create the trainer Object, one for each model to train.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVAfNdEIVaTE"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "'''model=model: You‚Äôre passing in the PEFT model (which has frozen base weights and trainable virtual tokens).\n",
    "args=training_args: Passes your previously created training configurations.\n",
    "train_dataset=train_dataset: Expects a dataset formatted with tokenized inputs (like your prompt or quotes datasets).\n",
    "data_collator=DataCollatorForLanguageModeling(...):\n",
    "This batches and pads samples dynamically during training.\n",
    "mlm=False because BLOOMZ is a causal LM, not masked LM (e.g., BERT).\n",
    "So this means it prepares inputs for autoregressive generation training.'''\n",
    "def create_trainer(model, training_args, train_dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
    "        args=training_args, #The args for the training.\n",
    "        train_dataset=train_dataset, #The dataset used to tyrain the model.\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
    "    )\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32e43bcf-23b2-46aa-9cf0-455b83ef4f38",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "1Sz9BeFZVaTF",
    "outputId": "39beef56-b054-483c-8fbf-4326603396a9"
   },
   "outputs": [],
   "source": [
    "#Training first model.\n",
    "'''Sets up the Trainer object with your prompt-tuned PEFT model, training args, and prompt dataset.'''\n",
    "trainer_prompt = create_trainer(peft_model_prompt, training_args_prompt, train_sample_prompt)\n",
    "'''Starts the training loop: Loads batches from your train_sample_prompt. Runs forward and backward passes only updating your virtual prompt tokens.\n",
    "Saves checkpoints to peft_outputs_prompt (your specified directory). Logs training progress (loss, step, etc.).'''\n",
    "trainer_prompt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "id": "afTotMckVaTF",
    "outputId": "e405d99d-abac-4fca-83e5-84443cbfb9ae"
   },
   "outputs": [],
   "source": [
    "#Training second model.\n",
    "trainer_sentences = create_trainer(peft_model_sentences, training_args_sentences, train_sample_sentences)\n",
    "trainer_sentences.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2Zsww_2VaTF"
   },
   "source": [
    "In less than 10 minutes (CPU time in a M1 Pro) we trained 2 different models, with two different missions with a same foundational model as a base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a6c8daf-8248-458a-9f6f-14865b4fbd2e",
     "showTitle": false,
     "title": ""
    },
    "id": "s5k10HwoVaTG"
   },
   "source": [
    "## Save models\n",
    "We are going to save the models. These models are ready to be used, as long as we have the pre-trained model from which they were created in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "409df5ce-e496-46d7-be2c-202a463cdc80",
     "showTitle": false,
     "title": ""
    },
    "id": "E3dn3PeMVaTG"
   },
   "outputs": [],
   "source": [
    "'''Your base foundation model (foundational_model) weights remain unchanged.\n",
    "The prompt tuning adapters (virtual tokens embeddings) are saved in those directories.\n",
    "This makes it easy to reload and apply the fine-tuned prompts later without retraining.'''\n",
    "trainer_prompt.model.save_pretrained(output_directory_prompt)\n",
    "trainer_sentences.model.save_pretrained(output_directory_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb14e3fd-bbf6-4d56-92c2-51bfe08de72a",
     "showTitle": false,
     "title": ""
    },
    "id": "rkUKpDDWVaTG"
   },
   "source": [
    "## Inference\n",
    "\n",
    "You can load the model from the path that you have saved to before, and ask the model to generate text based on our input before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cc48af16-c117-4019-a31a-ce1c93cd21d4",
     "showTitle": false,
     "title": ""
    },
    "id": "dlqXXN8oVaTG"
   },
   "outputs": [],
   "source": [
    "'''The PeftModel.from_pretrained() method expects the base model object (already loaded) as the first argument, not the model name string. So you need to first load the base model \n",
    "(foundational_model is presumably the model instance you loaded earlier), then wrap it with the PEFT adapter from your saved directory.'''\n",
    "from peft import PeftModel\n",
    "\n",
    "loaded_model_prompt = PeftModel.from_pretrained(foundational_model,\n",
    "                                         output_directory_prompt,\n",
    "                                         #device_map='auto',\n",
    "                                         is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b44524b-2ac5-4e74-81e6-c406d4414e42",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-4jd3zCGVaTG",
    "outputId": "419c714c-8357-48ca-a704-3c15a701b583"
   },
   "outputs": [],
   "source": [
    "'''Calls your previously defined get_outputs() function, which runs .generate() on the model with the tokenized input prompt.\n",
    "\n",
    "Then decodes the generated token IDs back into readable text, skipping special tokens like <eos>.'''\n",
    "loaded_model_prompt_outputs = get_outputs(loaded_model_prompt, input_prompt)\n",
    "print(tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHbeFTXjVaTG"
   },
   "source": [
    "If we compare both answers something changed.\n",
    "* ***Pretrained Model:*** *I want you to act as a motivational coach.  Don't be afraid of being challenged.*\n",
    "* ***Fine Tuned Model:*** *I want you to act as a motivational coach.  You can use this method if you're not sure what your goals are.*\n",
    "\n",
    "We have to keep in mind that we have only trained the model for a few minutes, but they have been enough to obtain a response closer to what we were looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuwAsq3uVaTG"
   },
   "outputs": [],
   "source": [
    "loaded_model_sentences = PeftModel.from_pretrained(foundational_model,\n",
    "                                         output_directory_sentences,\n",
    "                                         #device_map='auto',\n",
    "                                         is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQm--PWSVaTH",
    "outputId": "2d86bd1c-2832-4ed3-9f1a-83c4e8b17dd6"
   },
   "outputs": [],
   "source": [
    "loaded_model_sentences_outputs = get_outputs(loaded_model_sentences, input_sentences)\n",
    "print(tokenizer.batch_decode(loaded_model_sentences_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnR8y9gwVaTH"
   },
   "source": [
    "With the second model we have a similar result.\n",
    "* **Pretrained Model:** *There two thing that matter: the size and shape of a flower*\n",
    "* **Fine Tuned Model:** *There two thing that matter: one is the weather and another, what you do.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6TUjNtGVaTH"
   },
   "source": [
    "# Conclusion\n",
    "Prompt Tuning is an amazing technique that can save us hours of training and a significant amount of money. In the notebook, we have trained two models in just a few minutes, and we can have both models in memory, providing service to different clients.\n",
    "\n",
    "If you want to try different combinations and models, the notebook is ready to use another model from the Bloom family.\n",
    "\n",
    "You can change the number of epochs to train, the number of virtual tokens, and the model in the third cell. However, there are many configurations to change. If you're looking for a good exercise, you can replace the random initialization of the virtual tokens with a fixed value.\n",
    "\n",
    "*The responses of the fine-tuned models may vary every time we train them. I've pasted the results of one of my trainings, but the actual results may differ.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5OMyCWasVaTH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "LLM 02 - Prompt Tuning with PEFT",
   "widgets": {}
  },
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
